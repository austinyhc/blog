{
  
    
        "post0": {
            "title": "Continuous Self Motivation",
            "content": "The Four Big Ideas . Habits are the compound interest of self-improvement. | If you want better results, then forget about setting goals. Focus on your system instead. | The most effective way to change your habits is to focus not on what you want to achieve, but on who you wish to become. | The Four Laws of Behavior Change are a simple set of rules we can use to build better habits. They are make it obvious | make it attractive | make it easy | make it satisfying. | . | . I. Customize 1cycle . This learning rate scheduler allows us to easily train a network using Leslie Smith&#39;s 1cycle policy. To learn more about the 1cycle technique for training neural networks check out Leslie Smith&#39;s paper and for more graphical and intuitive explanation checkout out Sylvain Gugger&#39;s post. . To use 1cycle policy we will need an optimum learning rate. We can find this learning rate by using a learning finder which can be called by using lr_finder as fastai does. It will do a mock training by going over a large range of learning rates, then plot them against the losses. We will then pick a value a bit before the minimum, where the loss still improves. Our graph would something like this: . . There is somthing to add, if we are transfer learning, we do not want to start off with too large a learning rate, or we will erase the intelligence of the model already contained in its weights. Instead, we begin with a very small learning rate and increase it gradually before lowering it again to fine-tune the weights. . . Important: After digging into the rabbit hole, I found there are two different learning rate schedule utility in tensorflow, the naming is very confusing, keras.optimizers.schedules.LearningRateSchedule and keras.callbacks.LearningRateScheduler. Although the naming is very similar, they are different in some senses. - The former is subclassing from tf.keras.optimizers while the latter is from tf.keras.Callback . The former schedule the learning rate per iteration while the former is per epoch. | . II. EfficientNet . (Read the EfficientNet paper and summarize in one of the section of this notebook) . EfficientNet, first introduced in Tan and Le, 2019 is among the most efficient models (i.e. requiring least FLOPS for inference) that reaches state-of-the-art accracy on both imagenet and common image classification transfer learning tasks. . The smallest base model is similar to MnasNet, which reached near-SOTA with a significantly smaller model. By introducing a heuristic way to scale the model, EfficientNet provides a family of models (B0 to B7) that represents a good combination of efficiency and accuracy on a variety of scales. Such a scaling heuristics (compound-scaling, details see Tan and Le, 2019) allows the efficiency-oriented base model (B0) to surpass models at every scale, while avoiding extensive grid-search of hyperparameters. . A summary of the latest updates on the model is available at here, where various augmentation schemes and semi-supervised learning approaches are applied to further improve the imagenet performance of the models. These extensions of the model can be used by updating weights without changing model topology . B0 to B7 variats of EfficientNet . Keras implementation of EfficientNet . An implementation of EfficientNet B0 to B7 has been shipped with tf.keras since TF2.3. To use EfficientNetB0 for classifying 1000 classes of images from imagenet, run: . from tensorflow.keras.applications import EfficientNetB0 model = EfficientNetB0(weights=&#39;imagenet&#39;) . The B0 model takes input images of shape (224,224,3), and the input data should range [0,255]. Normailzation is included as part of the model. . Because training EfficientNet on imagenet takes a tremendous amount of resources and several techniques that are not a part of the model architecture itself. Hence the Keras implementations by default loads pre-trained weights obtained via training with AutoAugment. . From B0 to B7 base model, the input shapes are different. Here is a list of input shpae expected for each model: . Base model resolution . EfficientNetB0 | 224 | . EfficientNetB1 | 240 | . EfficientNetB2 | 260 | . EfficientNetB3 | 300 | . EfficientNetB4 | 380 | . EfficientNetB5 | 456 | . EfficientNetB6 | 528 | . EfficientNetB7 | 600 | . When the model is intended for transfer learning, the Keras implementation provides a option to remove the top layers: . model = EfficientNetB0(include_top=False, weights=&#39;imagenet&#39;) . This option excludes the final Dense layer that turns 1280 features on the penultimate layer into prediction of the 1000 ImageNet classes. Replacing the top layer with custom layers allows using EfficientNet as a feature extractor in a transfer learning workflow. . Another argument in the model constructor worth noticing is drop_connect_rate which controls the dropout rate responsible for stochastic depth. This parameter serves as a toggle for extra regularization in finetuning, but does not affect loaded weights. For example, when stronger regularization is desired, try: . model = EfficientNetB0(weights=&#39;imagenet&#39;, drop_connect_rate=0.4) . The default value for drop_connect_rate is 0. . Clarification . AutoAugment . In this article, in section Keras implementation of EfficientNet, it says . Because training EfficientNet on ImageNet takes a tremendous amount of resources and several techniques that are not a part of the model architecture itself. Hence the Keras implementation by default loads pre-trained weights obtained via training with AutoAugment . It means the weights of keras EfficientNet are trained on the pre-trained from AutoAugment. My follow-up question is what dataset does the AutoAugment trained on? .",
            "url": "https://austinyhc.github.io/blog/personal%20development/motivation/habit/2020/12/20/Continuous_Self_Motivation.html",
            "relUrl": "/personal%20development/motivation/habit/2020/12/20/Continuous_Self_Motivation.html",
            "date": " • Dec 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Cassava Leaf Disease Classification",
            "content": "Preliminaries . This notebook is a simple training pipeline in TensorFlow for the Cassava Leaf Competition where we are given 21,397 labeled images of cassava leaves classified as 5 different groups (4 diseases and a healthy group) and asked to predict on unseen images of cassava leaves. As with most image classification problems, we can use and experiment with many different forms of augmentation and we can explore transfer learning. . . Note: I am using Dimitre&#8217;s TFRecords that can be found here. He also has 128x128, 256x256, and 384x384 sized images that I added for experimental purposes. Please give his datasets an upvote (and his work in general, it is excellent). . Dependencies . import numpy as np import pandas as pd import seaborn as sns import albumentations as A import matplotlib.pyplot as plt import os, gc, cv2, random, warnings, math, sys, json, pprint, pdb import tensorflow as tf from tensorflow.keras import backend as K import tensorflow_hub as hub from sklearn.model_selection import train_test_split . Setup . DEVICE = &#39;GPU&#39; #@param [&quot;None&quot;, &quot;&#39;GPU&#39;&quot;, &quot;&#39;TPU&#39;&quot;] {type:&quot;raw&quot;, allow-input: true} if DEVICE == &quot;TPU&quot;: print(&quot;connecting to TPU...&quot;) try: tpu = tf.distribute.cluster_resolver.TPUClusterResolver() print(&#39;Running on TPU &#39;, tpu.master()) except ValueError: print(&quot;Could not connect to TPU&quot;) tpu = None if tpu: try: print(&quot;initializing TPU ...&quot;) tf.config.experimental_connect_to_cluster(tpu) tf.tpu.experimental.initialize_tpu_system(tpu) strategy = tf.distribute.experimental.TPUStrategy(tpu) print(&quot;TPU initialized&quot;) except _: print(&quot;failed to initialize TPU&quot;) else: DEVICE = &quot;GPU&quot; if DEVICE != &quot;TPU&quot;: print(&quot;Using default strategy for CPU and single GPU&quot;) strategy = tf.distribute.get_strategy() if DEVICE == &quot;GPU&quot;: print(&quot;Num GPUs Available: &quot;, len(tf.config.experimental.list_physical_devices(&#39;GPU&#39;))) AUTOTUNE = tf.data.experimental.AUTOTUNE REPLICAS = strategy.num_replicas_in_sync print(f&#39;REPLICAS: {REPLICAS}&#39;) . Using default strategy for CPU and single GPU Num GPUs Available: 0 REPLICAS: 1 . def seed_everything(seed=0): random.seed(seed) np.random.seed(seed) tf.random.set_seed(seed) os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) os.environ[&#39;TF_DETERMINISTIC_OPS&#39;] = &#39;1&#39; def is_colab(): return &#39;google.colab&#39; in str(get_ipython()) . . Tip: Adding seed helps reproduce results. Setting debug parameter wil run the model on smaller number of epochs to validate the architecture. . #@title Debugger { run: &quot;auto&quot; } SEED = 16 DEBUG = True #@param {type:&quot;boolean&quot;} TRAIN = True #@param {type:&quot;boolean&quot;} INFERENCE = True #@param {type:&quot;boolean&quot;} IS_COLAB = is_colab() warnings.simplefilter(&#39;ignore&#39;) seed_everything(SEED) print(f&quot;Using TensorFlow v{tf.__version__}&quot;) . Using TensorFlow v2.4.0 . if IS_COLAB: from google.colab import drive drive.mount(&#39;/content/gdrive&#39;, force_remount=True) . Mounted at /content/gdrive . root_path = &#39;/content/gdrive/MyDrive&#39; if IS_COLAB else &#39;&#39; input_path = f&#39;{root_path}/kaggle/input/cassava-leaf-disease-classification&#39; output_path = f&#39;{root_path}/kaggle/working/cassava-leaf-disease-classification&#39; model_path = f&#39;{root_path}/kaggle/working/cassava-leaf-disease-classification/models&#39; os.makedirs(model_path, exist_ok=True) os.listdir(input_path) . [&#39;label_num_to_disease_map.json&#39;, &#39;sample_submission.csv&#39;, &#39;train.csv&#39;, &#39;cassava-leaf-disease-classification.zip&#39;, &#39;test_images&#39;, &#39;test_tfrecords&#39;, &#39;train_images&#39;, &#39;train_tfrecords&#39;, &#39;.ipynb_checkpoints&#39;] . EDA . df = pd.read_csv(input_path + &#39;/train.csv&#39;) . df.head() . image_id label . 0 1000015157.jpg | 0 | . 1 1000201771.jpg | 3 | . 2 100042118.jpg | 1 | . 3 1000723321.jpg | 1 | . 4 1000812911.jpg | 3 | . Check how many images are available in the training dataset and also check if each item in the training set are unique . print(f&quot;There are {len(df)} train images&quot;) len(df.image_id) == len(df.image_id.unique()) . There are 21397 train images . True . (df.label.value_counts(normalize=True) * 100).plot.barh(figsize = (8, 5)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f64fed05b00&gt; . df[&#39;filename&#39;] = df[&#39;image_id&#39;].map(lambda x : input_path + &#39;/train_images/&#39; + x) df = df.drop(columns = [&#39;image_id&#39;]) df = df.sample(frac=1).reset_index(drop=True) . df.head() . label filename . 0 3 | /content/gdrive/MyDrive/kaggle/input/cassava-l... | . 1 3 | /content/gdrive/MyDrive/kaggle/input/cassava-l... | . 2 3 | /content/gdrive/MyDrive/kaggle/input/cassava-l... | . 3 3 | /content/gdrive/MyDrive/kaggle/input/cassava-l... | . 4 3 | /content/gdrive/MyDrive/kaggle/input/cassava-l... | . if DEBUG: _, df = train_test_split( df, test_size = 0.1, random_state=SEED, shuffle=True, stratify=df[&#39;label&#39;]) . with open(input_path + &#39;/label_num_to_disease_map.json&#39;) as file: id2label = json.loads(file.read()) id2label . {&#39;0&#39;: &#39;Cassava Bacterial Blight (CBB)&#39;, &#39;1&#39;: &#39;Cassava Brown Streak Disease (CBSD)&#39;, &#39;2&#39;: &#39;Cassava Green Mottle (CGM)&#39;, &#39;3&#39;: &#39;Cassava Mosaic Disease (CMD)&#39;, &#39;4&#39;: &#39;Healthy&#39;} . In this case, we have 5 labels (4 diseases and healthy): . Cassava Bacterial Blight (CBB) | Cassava Brown Streak Disease (CBSD) | Cassava Green Mottle (CGM) | Cassava Mosaic Disease (CMD) | Healthy | In this case label 3, Cassava Mosaic Disease (CMD) is the most common label. This imbalance may have to be addressed with a weighted loss function or oversampling. I might try this in a future iteration of this kernel or in a new kernel. . Let&#39;s check an example image to see what it looks like . from PIL import Image img = Image.open(df[df.label==3][&#39;filename&#39;].iloc[0]) . width, height = img.size print(f&quot;Width: {width}, Height: {height}&quot;) . Width: 800, Height: 600 . img . EfficientNet . Configuration . BASE_MODEL, IMG_SIZE = (&#39;efficientnet_b3&#39;, 300) #@param [&quot;(&#39;efficientnet_b3&#39;, 300)&quot;, &quot;(&#39;efficientnet_b4&#39;, 380)&quot;, &quot;(&#39;efficientnet_b2&#39;, 260)&quot;] {type:&quot;raw&quot;, allow-input: true} BATCH_SIZE = 32 #@param {type:&quot;integer&quot;} IMG_SIZE = (IMG_SIZE, IMG_SIZE) #@param [&quot;(IMG_SIZE, IMG_SIZE)&quot;, &quot;(512,512)&quot;] {type:&quot;raw&quot;} print(&quot;Using {} with input size {}&quot;.format(BASE_MODEL, IMG_SIZE)) . Using efficientnet_b3 with input size (300, 300) . Loading data . After my quick and rough EDA, let&#39;s load the PIL Image to a Numpy array, so we can move on to data augmentation. . In fastai, they have item_tfms and batch_tfms defined for their data loader API. The item transforms performs a fairly large crop to 224 and also apply other standard augmentations (in aug_tranforms) at the batch level on the GPU. The batch size is set to 32 here. . Splitting . train_df, valid_df = train_test_split( df ,test_size = 0.2 ,random_state = SEED ,shuffle = True ,stratify = df[&#39;label&#39;]) . Constructing Dataset . train_ds = tf.data.Dataset.from_tensor_slices( (train_df.filename.values,train_df.label.values)) valid_ds = tf.data.Dataset.from_tensor_slices( (valid_df.filename.values, valid_df.label.values)) adapt_ds = tf.data.Dataset.from_tensor_slices( train_df.filename.values) . for x,y in valid_ds.take(3): print(x, y) . tf.Tensor(b&#39;/content/gdrive/MyDrive/kaggle/input/cassava-leaf-disease-classification/train_images/2779684221.jpg&#39;, shape=(), dtype=string) tf.Tensor(3, shape=(), dtype=int64) tf.Tensor(b&#39;/content/gdrive/MyDrive/kaggle/input/cassava-leaf-disease-classification/train_images/410264599.jpg&#39;, shape=(), dtype=string) tf.Tensor(3, shape=(), dtype=int64) tf.Tensor(b&#39;/content/gdrive/MyDrive/kaggle/input/cassava-leaf-disease-classification/train_images/1144328728.jpg&#39;, shape=(), dtype=string) tf.Tensor(3, shape=(), dtype=int64) . . Important: At this point, you may have noticed that I have not used any kind of normalization or rescaling. I recently discovered that there is Normalization layer included in Keras&#8217; pretrained EfficientNet, as mentioned here. . Item transformation . Basically item transformations mainly make sure the input data is of the same size so that it can be collated in batches. . def decode_image(filename): img = tf.io.read_file(filename) img = tf.image.decode_jpeg(img, channels=3) return img def collate_train(filename, label): img = decode_image(filename) img = tf.image.random_brightness(img, 0.3) img = tf.image.random_flip_left_right(img, seed=None) img = tf.image.random_crop(img, size=[*IMG_SIZE, 3]) return img, label def process_adapt(filename): img = decode_image(filename) img = tf.keras.layers.experimental.preprocessing.Rescaling(1.0 / 255)(img) return img def collate_valid(filename, label): img = decode_image(filename) img = tf.image.resize(img, [*IMG_SIZE]) return img, label . train_ds = train_ds.map(collate_train, num_parallel_calls=AUTOTUNE) valid_ds = valid_ds.map(collate_valid, num_parallel_calls=AUTOTUNE) adapt_ds = adapt_ds.map(process_adapt, num_parallel_calls=AUTOTUNE) . def show_images(ds): _,axs = plt.subplots(4,6,figsize=(24,16)) for ((x, y), ax) in zip(ds.take(24), axs.flatten()): ax.imshow(x.numpy().astype(np.uint8)) ax.set_title(np.argmax(y)) ax.axis(&#39;off&#39;) . show_images(train_ds) . show_images(valid_ds) . Batching Dataset . . Note: I was shuffing the validation set which is a bug . train_ds_batch = (train_ds .cache(output_path + &#39;/dump.tfcache&#39;) .shuffle(buffer_size=1000) .batch(BATCH_SIZE) .prefetch(buffer_size=AUTOTUNE)) valid_ds_batch = (valid_ds #.shuffle(buffer_size=1000) .batch(BATCH_SIZE*2) .prefetch(buffer_size=AUTOTUNE)) adapt_ds_batch = (adapt_ds .shuffle(buffer_size=1000) .batch(BATCH_SIZE) .prefetch(buffer_size=AUTOTUNE)) . Batch augmentation . data_augmentation = tf.keras.Sequential( [ tf.keras.layers.experimental.preprocessing.RandomCrop(*IMG_SIZE), tf.keras.layers.experimental.preprocessing.RandomFlip(&quot;horizontal_and_vertical&quot;), tf.keras.layers.experimental.preprocessing.RandomRotation(0.25), tf.keras.layers.experimental.preprocessing.RandomZoom((-0.2, 0)), tf.keras.layers.experimental.preprocessing.RandomContrast((0.2,0.2)) ] ) . func = lambda x,y: (data_augmentation(x), y) x = (train_ds .batch(BATCH_SIZE) .take(1) .map(func, num_parallel_calls=AUTOTUNE)) . show_images(x.unbatch()) . Building a model . I am using an EfficientNetB3 on top of which I add some output layers to predict our 5 disease classes. I decided to load the imagenet pretrained weights locally to keep the internet off (part of the requirements to submit a kernal to this competition). . from tensorflow.keras.applications import EfficientNetB3 . efficientnet = EfficientNetB3( weights = &#39;imagenet&#39; if TRAIN else None, include_top = False, input_shape = (*IMG_SIZE, 3), pooling=&#39;avg&#39;) . Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5 43941888/43941136 [==============================] - 1s 0us/step . def build_model(base_model, num_class): inputs = tf.keras.layers.Input(shape=(*IMG_SIZE, 3)) x = data_augmentation(inputs) x = base_model(x) x = tf.keras.layers.Dropout(0.4)(x) outputs = tf.keras.layers.Dense(num_class, activation=&quot;softmax&quot;, name=&quot;pred&quot;)(x) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) return model . model = build_model(base_model=efficientnet, num_class=len(id2label)) . model.summary() . Model: &#34;model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 300, 300, 3)] 0 _________________________________________________________________ sequential (Sequential) (None, 300, 300, 3) 0 _________________________________________________________________ efficientnetb3 (Functional) (None, 1536) 10783535 _________________________________________________________________ dropout (Dropout) (None, 1536) 0 _________________________________________________________________ pred (Dense) (None, 5) 7685 ================================================================= Total params: 10,791,220 Trainable params: 10,703,917 Non-trainable params: 87,303 _________________________________________________________________ . Fine tune . The 3rd layer of the Efficient is the Normalization layer, which can be tuned to our new dataset instead of imagenet. Be patient on this one, it does take a bit of time as we&#39;re going through the entire training set. . %%time if TRAIN: if not os.path.exists(output_path + &quot;/models/000_normalization.index&quot;): model.get_layer(&#39;efficientnetb3&#39;).get_layer(&#39;normalization&#39;).adapt(adapt_ds_batch) model.save_weights(filepath = output_path + &quot;/models/000_normalization&quot;) else: model.load_weights(filepath = output_path + &quot;/models/000_normalization&quot;) . CPU times: user 4 µs, sys: 0 ns, total: 4 µs Wall time: 6.68 µs . Optimizer . CosineDecay . . Important: I always wanted to try the new CosineDecayRestarts function implemented in tf.keras as it seemed promising and I struggled to find the right settings (if there were any) for the ReduceLROnPlateau . EPOCHS = 8 STEPS = int(round(len(train_df)/BATCH_SIZE)) * EPOCHS schedule = tf.keras.experimental.CosineDecayRestarts( initial_learning_rate=1e-4, first_decay_steps=65 ) . schedule.get_config() . {&#39;alpha&#39;: 0.0, &#39;first_decay_steps&#39;: 65, &#39;initial_learning_rate&#39;: 5e-05, &#39;m_mul&#39;: 1.0, &#39;name&#39;: None, &#39;t_mul&#39;: 2.0} . x = [i for i in range(STEPS)] y = [schedule(s) for s in range(STEPS)] plt.plot(x, y) . [&lt;matplotlib.lines.Line2D at 0x7f64eea542b0&gt;] . . Warning: There is a gap between what I had expected and the acutal LearningRateScheduler that tensorflow gives us. The LearningRateScheduler update the lr on_epoch_begin while it makes more sense to do it on_batch_end or on_batch_begin. . Callbacks . callbacks = [ tf.keras.callbacks.ModelCheckpoint( filepath=output_path+&#39;/models/001_best_model.h5&#39;, monitor=&#39;val_loss&#39;, save_best_only=True), ] model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(schedule), metrics=[&quot;accuracy&quot;]) . Training . if TRAIN: history = model.fit(train_ds_batch, epochs = EPOCHS, validation_data=valid_ds_batch, callbacks=callbacks) . Epoch 1/8 12/54 [=====&gt;........................] - ETA: 21:15 - loss: 0.4106 - accuracy: 0.8684 . Evaluating . def plot_hist(hist): plt.plot(history.history[&#39;loss&#39;]) plt.plot(history.history[&#39;val_loss&#39;]) plt.title(&#39;Loss over epochs&#39;) plt.ylabel(&#39;loss&#39;) plt.xlabel(&#39;epoch&#39;) plt.legend([&#39;train&#39;, &#39;valid&#39;], loc=&#39;best&#39;) plt.show() . if TRAIN: plot_hist(history) . We load the best weight that were kept from the training phase. Just to check how our model is performing, we will attempt predictions over the validation set. This can help to highlight any classes that will be consistently miscategorised. . model.load_weights(output_path + &#39;/models/001_best_model.h5&#39;) . Prediction . x = train_df.sample(1).filename.values[0] img = decode_image(x) . %%time imgs = [tf.image.random_crop(img, size=[*IMG_SIZE, 3]) for _ in range(4)] _,axs = plt.subplots(1,4,figsize=(16,4)) for (x, ax) in zip(imgs, axs.flatten()): ax.imshow(x.numpy().astype(np.uint8)) ax.axis(&#39;off&#39;) . CPU times: user 57.3 ms, sys: 870 µs, total: 58.2 ms Wall time: 62.1 ms . I apply some very basic test time augmentation to every local image extracted from the original 600-by-800 images. We know we can do some fancy augmentation with albumentations but I wanted to do that exclusively with Keras preprocessing layers to keep the cleanest pipeline possible. . tta = tf.keras.Sequential( [ tf.keras.layers.experimental.preprocessing.RandomCrop((*IMG_SIZE)), tf.keras.layers.experimental.preprocessing.RandomFlip(&quot;horizontal_and_vertical&quot;), tf.keras.layers.experimental.preprocessing.RandomZoom((-0.2, 0.2)), tf.keras.layers.experimental.preprocessing.RandomContrast((0.2,0.2)) ] ) . def predict_tta(filename, num_tta=4): img = decode_image(filename) img = tf.expand_dims(img, 0) imgs = tf.concat([tta(img) for _ in range(num_tta)], 0) preds = model.predict(imgs) return preds.sum(0).argmax() . pred = predict_tta(df.sample(1).filename.values[0]) print(pred) . 3 . if INFERENCE: from tqdm import tqdm preds = [] with tqdm(total=len(valid_df)) as pbar: for filename in valid_df.filename: pbar.update() preds.append(predict_tta(filename, num_tta=4)) . 100%|██████████| 4280/4280 [25:34&lt;00:00, 2.79it/s] . if INFERENCE: cm = tf.math.confusion_matrix(valid_df.label.values, np.array(preds)) plt.figure(figsize=(10, 8)) sns.heatmap(cm, xticklabels=id2label.values(), yticklabels=id2label.values(), annot=True, fmt=&#39;g&#39;, cmap=&quot;Blues&quot;) plt.xlabel(&#39;Prediction&#39;) plt.ylabel(&#39;Label&#39;) plt.show() . test_folder = input_path + &#39;/test_images/&#39; submission_df = pd.DataFrame(columns={&quot;image_id&quot;,&quot;label&quot;}) submission_df[&quot;image_id&quot;] = os.listdir(test_folder) submission_df[&quot;label&quot;] = 0 . submission_df[&#39;label&#39;] = (submission_df[&#39;image_id&#39;] .map(lambda x : predict_tta(test_folder+x))) . submission_df . image_id label . 0 2216849948.jpg | 4 | . submission_df.to_csv(&quot;submission.csv&quot;, index=False) . 1% Better Everyday . reference . https://www.kaggle.com/c/cassava-leaf-disease-classification | https://www.kaggle.com/dimitreoliveira/cassava-leaf-disease-training-with-tpu-v2-pods/notebook#Training-data-samples-(with-augmentation) | https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/#keras-implementation-of-efficientnet | https://www.tensorflow.org/guide/gpu_performance_analysis | https://www.tensorflow.org/guide/data_performance#prefetching | https://www.tensorflow.org/guide/data_performance_analysis | . . todos . See if I can integrate the Cutmix/Mixup augmentations in the appendix into our existing notebook. This is an excellent example | Still want to figure out some intuition of item aug and batch aug. I don&#39;t know, maybe there is some limitation or how to do so to help to speed up. | Learn more about the adapt function that being used to retrain the normalization layer of the EfficientNetB3. | . . done . Predict in batch to speed up | Add a cell for checkbox parameter to select between kaggle and colab, default is Kaggle. | Try out the data_generator and the data_frame_iterator | Removing normalizaiton step in generator since in EfficientNet, normalization is done within the model itself and the model expects input in the range of [0,255] | Find out the intuition and the difference between item_tfm and batch_tfm . In fastai, item_tfm defines the transforms that are done on the CPU and batch_tfm defines those done on the GPU. . | Customize my own data generator as fastai creates their Dataloader . No need, things are much easier than what I was originally expecting. Please refer to the Loading data section in this notebook. . | The 3rd layer of the Efficientnet is the Normalization layer, which can be tuned to our new dataset instead of imagenet. Be patient on this one, it does take a bit of time we&#39;re going through the entire training set. . | Add seed_everything function | . Appendix . The albumentation is primarily used for resizing and normalization. . def albu_transforms_train(data_resize): return A.Compose([ A.ToFloat(), A.Resize(data_resize, data_resize), ], p=1.) # For Validation def albu_transforms_valid(data_resize): return A.Compose([ A.ToFloat(), A.Resize(data_resize, data_resize), ], p=1.) . def CutMix(image, label, DIM, PROBABILITY = 1.0): # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3] # output - a batch of images with cutmix applied CLASSES = 5 imgs = []; labs = [] for j in range(len(image)): # DO CUTMIX WITH PROBABILITY DEFINED ABOVE P = tf.cast( tf.random.uniform([],0,1)&lt;=PROBABILITY, tf.int32) # CHOOSE RANDOM IMAGE TO CUTMIX WITH k = tf.cast( tf.random.uniform([],0,len(image)),tf.int32) # CHOOSE RANDOM LOCATION x = tf.cast( tf.random.uniform([],0,DIM),tf.int32) y = tf.cast( tf.random.uniform([],0,DIM),tf.int32) b = tf.random.uniform([],0,1) # this is beta dist with alpha=1.0 WIDTH = tf.cast( DIM * tf.math.sqrt(1-b),tf.int32) * P ya = tf.math.maximum(0,y-WIDTH//2) yb = tf.math.minimum(DIM,y+WIDTH//2) xa = tf.math.maximum(0,x-WIDTH//2) xb = tf.math.minimum(DIM,x+WIDTH//2) # MAKE CUTMIX IMAGE one = image[j,ya:yb,0:xa,:] two = image[k,ya:yb,xa:xb,:] three = image[j,ya:yb,xb:DIM,:] middle = tf.concat([one,two,three],axis=1) img = tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM,:,:]],axis=0) imgs.append(img) # MAKE CUTMIX LABEL a = tf.cast(WIDTH*WIDTH/DIM/DIM,tf.float32) labs.append((1-a)*label[j] + a*label[k]) # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?) image2 = tf.reshape(tf.stack(imgs),(len(image),DIM,DIM,3)) label2 = tf.reshape(tf.stack(labs),(len(image),CLASSES)) return image2,label2 . def MixUp(image, label, DIM, PROBABILITY = 1.0): # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3] # output - a batch of images with mixup applied CLASSES = 5 imgs = []; labs = [] for j in range(len(image)): # DO MIXUP WITH PROBABILITY DEFINED ABOVE P = tf.cast( tf.random.uniform([],0,1)&lt;=PROBABILITY, tf.float32) # CHOOSE RANDOM k = tf.cast( tf.random.uniform([],0,len(image)),tf.int32) a = tf.random.uniform([],0,1)*P # this is beta dist with alpha=1.0 # MAKE MIXUP IMAGE img1 = image[j,] img2 = image[k,] imgs.append((1-a)*img1 + a*img2) # MAKE CUTMIX LABEL labs.append((1-a)*label[j] + a*label[k]) # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?) image2 = tf.reshape(tf.stack(imgs),(len(image),DIM,DIM,3)) label2 = tf.reshape(tf.stack(labs),(len(image),CLASSES)) return image2,label2 . .",
            "url": "https://austinyhc.github.io/blog/plant/disease/classification/efficientnet/2020/12/20/Cassava_Leaf_Disease_Classification.html",
            "relUrl": "/plant/disease/classification/efficientnet/2020/12/20/Cassava_Leaf_Disease_Classification.html",
            "date": " • Dec 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://austinyhc.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://austinyhc.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am Austin; after years plunging into Smartphone Industry and IC Design, I have found data science is crucial in every aspect possible. Thanks to Jeremy Howard, the founder of Fast.ai, he re-enlightens my passion for Machine Learning, especially Deep Learning and Transfer Learning. I have freelanced with different organizations and continuously strive to apply DL to any project, team, or goal. Hope to meet a ton of you in the sphere of AI and to contribute as best as I can to your community. .",
          "url": "https://austinyhc.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://austinyhc.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}