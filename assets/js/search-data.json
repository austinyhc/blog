{
  
    
        "post0": {
            "title": "The Real World is not a Kaggle Competition",
            "content": "Outline . introduction | over-engineering | the best performance | be mindful of your surroundings | the trade-off | getting it just right | minimalism | . . Sunoud Collection from my Mind . to me Kaggle competetion is only a part of a real-lift data science project. In real life there are multiple other aspects that Kaggle doesn&#39;t touch on. Kagggle is a perfect platform for hosting and learning specific skills, but a real-life science problems are usually much bigger challengesfso the distinction is worth making. | . . Note: Draft 1 - In my experience, the most challenging part is to convince the Business Users (Board of Directors and other C-level executives) and merchants to follow through your solution. You really need to have a data backed strong analysis result to persuade them that your solution is indeed better than their business acumen, which they have inculcated over the years. And in such a scenario, black-box type models involving cool stacking and blending which is used to climb up the leader-board, often fail. Business Users can get quite defensive and be reluctant to take ahead your analysis, just because it achieves 95% accuracy! So most of the time, what your want is a white-box algorithm with a reasonable accuracy, say 90%, is easy to follow and explainable in business terms. . . Note: Draft 2 for over-engineering - To give a concrete example, let&#39;s follow through the Promotion Effectiveness Analysis which is a classic problem for any Retailer. Usually what htey want out of the exercise is some way to quantify the effectiveness of Promotion X which they ran and a list of products which are likely to perform well under it. Now let&#39;s assume two data scientist are given with this task. You being a pro in Kaggle, use a lot of ensembling, blending and stacking and now the new trend of incorporating data leaks and other cool hacks up your sleeve to make your model most accurate in terms of perdicting which products are expected to perform well under Promotion X. But you should mentally prepare yourself before hand, for you might be turned down by the Business because these usually influence million-dollar decisions and they have a hard time relying on a black-box! . Introduction . Machine Learning and Deep Learning are evolving at a faster pace than ever since early 2010s. . . Note: Draft 1 for Introduction, the don&amp;#8217;t get me wrong tone which is inspired by this Quora reply - I am in no way undermining the capabiliteis you can develop by participanting in these excellent Data Science competitions. In fact, kaggle teaches you feature engineering really well! But what I want you know, is that these problems are subset of what you will face in real-life as a data scientist. There will be cases when you will benefit from all the cool hacks you learn in these platforms for a solution that doesn&#39;t require a peek into what&#39;s going on inside the bos. There will be some which will require you to discover, gain confidence and slowly move to black-box. The key is in understanding the situation, business problem at hand, expectation of stakeholders and some how balancing all these, yet rendering out a reasonable accuracy. . The Best Performance . Be Mindful of Your Surroundings . The Trade-Off . Getting the Size Just Right . Minimal Model .",
            "url": "https://austinyhc.github.io/blog/deep%20learning/2020/12/24/the-real-world-is-not-a-kaggle-competition.html",
            "relUrl": "/deep%20learning/2020/12/24/the-real-world-is-not-a-kaggle-competition.html",
            "date": " • Dec 24, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Petals to the Metal",
            "content": "%%writefile conditional_cell_extension.py def run_if(line, cell=None): &#39;&#39;&#39;Execute current line/cell if line evaluates to True.&#39;&#39;&#39; if not eval(line): return get_ipython().ex(cell) def load_ipython_extension(shell): &#39;&#39;&#39;Registers the run_if magic when the extension loads.&#39;&#39;&#39; shell.register_magic_function(run_if, &#39;line_cell&#39;) def unload_ipython_extension(shell): &#39;&#39;&#39;Unregisters the run_if magic when the extension unloads.&#39;&#39;&#39; del shell.magics_manager.magics[&#39;cell&#39;][&#39;run_if&#39;] . Overwriting conditional_cell_extension.py . %reload_ext skip_kernel_extension . import numpy as np import pandas as pd import seaborn as sns import albumentations as A import matplotlib.pyplot as plt import os, gc, cv2, random, re import warnings, math, sys, json, pprint, pdb import tensorflow as tf from tensorflow.keras import backend as K import tensorflow_hub as hub from sklearn.model_selection import train_test_split warnings.simplefilter(&#39;ignore&#39;) print(f&quot;Using TensorFlow v{tf.__version__}&quot;) . . Using TensorFlow v2.4.0 . #@title Accelerator { run: &quot;auto&quot; } DEVICE = &#39;TPU&#39; #@param [&quot;None&quot;, &quot;&#39;GPU&#39;&quot;, &quot;&#39;TPU&#39;&quot;] {type:&quot;raw&quot;, allow-input: true} if DEVICE == &quot;TPU&quot;: print(&quot;connecting to TPU...&quot;) try: tpu = tf.distribute.cluster_resolver.TPUClusterResolver() print(&#39;Running on TPU &#39;, tpu.master()) except ValueError: print(&quot;Could not connect to TPU&quot;) tpu = None if tpu: try: print(&quot;initializing TPU ...&quot;) tf.config.experimental_connect_to_cluster(tpu) tf.tpu.experimental.initialize_tpu_system(tpu) strategy = tf.distribute.experimental.TPUStrategy(tpu) print(&quot;TPU initialized&quot;) except _: print(&quot;failed to initialize TPU&quot;) else: DEVICE = &quot;GPU&quot; if DEVICE != &quot;TPU&quot;: print(&quot;Using default strategy for CPU and single GPU&quot;) strategy = tf.distribute.get_strategy() if DEVICE == &quot;GPU&quot;: print(&quot;Num GPUs Available: &quot;, len(tf.config.experimental.list_physical_devices(&#39;GPU&#39;))) AUTOTUNE = tf.data.experimental.AUTOTUNE REPLICAS = strategy.num_replicas_in_sync print(f&#39;REPLICAS: {REPLICAS}&#39;) . . Tip: Adding seed helps reproduce results. Setting debug parameter wil run the model on smaller number of epochs to validate the architecture. . def seed_everything(seed=0): random.seed(seed) np.random.seed(seed) tf.random.set_seed(seed) os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) os.environ[&#39;TF_DETERMINISTIC_OPS&#39;] = &#39;1&#39; GOOGLE = &#39;google.colab&#39; in str(get_ipython()) KAGGLE = not Colab . . project_name = &#39;tpu-getting-started&#39; root_path = &#39;/content/gdrive/MyDrive/&#39; if GOOGLE else &#39;/&#39; input_path = f&#39;{root_path}kaggle/input/{project_name}/&#39; working_path = f&#39;{input_path}working/&#39; if GOOGLE else &#39;/kaggle/working/&#39; os.makedirs(working_path, exist_ok=True) os.chdir(working_path) os.listdir(input_path) . [&#39;working&#39;] . TFRecord basics . GCS_PATTERN = &#39;gs://flowers-public/*/*.jpg&#39; GCS_OUTPUT = &#39;gs://flowers-public/tfrecords-jpeg-192x192-2/flowers&#39; SHARDS = 16 TARGET_SIZE = [192, 192] CLASSES = [b&#39;daisy&#39;, b&#39;dandelion&#39;, b&#39;roses&#39;, b&#39;sunflowers&#39;, b&#39;tulips&#39;] . Read images and labels . def decode_image_and_label(filename): bits = tf.io.read_file(filename) image = tf.image.decode_jpeg(bits) label = tf.strings.split(tf.expand_dims(filename, axis=-1), sep=&#39;/&#39;) #label = tf.strings.split(filename, sep=&#39;/&#39;) label = label.values[-2] label = tf.cast((CLASSES==label), tf.int8) return image, label . filenames = tf.data.Dataset.list_files(GCS_PATTERN, seed=16) . for x in filenames.take(10): print(x) . tf.Tensor(b&#39;gs://flowers-public/tulips/251811158_75fa3034ff.jpg&#39;, shape=(), dtype=string) tf.Tensor(b&#39;gs://flowers-public/daisy/506348009_9ecff8b6ef.jpg&#39;, shape=(), dtype=string) tf.Tensor(b&#39;gs://flowers-public/daisy/2019064575_7656b9340f_m.jpg&#39;, shape=(), dtype=string) tf.Tensor(b&#39;gs://flowers-public/tulips/8713396140_5af8136136.jpg&#39;, shape=(), dtype=string) tf.Tensor(b&#39;gs://flowers-public/roses/218630974_5646dafc63_m.jpg&#39;, shape=(), dtype=string) tf.Tensor(b&#39;gs://flowers-public/roses/410421672_563550467c.jpg&#39;, shape=(), dtype=string) tf.Tensor(b&#39;gs://flowers-public/tulips/8614237582_74417799f4_m.jpg&#39;, shape=(), dtype=string) tf.Tensor(b&#39;gs://flowers-public/dandelion/8797114213_103535743c_m.jpg&#39;, shape=(), dtype=string) tf.Tensor(b&#39;gs://flowers-public/dandelion/11296320473_1d9261ddcb.jpg&#39;, shape=(), dtype=string) tf.Tensor(b&#39;gs://flowers-public/dandelion/14554897292_b3e30e52f2.jpg&#39;, shape=(), dtype=string) . ds0 = filenames.map(decode_image_and_label, num_parallel_calls=AUTOTUNE) . def show_images(ds): _,axs = plt.subplots(3,3,figsize=(16,16)) for ((x, y), ax) in zip(ds.take(9), axs.flatten()): ax.imshow(x.numpy().astype(np.uint8)) ax.set_title(np.argmax(y)) ax.axis(&#39;off&#39;) . show_images(ds0) . Resize and crop images to common size . No need to study the code in this cell. It&#39;s only image resizing. . def resize_and_crop_image(image, label): # Resize and crop using &quot;fill&quot; algorithm: # always make sure the resulting image # is cut out from the source image so that # it fills the TARGET_SIZE entirely with no # black bars and a preserved aspect ratio. w = tf.shape(image)[0] h = tf.shape(image)[1] tw = TARGET_SIZE[1] th = TARGET_SIZE[0] resize_crit = (w * th) / (h * tw) image = tf.cond(resize_crit &lt; 1, lambda: tf.image.resize(image, [w*tw/w, h*tw/w]), # if true lambda: tf.image.resize(image, [w*th/h, h*th/h]) # if false ) nw = tf.shape(image)[0] nh = tf.shape(image)[1] image = tf.image.crop_to_bounding_box(image, (nw - tw) // 2, (nh - th) // 2, tw, th) return image, label . ds1 = ds0.map(resize_and_crop_image, num_parallel_calls=AUTOTUNE) . show_images(ds1) . Speed test: too slow . Google Cloud Storage is capable of great throughput but has a per-file access penalty. Run the cell below and see that throughput is around 8 images per second. That is too slow. Training on thousands of individual files will not work. We have to use the TFRecord format to group files together. . %%time for image,label in ds1.batch(8).take(10): print(&quot;Image batch shape {} {}&quot;.format( image.numpy().shape, [np.argmax(lbl) for lbl in label.numpy()])) . Image batch shape (8, 192, 192, 3) [0, 1, 0, 0, 1, 3, 2, 1] Image batch shape (8, 192, 192, 3) [3, 4, 4, 0, 3, 4, 3, 0] Image batch shape (8, 192, 192, 3) [0, 3, 0, 4, 2, 4, 2, 4] Image batch shape (8, 192, 192, 3) [3, 4, 4, 0, 2, 3, 2, 3] Image batch shape (8, 192, 192, 3) [1, 3, 4, 3, 0, 3, 1, 3] Image batch shape (8, 192, 192, 3) [4, 4, 3, 0, 0, 4, 4, 1] Image batch shape (8, 192, 192, 3) [1, 3, 1, 3, 1, 2, 4, 2] Image batch shape (8, 192, 192, 3) [1, 4, 2, 4, 2, 2, 1, 0] Image batch shape (8, 192, 192, 3) [0, 3, 2, 2, 3, 4, 0, 1] Image batch shape (8, 192, 192, 3) [1, 2, 0, 1, 0, 3, 4, 1] CPU times: user 65.8 ms, sys: 42.5 ms, total: 108 ms Wall time: 4.39 s . Recompress the images . The bandwidth savings outweight the decoding CPU cost . def recompress_image(image, label): height = tf.shape(image)[0] width = tf.shape(image)[1] image = tf.cast(image, tf.uint8) image = tf.image.encode_jpeg(image, optimize_size=True, chroma_downsampling=False) return image, label, height, width . IMAGE_SIZE = len(tf.io.gfile.glob(GCS_PATTERN)) SHARD_SIZE = math.ceil(1.0 * IMAGE_SIZE / SHARDS) . ds2 = ds1.map(recompress_image, num_parallel_calls=AUTOTUNE) ds2 = ds2.batch(SHARD_SIZE) # sharding: there will be one &quot;batch&quot; of images per file . Why TFRecords? . TPUs have eight cores which act as eight independent workers. We can get data to each core more efficiently by splitting the dataset into multiple files or shards. This way, each core can grab an independent part of the data as it needs. The most convenient kind of file to use for sharding in TensorFlow is a TFRecord. A TFRecord is a binary file that contains sequences of byte-strings. Data needs to be serialized (encoded as a byte-string) before being written into a TFRecord. The most convenient way of serializing data in TensorFlow is to wrap the data with tf.Example. This is a record format based on Google&#39;s protobufs but designed for TensorFlow. It&#39;s more or less like a dict with some type annotations . x = tf.constant([[1,2], [3, 4]], dtype=tf.uint8) print(x) . tf.Tensor( [[1 2] [3 4]], shape=(2, 2), dtype=uint8) . x_in_bytes = tf.io.serialize_tensor(x) print(x_in_bytes) . tf.Tensor(b&#39; x08 x04 x12 x08 x12 x02 x08 x02 x12 x02 x08 x02&#34; x04 x01 x02 x03 x04&#39;, shape=(), dtype=string) . print(tf.io.parse_tensor(x_in_bytes, out_type=tf.uint8)) . tf.Tensor( [[1 2] [3 4]], shape=(2, 2), dtype=uint8) . A TFRecord is a sequence of bytes, so we have to turn our data into byte-strings before it can go into a TFRecord. We can use tf.io.serialize_tensor to turn a tensor into a byte-string and tf.io.parse_tensor to turn it back. It&#39;s important to keep track of your tensor&#39;s datatype (in this case tf.uint8) since you have to specify it when parsing the string back to a tensor again . Write dataset to TFRecord files . . Note: Will uncomment the cells in this section when I find a gs:// domain to write to. . # return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings)) # #def _int_feature(list_of_ints): # int64 # return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints)) # #def _float_feature(list_of_floats): # float32 # return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats)) # #def to_tfrecord(tfrec_filewriter, img_bytes, label, height, width): # id = np.argmax(np.array(CLASSES)==label) # one_hot = np.eye(len(CLASSES))[id] # feature = { # &quot;image&quot;: _bytestring_feature([img_bytes]), # one image in the list # &quot;id&quot;: _int_feature([id]), # one class in the list # &quot;label&quot;: _bytestring_feature([label]), # fixed length (1) list of strings, the text label # &quot;size&quot; : _int_feature([height, width]), # fixed length (2) list of ints # &quot;one_hot&quot;: _float_feature(one_hot.tolist())# variable length list of floats, n=len(CLASSES) # } # return tf.train.Example(features=tf.train.Features(feature=feature)) . #for shard_id, (image, label, height, width) in ds2.enumerate(): # shard_size = image.numpy().shape[0] # filename = GCS_OUTPUT + &quot;{:02d}-{}tfrec&quot;.format(shard_id, shard_size) # # with tf.io.TFRecordWriter(filename) as outfile: # for i in range(shard_size): # example = to_tfrecord(out_file, # image.numpy()[i], # label.numpy()[i], # height.numpy()[i], # width.numpy()[i]) # out_file.write(example.SerializeToString()) # print(&quot;Wrote file {} containing {} records&quot;.format(filename, shard_size)) . Read from TFRecord Dataset . def read_tfrecord(example): features = { &quot;image&quot;: tf.io.FixedLenFeature([], tf.string), # tf.string = bytestring (not text string) &quot;class&quot;: tf.io.FixedLenFeature([], tf.int64), # shape [] means scalar # additional (not very useful) fields to demonstrate TFRecord writing/reading of different types of data &quot;label&quot;: tf.io.FixedLenFeature([], tf.string), # one bytestring &quot;size&quot;: tf.io.FixedLenFeature([2], tf.int64), # two integers &quot;one_hot_class&quot;: tf.io.VarLenFeature(tf.float32) # a certain number of floats } # decode the TFRecord example = tf.io.parse_single_example(example, features) # FixedLenFeature fields are now ready to use: exmple[&#39;size&#39;] # VarLenFeature fields require additional sparse_to_dense decoding image = tf.image.decode_jpeg(example[&#39;image&#39;], channels=3) image = tf.reshape(image, [*TARGET_SIZE, 3]) class_num = example[&#39;class&#39;] label = example[&#39;label&#39;] height = example[&#39;size&#39;][0] width = example[&#39;size&#39;][1] one_hot_class = tf.sparse.to_dense(example[&#39;one_hot_class&#39;]) return image, class_num, label, height, width, one_hot_class . option_no_order = tf.data.Options() option_no_order.experimental_deterministic = False filenames = tf.io.gfile.glob(GCS_OUTPUT + &quot;*tfrec&quot;) ds3 = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) ds3 = (ds3.with_options(option_no_order) .map(read_tfrecord, num_parallel_calls=AUTOTUNE) .shuffle(30)) . ds3_to_show = ds3.map(lambda image, id, label, height, width, one_hot: (image, label)) show_images(ds3_to_show) . Speed test: fast . Loading form TFRecords is almost 10x time faster than loading from JPEGs. . %%time for image, class_num, label, height, width, one_hot_class in ds3.batch(8).take(10): print(&quot;Image batch shape {} {}&quot;.format( image.numpy().shape, [lbl.decode(&#39;utf8&#39;) for lbl in label.numpy()])) . Image batch shape (8, 192, 192, 3) [&#39;tulips&#39;, &#39;dandelion&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;, &#39;roses&#39;, &#39;dandelion&#39;, &#39;sunflowers&#39;] Image batch shape (8, 192, 192, 3) [&#39;sunflowers&#39;, &#39;tulips&#39;, &#39;tulips&#39;, &#39;dandelion&#39;, &#39;roses&#39;, &#39;dandelion&#39;, &#39;tulips&#39;, &#39;roses&#39;] Image batch shape (8, 192, 192, 3) [&#39;daisy&#39;, &#39;daisy&#39;, &#39;sunflowers&#39;, &#39;roses&#39;, &#39;tulips&#39;, &#39;daisy&#39;, &#39;roses&#39;, &#39;tulips&#39;] Image batch shape (8, 192, 192, 3) [&#39;roses&#39;, &#39;tulips&#39;, &#39;tulips&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;, &#39;tulips&#39;, &#39;tulips&#39;, &#39;sunflowers&#39;] Image batch shape (8, 192, 192, 3) [&#39;sunflowers&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;, &#39;daisy&#39;, &#39;tulips&#39;, &#39;dandelion&#39;, &#39;roses&#39;, &#39;dandelion&#39;] Image batch shape (8, 192, 192, 3) [&#39;roses&#39;, &#39;dandelion&#39;, &#39;tulips&#39;, &#39;daisy&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;] Image batch shape (8, 192, 192, 3) [&#39;dandelion&#39;, &#39;sunflowers&#39;, &#39;daisy&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;, &#39;dandelion&#39;, &#39;daisy&#39;] Image batch shape (8, 192, 192, 3) [&#39;roses&#39;, &#39;daisy&#39;, &#39;dandelion&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;, &#39;daisy&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;] Image batch shape (8, 192, 192, 3) [&#39;tulips&#39;, &#39;sunflowers&#39;, &#39;roses&#39;, &#39;dandelion&#39;, &#39;sunflowers&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;, &#39;sunflowers&#39;] Image batch shape (8, 192, 192, 3) [&#39;daisy&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;, &#39;dandelion&#39;, &#39;daisy&#39;, &#39;roses&#39;, &#39;sunflowers&#39;, &#39;dandelion&#39;] CPU times: user 34.5 ms, sys: 14.5 ms, total: 49 ms Wall time: 549 ms . Hyperparameters . BASE_MODEL = &#39;efficientnet_b3&#39; #@param [&quot;&#39;efficientnet_b3&#39;&quot;, &quot;&#39;efficientnet_b4&#39;&quot;, &quot;&#39;efficientnet_b2&#39;&quot;] {type:&quot;raw&quot;, allow-input: true} HEIGHT = 300#@param {type:&quot;number&quot;} WIDTH = 300#@param {type:&quot;number&quot;} CHANNELS = 3#@param {type:&quot;number&quot;} IMG_SIZE = (HEIGHT, WIDTH, CHANNELS) EPOCHS = 50#@param {type:&quot;number&quot;} BATCH_SIZE = 32 * strategy.num_replicas_in_sync #@param {type:&quot;raw&quot;} print(&quot;Use {} with input size {}&quot;.format(BASE_MODEL, IMG_SIZE)) print(&quot;Train on batch size of {} for {} epochs&quot;.format(BATCH_SIZE, EPOCHS)) . Use efficientnet_b3 with input size (300, 300, 3) Train on batch size of 32 for 50 epochs . Data . Loading data . GCS_PATH = &#39;gs://kds-e93303da9a97ef8fd254ceb5e9ed104470f247527dd45aba9685bdf5&#39; #@param {type: &quot;string&quot;} GCS_PATH += &#39;/tfrecords-jpeg-512x512&#39; #@param {type: &quot;string&quot;} CLASSES = [&#39;pink primrose&#39;, &#39;hard-leaved pocket orchid&#39;, &#39;canterbury bells&#39;, &#39;sweet pea&#39;, &#39;wild geranium&#39;, &#39;tiger lily&#39;, &#39;moon orchid&#39;, &#39;bird of paradise&#39;, &#39;monkshood&#39;, &#39;globe thistle&#39;, # 00 - 09 &#39;snapdragon&#39;, &quot;colt&#39;s foot&quot;, &#39;king protea&#39;, &#39;spear thistle&#39;, &#39;yellow iris&#39;, &#39;globe-flower&#39;, &#39;purple coneflower&#39;, &#39;peruvian lily&#39;, &#39;balloon flower&#39;, &#39;giant white arum lily&#39;, # 10 - 19 &#39;fire lily&#39;, &#39;pincushion flower&#39;, &#39;fritillary&#39;, &#39;red ginger&#39;, &#39;grape hyacinth&#39;, &#39;corn poppy&#39;, &#39;prince of wales feathers&#39;, &#39;stemless gentian&#39;, &#39;artichoke&#39;, &#39;sweet william&#39;, # 20 - 29 &#39;carnation&#39;, &#39;garden phlox&#39;, &#39;love in the mist&#39;, &#39;cosmos&#39;, &#39;alpine sea holly&#39;, &#39;ruby-lipped cattleya&#39;, &#39;cape flower&#39;, &#39;great masterwort&#39;, &#39;siam tulip&#39;, &#39;lenten rose&#39;, # 30 - 39 &#39;barberton daisy&#39;, &#39;daffodil&#39;, &#39;sword lily&#39;, &#39;poinsettia&#39;, &#39;bolero deep blue&#39;, &#39;wallflower&#39;, &#39;marigold&#39;, &#39;buttercup&#39;, &#39;daisy&#39;, &#39;common dandelion&#39;, # 40 - 49 &#39;petunia&#39;, &#39;wild pansy&#39;, &#39;primula&#39;, &#39;sunflower&#39;, &#39;lilac hibiscus&#39;, &#39;bishop of llandaff&#39;, &#39;gaura&#39;, &#39;geranium&#39;, &#39;orange dahlia&#39;, &#39;pink-yellow dahlia&#39;, # 50 - 59 &#39;cautleya spicata&#39;, &#39;japanese anemone&#39;, &#39;black-eyed susan&#39;, &#39;silverbush&#39;, &#39;californian poppy&#39;, &#39;osteospermum&#39;, &#39;spring crocus&#39;, &#39;iris&#39;, &#39;windflower&#39;, &#39;tree poppy&#39;, # 60 - 69 &#39;gazania&#39;, &#39;azalea&#39;, &#39;water lily&#39;, &#39;rose&#39;, &#39;thorn apple&#39;, &#39;morning glory&#39;, &#39;passion flower&#39;, &#39;lotus&#39;, &#39;toad lily&#39;, &#39;anthurium&#39;, # 70 - 79 &#39;frangipani&#39;, &#39;clematis&#39;, &#39;hibiscus&#39;, &#39;columbine&#39;, &#39;desert-rose&#39;, &#39;tree mallow&#39;, &#39;magnolia&#39;, &#39;cyclamen &#39;, &#39;watercress&#39;, &#39;canna lily&#39;, # 80 - 89 &#39;hippeastrum &#39;, &#39;bee balm&#39;, &#39;pink quill&#39;, &#39;foxglove&#39;, &#39;bougainvillea&#39;, &#39;camellia&#39;, &#39;mallow&#39;, &#39;mexican petunia&#39;, &#39;bromelia&#39;, &#39;blanket flower&#39;, # 90 - 99 &#39;trumpet creeper&#39;, &#39;blackberry lily&#39;, &#39;common tulip&#39;, &#39;wild rose&#39;] print(f&quot;Sourcing images from {GCS_PATH}&quot;) . Sourcing images from gs://kds-e93303da9a97ef8fd254ceb5e9ed104470f247527dd45aba9685bdf5/tfrecords-jpeg-512x512 . def decode_image(image_data): image = tf.image.decode_jpeg(image_data, channels=CHANNELS) image = (tf.cast(image, tf.float32) if GOOGLE else tf.cast(image, tf.float32) / 255.0) image = tf.image.random_crop(image, IMG_SIZE) return image def collate_labeled_tfrecord(example): LABELED_TFREC_FORMAT = { &quot;image&quot;: tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring &quot;class&quot;: tf.io.FixedLenFeature([], tf.int64), # shape [] means single element } example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT) image = decode_image(example[&#39;image&#39;]) label = tf.cast(example[&#39;class&#39;], tf.int32) return image, label def process_unlabeled_tfrecord(example): UNLABELED_TFREC_FORMAT = { &quot;image&quot;: tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring &quot;id&quot;: tf.io.FixedLenFeature([], tf.string), # shape [] means single element } example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT) image = decode_image(example[&#39;image&#39;]) idnum = example[&#39;id&#39;] return image, idnum def count_data_items(filenames): n = [int(re.compile(r&quot;-([0-9]*) .&quot;).search(filename).group(1)) for filename in filenames] return np.sum(n) . train_filenames = tf.io.gfile.glob(GCS_PATH + &#39;/train/*.tfrec&#39;) valid_filenames = tf.io.gfile.glob(GCS_PATH + &#39;/val/*.tfrec&#39;) test_filenames = tf.io.gfile.glob(GCS_PATH + &#39;/test/*.tfrec&#39;) . count_data_items(train_filenames) . 0.0 . Data augmentation . def data_augment(image, label): p_rotation = tf.random.uniform([], 0, 1.0, dtype=tf.float32) p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32) p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32) p_pixel = tf.random.uniform([], 0, 1.0, dtype=tf.float32) p_shear = tf.random.uniform([], 0, 1.0, dtype=tf.float32) p_shift = tf.random.uniform([], 0, 1.0, dtype=tf.float32) p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32) # Flips if p_spatial &gt;= .2: image = tf.image.random_flip_left_right(image) image = tf.image.random_flip_up_down(image) # Rotates if p_rotate &gt; .75: image = tf.image.rot90(image, k=3) # rotate 270º elif p_rotate &gt; .5: image = tf.image.rot90(image, k=2) # rotate 180º elif p_rotate &gt; .25: image = tf.image.rot90(image, k=1) # rotate 90º if p_rotation &gt;= .3: # Rotation image = transform_rotation(image, height=HEIGHT, rotation=45.) if p_shift &gt;= .3: # Shift image = transform_shift(image, height=HEIGHT, h_shift=15., w_shift=15.) if p_shear &gt;= .3: # Shear image = transform_shear(image, height=HEIGHT, shear=20.) # Crops if p_crop &gt; .4: crop_size = tf.random.uniform([], int(HEIGHT*.7), HEIGHT, dtype=tf.int32) image = tf.image.random_crop(image, size=[crop_size, crop_size, CHANNELS]) elif p_crop &gt; .7: if p_crop &gt; .9: image = tf.image.central_crop(image, central_fraction=.7) elif p_crop &gt; .8: image = tf.image.central_crop(image, central_fraction=.8) else: image = tf.image.central_crop(image, central_fraction=.9) image = tf.image.resize(image, size=[HEIGHT, WIDTH]) # Pixel-level transforms if p_pixel &gt;= .2: if p_pixel &gt;= .8: image = tf.image.random_saturation(image, lower=0, upper=2) elif p_pixel &gt;= .6: image = tf.image.random_contrast(image, lower=.8, upper=2) elif p_pixel &gt;= .4: image = tf.image.random_brightness(image, max_delta=.2) else: image = tf.image.adjust_gamma(image, gamma=.6) return image, label . . option_no_order = tf.data.Options() option_no_order.experimental_deterministic = False . train_ds = tf.data.TFRecordDataset(train_filenames, num_parallel_reads=AUTOTUNE) train_ds = (train_ds .map(collate_labeled_tfrecord, num_parallel_calls=AUTOTUNE) .map(data_augment, num_parallel_calls=AUTOTUNE) .repeat() .shuffle(2048) .batch(BATCH_SIZE) .prefetch(AUTOTUNE)) . show_images(train_ds.take(1).unbatch()) . valid_ds = tf.data.TFRecordDataset(valid_filenames, num_parallel_reads=AUTOTUNE) valid_ds = (valid_ds .with_options(option_no_order) .map(collate_labeled_tfrecord, num_parallel_calls=AUTOTUNE) .batch(BATCH_SIZE) .cache() .prefetch(AUTOTUNE)) . show_images(valid_ds.take(1).unbatch()) . test_ds = tf.data.TFRecordDataset(test_filenames, num_parallel_reads=AUTOTUNE) test_ds = (test_ds .with_options(option_no_order) .map(process_unlabeled_tfrecord, num_parallel_calls=AUTOTUNE) .batch(BATCH_SIZE) .prefetch(AUTOTUNE)) . Model . Batch augmentation . Augmentation can be applied in two ways. . Using the Keras Preprocessing Layers | Using the tf.image . Important: The Keras Preprocessing Layers are currently experimental so it seems it does not have supporting TPU OpKernel yet. | . # [ # tf.keras.layers.experimental.preprocessing.RandomCrop(*IMG_SIZE), # tf.keras.layers.experimental.preprocessing.RandomFlip(&quot;horizontal_and_vertical&quot;), # tf.keras.layers.experimental.preprocessing.RandomRotation(0.25), # tf.keras.layers.experimental.preprocessing.RandomZoom((-0.2, 0)), # tf.keras.layers.experimental.preprocessing.RandomContrast((0.2,0.2)) # ] #) . #x = (train_ds # .take(1) # .map(func, num_parallel_calls=AUTOTUNE)) . Building a model . Now we&#39;re ready to create a neural network for classifying images! We&#39;ll use what&#39;s known as transfer learning. With transfer learning, you reuse the body part of a pretrained model and replace its&#39; head or tail with custom layers depending on the problem that we are solving. . For this tutorial, we&#39;ll use EfficientNetb3 which is pretrained on ImageNet. Later, I might want to experiment with other models. (Xception wouldn&#39;t be a bad choice.) . Important: The distribution strategy we created earilier contains a context manager, straategy.scope. This context manager tells TensorFlow how to divide the work of training among the eight TPU cores. When using TensorFlow with a TPU, it&#8217;s important to define your model in strategy.sceop() context. . from tensorflow.keras.applications import EfficientNetB3 from tensorflow.keras.applications import VGG16 . def build_model(base_model, num_class): inputs = tf.keras.layers.Input(shape=IMG_SIZE) x = base_model(inputs) x = tf.keras.layers.Dropout(0.4)(x) outputs = tf.keras.layers.Dense(num_class, activation=&quot;softmax&quot;, name=&quot;pred&quot;)(x) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) return model . with strategy.scope(): efficientnet = EfficientNetB3( weights = &#39;imagenet&#39; if TRAIN else None, include_top = False, input_shape = IMG_SIZE, pooling=&#39;avg&#39;) #efficientnet.trainable = False; model = build_model(base_model=efficientnet, num_class=len(CLASSES)) . Optimizer . . Important: I always wanted to try the new CosineDecayRestarts function implemented in tf.keras as it seemed promising and I struggled to find the right settings (if there were any) for the ReduceLROnPlateau . STEPS = math.ceil(count_data_items(train_filenames) / BATCH_SIZE) * EPOCHS LR_START = 1e-3 #@param {type: &quot;number&quot;} LR_START *= strategy.num_replicas_in_sync LR_MIN = 1e-5 #@param {type: &quot;number&quot;} N_RESTARTS = 5#@param {type: &quot;number&quot;} T_MUL = 2.0 #@param {type: &quot;number&quot;} M_MUL = 1#@param {type: &quot;number&quot;} STEPS_START = math.ceil((T_MUL-1)/(T_MUL**(N_RESTARTS+1)-1) * STEPS) schedule = tf.keras.experimental.CosineDecayRestarts( first_decay_steps=STEPS_START, initial_learning_rate=LR_START, alpha=LR_MIN, m_mul=M_MUL, t_mul=T_MUL) x = [i for i in range(STEPS)] y = [schedule(s) for s in range(STEPS)] plt.plot(x, y) print(&#39;{:d} total epochs and {:d} steps per epoch&#39; .format(EPOCHS, STEPS // EPOCHS)) print(schedule.get_config()) . 30 total epochs and 50 steps per epoch {&#39;initial_learning_rate&#39;: 0.0064, &#39;first_decay_steps&#39;: 24, &#39;t_mul&#39;: 2.0, &#39;m_mul&#39;: 1, &#39;alpha&#39;: 1e-05, &#39;name&#39;: None} . Callbacks . callbacks = [ tf.keras.callbacks.ModelCheckpoint( filepath=&#39;001_best_model.h5&#39;, monitor=&#39;val_loss&#39;, save_best_only=True), tf.keras.callbacks.EarlyStopping( monitor=&#39;val_loss&#39;, mode=&#39;min&#39;, patience=5, restore_best_weights=True, verbose=1) ] model.compile( optimizer=tf.keras.optimizers.Adam(schedule), loss = &#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;sparse_categorical_accuracy&#39;], ) . model.summary() . Model: &#34;model_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_4 (InputLayer) [(None, 300, 300, 3)] 0 _________________________________________________________________ efficientnetb3 (Functional) (None, 1536) 10783535 _________________________________________________________________ dropout_1 (Dropout) (None, 1536) 0 _________________________________________________________________ pred (Dense) (None, 104) 159848 ================================================================= Total params: 10,943,383 Trainable params: 10,856,080 Non-trainable params: 87,303 _________________________________________________________________ . Training . Tune the normalization layer . def generate_unlabeled_tfrecord(example): LABELED_TFREC_FORMAT = { &quot;image&quot;: tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring &quot;class&quot;: tf.io.FixedLenFeature([], tf.int64), # shape [] means single element } example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT) image = decode_image(example[&#39;image&#39;]) image = image / 255.0 return image . adapt_ds = tf.data.TFRecordDataset(train_filenames, num_parallel_reads=AUTOTUNE) adapt_ds = (adapt_ds .map(generate_unlabeled_tfrecord, num_parallel_calls=AUTOTUNE) .shuffle(2048) .batch(BATCH_SIZE) .prefetch(AUTOTUNE)) . %%time model.get_layer(&#39;efficientnetb3&#39;).get_layer(&#39;normalization_1&#39;).adapt(adapt_ds) . model.save_weights(&quot;000_normalization.h5&quot;) . model.load_weights(&quot;000_normalization.h5&quot;) . Train the whole . history = model.fit( x=train_ds, validation_data=valid_ds, epochs=EPOCHS, steps_per_epoch=STEPS//BATCH_SIZE, callbacks=callbacks, verbose=2 ) . Epoch 1/30 5/5 - 9s - loss: 4.6731 - sparse_categorical_accuracy: 0.0164 - val_loss: 4.4839 - val_sparse_categorical_accuracy: 0.0428 Epoch 2/30 5/5 - 5s - loss: 4.2220 - sparse_categorical_accuracy: 0.1227 - val_loss: 3.9313 - val_sparse_categorical_accuracy: 0.1541 Epoch 3/30 5/5 - 5s - loss: 3.6802 - sparse_categorical_accuracy: 0.2195 - val_loss: 4.1627 - val_sparse_categorical_accuracy: 0.1711 Epoch 4/30 5/5 - 5s - loss: 3.1707 - sparse_categorical_accuracy: 0.3148 - val_loss: 3.2700 - val_sparse_categorical_accuracy: 0.2980 Epoch 5/30 5/5 - 4s - loss: 2.8780 - sparse_categorical_accuracy: 0.3516 - val_loss: 3.0248 - val_sparse_categorical_accuracy: 0.3279 Epoch 6/30 5/5 - 5s - loss: 2.3836 - sparse_categorical_accuracy: 0.4508 - val_loss: 3.0758 - val_sparse_categorical_accuracy: 0.3373 Epoch 7/30 5/5 - 5s - loss: 2.1572 - sparse_categorical_accuracy: 0.4813 - val_loss: 2.4670 - val_sparse_categorical_accuracy: 0.4370 Epoch 8/30 5/5 - 5s - loss: 1.9592 - sparse_categorical_accuracy: 0.5367 - val_loss: 2.0682 - val_sparse_categorical_accuracy: 0.4989 Epoch 9/30 5/5 - 4s - loss: 1.6311 - sparse_categorical_accuracy: 0.6148 - val_loss: 1.8940 - val_sparse_categorical_accuracy: 0.5315 Epoch 10/30 5/5 - 5s - loss: 1.6470 - sparse_categorical_accuracy: 0.5953 - val_loss: 1.8254 - val_sparse_categorical_accuracy: 0.5401 Epoch 11/30 5/5 - 5s - loss: 1.4733 - sparse_categorical_accuracy: 0.6375 - val_loss: 1.7074 - val_sparse_categorical_accuracy: 0.5612 Epoch 12/30 5/5 - 5s - loss: 1.4351 - sparse_categorical_accuracy: 0.6484 - val_loss: 1.5816 - val_sparse_categorical_accuracy: 0.5916 Epoch 13/30 5/5 - 5s - loss: 1.3274 - sparse_categorical_accuracy: 0.6844 - val_loss: 1.4337 - val_sparse_categorical_accuracy: 0.6360 Epoch 14/30 5/5 - 5s - loss: 1.1497 - sparse_categorical_accuracy: 0.7188 - val_loss: 1.3105 - val_sparse_categorical_accuracy: 0.6673 Epoch 15/30 5/5 - 4s - loss: 1.1825 - sparse_categorical_accuracy: 0.7016 - val_loss: 1.2016 - val_sparse_categorical_accuracy: 0.6996 Epoch 16/30 5/5 - 4s - loss: 1.1291 - sparse_categorical_accuracy: 0.7273 - val_loss: 1.1408 - val_sparse_categorical_accuracy: 0.7136 Epoch 17/30 5/5 - 5s - loss: 1.1066 - sparse_categorical_accuracy: 0.7250 - val_loss: 1.0682 - val_sparse_categorical_accuracy: 0.7276 Epoch 18/30 5/5 - 5s - loss: 1.0647 - sparse_categorical_accuracy: 0.7406 - val_loss: 1.0152 - val_sparse_categorical_accuracy: 0.7392 Epoch 19/30 5/5 - 8s - loss: 0.9543 - sparse_categorical_accuracy: 0.7602 - val_loss: 0.9607 - val_sparse_categorical_accuracy: 0.7554 Epoch 20/30 5/5 - 5s - loss: 0.9367 - sparse_categorical_accuracy: 0.7586 - val_loss: 0.9117 - val_sparse_categorical_accuracy: 0.7656 Epoch 21/30 5/5 - 5s - loss: 1.0398 - sparse_categorical_accuracy: 0.7414 - val_loss: 0.8712 - val_sparse_categorical_accuracy: 0.7759 Epoch 22/30 5/5 - 5s - loss: 0.9956 - sparse_categorical_accuracy: 0.7516 - val_loss: 0.8331 - val_sparse_categorical_accuracy: 0.7839 Epoch 23/30 5/5 - 5s - loss: 0.9891 - sparse_categorical_accuracy: 0.7578 - val_loss: 0.7999 - val_sparse_categorical_accuracy: 0.7934 Epoch 24/30 5/5 - 5s - loss: 0.9353 - sparse_categorical_accuracy: 0.7672 - val_loss: 0.7747 - val_sparse_categorical_accuracy: 0.8006 Epoch 25/30 5/5 - 5s - loss: 0.9399 - sparse_categorical_accuracy: 0.7625 - val_loss: 0.7532 - val_sparse_categorical_accuracy: 0.8036 Epoch 26/30 5/5 - 5s - loss: 0.8710 - sparse_categorical_accuracy: 0.7773 - val_loss: 0.7358 - val_sparse_categorical_accuracy: 0.8058 Epoch 27/30 5/5 - 5s - loss: 0.8857 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.7228 - val_sparse_categorical_accuracy: 0.8093 Epoch 28/30 5/5 - 5s - loss: 0.8572 - sparse_categorical_accuracy: 0.7969 - val_loss: 0.7115 - val_sparse_categorical_accuracy: 0.8125 Epoch 29/30 5/5 - 5s - loss: 0.8800 - sparse_categorical_accuracy: 0.7781 - val_loss: 0.7011 - val_sparse_categorical_accuracy: 0.8157 Epoch 30/30 5/5 - 5s - loss: 0.9051 - sparse_categorical_accuracy: 0.7719 - val_loss: 0.6922 - val_sparse_categorical_accuracy: 0.8198 . . def plot_hist(hist): plt.plot(history.history[&#39;loss&#39;]) plt.plot(history.history[&#39;val_loss&#39;]) plt.title(&#39;Loss over epochs&#39;) plt.ylabel(&#39;loss&#39;) plt.xlabel(&#39;epoch&#39;) plt.legend([&#39;train&#39;, &#39;valid&#39;], loc=&#39;best&#39;) plt.show() def display_training_curves(training, validation, title, subplot): if subplot%10==1: # set up the subplots on the first call plt.subplots(figsize=(10,10), facecolor=&#39;#F0F0F0&#39;) plt.tight_layout() ax = plt.subplot(subplot) ax.set_facecolor(&#39;#F8F8F8&#39;) ax.plot(training) ax.plot(validation) ax.set_title(&#39;model &#39;+ title) ax.set_ylabel(title) #ax.set_ylim(0.28,1.05) ax.set_xlabel(&#39;epoch&#39;) ax.legend([&#39;train&#39;, &#39;valid.&#39;]) import matplotlib.pyplot as plt from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix def display_confusion_matrix(cmat, score, precision, recall): plt.figure(figsize=(15,15)) ax = plt.gca() ax.matshow(cmat, cmap=&#39;Reds&#39;) ax.set_xticks(range(len(CLASSES))) ax.set_xticklabels(CLASSES, fontdict={&#39;fontsize&#39;: 7}) plt.setp(ax.get_xticklabels(), rotation=45, ha=&quot;left&quot;, rotation_mode=&quot;anchor&quot;) ax.set_yticks(range(len(CLASSES))) ax.set_yticklabels(CLASSES, fontdict={&#39;fontsize&#39;: 7}) plt.setp(ax.get_yticklabels(), rotation=45, ha=&quot;right&quot;, rotation_mode=&quot;anchor&quot;) titlestring = &quot;&quot; if score is not None: titlestring += &#39;f1 = {:.3f} &#39;.format(score) if precision is not None: titlestring += &#39; nprecision = {:.3f} &#39;.format(precision) if recall is not None: titlestring += &#39; nrecall = {:.3f} &#39;.format(recall) if len(titlestring) &gt; 0: ax.text(101, 1, titlestring, fontdict={&#39;fontsize&#39;: 18, &#39;horizontalalignment&#39;:&#39;right&#39;, &#39;verticalalignment&#39;:&#39;top&#39;, &#39;color&#39;:&#39;#804040&#39;}) plt.show() def display_training_curves(training, validation, title, subplot): if subplot%10==1: # set up the subplots on the first call plt.subplots(figsize=(10,10), facecolor=&#39;#F0F0F0&#39;) plt.tight_layout() ax = plt.subplot(subplot) ax.set_facecolor(&#39;#F8F8F8&#39;) ax.plot(training) ax.plot(validation) ax.set_title(&#39;model &#39;+ title) ax.set_ylabel(title) #ax.set_ylim(0.28,1.05) ax.set_xlabel(&#39;epoch&#39;) ax.legend([&#39;train&#39;, &#39;valid.&#39;]) . display_training_curves( history.history[&#39;loss&#39;], history.history[&#39;val_loss&#39;], &#39;loss&#39;, 211, ) display_training_curves( history.history[&#39;sparse_categorical_accuracy&#39;], history.history[&#39;val_sparse_categorical_accuracy&#39;], &#39;accuracy&#39;, 212, ) . cmat_ds = tf.data.TFRecordDataset(valid_filenames, num_parallel_reads=AUTOTUNE) cmat_ds = (cmat_ds .map(collate_labeled_tfrecord) .batch(BATCH_SIZE) .cache() .prefetch(AUTOTUNE)) images_ds = cmat_ds.map(lambda image, label: image) labels_ds = cmat_ds.map(lambda image, label: label).unbatch() . cm_correct_labels = next(iter(labels_ds.batch(count_data_items(valid_filenames)))).numpy() cm_probabilities = model.predict(images_ds) cm_predictions = np.argmax(cm_probabilities, axis=-1) labels = range(len(CLASSES)) cmat = confusion_matrix( cm_correct_labels, cm_predictions, labels=labels, ) cmat = (cmat.T / cmat.sum(axis=1)).T # normalize . You might be familiar with metrics like F1-score or precision and recall. This cell will compute these metrics and display them with a plot of the confusion matrix. (These metrics are defined in the Scikit-learn module sklearn.metrics; we&#39;ve imported them in the helper script for you.) . score = f1_score( cm_correct_labels, cm_predictions, labels=labels, average=&#39;macro&#39;, ) precision = precision_score( cm_correct_labels, cm_predictions, labels=labels, average=&#39;macro&#39;, ) recall = recall_score( cm_correct_labels, cm_predictions, labels=labels, average=&#39;macro&#39;, ) display_confusion_matrix(cmat, score, precision, recall) . Visual Validation . It can also be helpful to look at some examples from the validation set and see what class your model predicted. This can help reveal patterns in the kinds of images your model has trouble with. This cell will set up the validation set to display 20 images at a time -- you can change this to display more or fewer, if you like. . 1% Better Everyday . reference . Create Your First Submission | How to use my own data source? | TPU-speed data pipelines: tf.data.Dataset and TFRecords | . . todos . Comment out the 1/255.0 in the image preprocessing | Reorganize the notebook structure | . . done .",
            "url": "https://austinyhc.github.io/blog/plant/classification/tpu/2020/12/24/petals-to-the-metal.html",
            "relUrl": "/plant/classification/tpu/2020/12/24/petals-to-the-metal.html",
            "date": " • Dec 24, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Continuous Self Motivation",
            "content": "The Four Big Ideas . Habits are the compound interest of self-improvement. | If you want better results, then forget about setting goals. Focus on your system instead. | The most effective way to change your habits is to focus not on what you want to achieve, but on who you wish to become. | The Four Laws of Behavior Change are a simple set of rules we can use to build better habits. They are make it obvious | make it attractive | make it easy | make it satisfying. | . | . I. Customize 1cycle . This learning rate scheduler allows us to easily train a network using Leslie Smith&#39;s 1cycle policy. To learn more about the 1cycle technique for training neural networks check out Leslie Smith&#39;s paper and for more graphical and intuitive explanation checkout out Sylvain Gugger&#39;s post. . To use 1cycle policy we will need an optimum learning rate. We can find this learning rate by using a learning finder which can be called by using lr_finder as fastai does. It will do a mock training by going over a large range of learning rates, then plot them against the losses. We will then pick a value a bit before the minimum, where the loss still improves. Our graph would something like this: . . There is somthing to add, if we are transfer learning, we do not want to start off with too large a learning rate, or we will erase the intelligence of the model already contained in its weights. Instead, we begin with a very small learning rate and increase it gradually before lowering it again to fine-tune the weights. . . Important: After digging into the rabbit hole, I found there are two different learning rate schedule utility in tensorflow, the naming is very confusing, keras.optimizers.schedules.LearningRateSchedule and keras.callbacks.LearningRateScheduler. Although the naming is very similar, they are different in some senses. - The former is subclassing from tf.keras.optimizers while the latter is from tf.keras.Callback . The former schedule the learning rate per iteration while the former is per epoch. | . II. EfficientNet . (Read the EfficientNet paper and summarize in one of the section of this notebook) . EfficientNet, first introduced in Tan and Le, 2019 is among the most efficient models (i.e. requiring least FLOPS for inference) that reaches state-of-the-art accracy on both imagenet and common image classification transfer learning tasks. . The smallest base model is similar to MnasNet, which reached near-SOTA with a significantly smaller model. By introducing a heuristic way to scale the model, EfficientNet provides a family of models (B0 to B7) that represents a good combination of efficiency and accuracy on a variety of scales. Such a scaling heuristics (compound-scaling, details see Tan and Le, 2019) allows the efficiency-oriented base model (B0) to surpass models at every scale, while avoiding extensive grid-search of hyperparameters. . A summary of the latest updates on the model is available at here, where various augmentation schemes and semi-supervised learning approaches are applied to further improve the imagenet performance of the models. These extensions of the model can be used by updating weights without changing model topology . B0 to B7 variats of EfficientNet . Keras implementation of EfficientNet . An implementation of EfficientNet B0 to B7 has been shipped with tf.keras since TF2.3. To use EfficientNetB0 for classifying 1000 classes of images from imagenet, run: . from tensorflow.keras.applications import EfficientNetB0 model = EfficientNetB0(weights=&#39;imagenet&#39;) . The B0 model takes input images of shape (224,224,3), and the input data should range [0,255]. Normailzation is included as part of the model. . Because training EfficientNet on imagenet takes a tremendous amount of resources and several techniques that are not a part of the model architecture itself. Hence the Keras implementations by default loads pre-trained weights obtained via training with AutoAugment. . From B0 to B7 base model, the input shapes are different. Here is a list of input shpae expected for each model: . Base model resolution . EfficientNetB0 | 224 | . EfficientNetB1 | 240 | . EfficientNetB2 | 260 | . EfficientNetB3 | 300 | . EfficientNetB4 | 380 | . EfficientNetB5 | 456 | . EfficientNetB6 | 528 | . EfficientNetB7 | 600 | . When the model is intended for transfer learning, the Keras implementation provides a option to remove the top layers: . model = EfficientNetB0(include_top=False, weights=&#39;imagenet&#39;) . This option excludes the final Dense layer that turns 1280 features on the penultimate layer into prediction of the 1000 ImageNet classes. Replacing the top layer with custom layers allows using EfficientNet as a feature extractor in a transfer learning workflow. . Another argument in the model constructor worth noticing is drop_connect_rate which controls the dropout rate responsible for stochastic depth. This parameter serves as a toggle for extra regularization in finetuning, but does not affect loaded weights. For example, when stronger regularization is desired, try: . model = EfficientNetB0(weights=&#39;imagenet&#39;, drop_connect_rate=0.4) . The default value for drop_connect_rate is 0. . Clarification . AutoAugment . In this article, in section Keras implementation of EfficientNet, it says . Because training EfficientNet on ImageNet takes a tremendous amount of resources and several techniques that are not a part of the model architecture itself. Hence the Keras implementation by default loads pre-trained weights obtained via training with AutoAugment . It means the weights of keras EfficientNet are trained on the pre-trained from AutoAugment. My follow-up question is what dataset does the AutoAugment trained on? .",
            "url": "https://austinyhc.github.io/blog/personal%20development/motivation/habit/2020/12/24/continuous-self-motivation.html",
            "relUrl": "/personal%20development/motivation/habit/2020/12/24/continuous-self-motivation.html",
            "date": " • Dec 24, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Cassava Leaf Disease Classification",
            "content": "Preliminaries . This notebook is a simple training pipeline in TensorFlow for the Cassava Leaf Competition where we are given 21,397 labeled images of cassava leaves classified as 5 different groups (4 diseases and a healthy group) and asked to predict on unseen images of cassava leaves. As with most image classification problems, we can use and experiment with many different forms of augmentation and we can explore transfer learning. . . Note: I am using Dimitre&#8217;s TFRecords that can be found here. He also has 128x128, 256x256, and 384x384 sized images that I added for experimental purposes. Please give his datasets an upvote (and his work in general, it is excellent). . Dependencies . import numpy as np import pandas as pd import seaborn as sns import albumentations as A import matplotlib.pyplot as plt import os, gc, cv2, random, warnings, math, sys, json, pprint, pdb import tensorflow as tf from tensorflow.keras import backend as K import tensorflow_hub as hub from sklearn.model_selection import train_test_split . Setup . DEVICE = &#39;GPU&#39; #@param [&quot;None&quot;, &quot;&#39;GPU&#39;&quot;, &quot;&#39;TPU&#39;&quot;] {type:&quot;raw&quot;, allow-input: true} if DEVICE == &quot;TPU&quot;: print(&quot;connecting to TPU...&quot;) try: tpu = tf.distribute.cluster_resolver.TPUClusterResolver() print(&#39;Running on TPU &#39;, tpu.master()) except ValueError: print(&quot;Could not connect to TPU&quot;) tpu = None if tpu: try: print(&quot;initializing TPU ...&quot;) tf.config.experimental_connect_to_cluster(tpu) tf.tpu.experimental.initialize_tpu_system(tpu) strategy = tf.distribute.experimental.TPUStrategy(tpu) print(&quot;TPU initialized&quot;) except _: print(&quot;failed to initialize TPU&quot;) else: DEVICE = &quot;GPU&quot; if DEVICE != &quot;TPU&quot;: print(&quot;Using default strategy for CPU and single GPU&quot;) strategy = tf.distribute.get_strategy() if DEVICE == &quot;GPU&quot;: print(&quot;Num GPUs Available: &quot;, len(tf.config.experimental.list_physical_devices(&#39;GPU&#39;))) AUTOTUNE = tf.data.experimental.AUTOTUNE REPLICAS = strategy.num_replicas_in_sync print(f&#39;REPLICAS: {REPLICAS}&#39;) . Using default strategy for CPU and single GPU Num GPUs Available: 0 REPLICAS: 1 . def seed_everything(seed=0): random.seed(seed) np.random.seed(seed) tf.random.set_seed(seed) os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) os.environ[&#39;TF_DETERMINISTIC_OPS&#39;] = &#39;1&#39; def is_colab(): return &#39;google.colab&#39; in str(get_ipython()) . . Tip: Adding seed helps reproduce results. Setting debug parameter wil run the model on smaller number of epochs to validate the architecture. . #@title Debugger { run: &quot;auto&quot; } SEED = 16 DEBUG = False #@param {type:&quot;boolean&quot;} TRAIN = True #@param {type:&quot;boolean&quot;} INFERENCE = True #@param {type:&quot;boolean&quot;} IS_COLAB = is_colab() warnings.simplefilter(&#39;ignore&#39;) seed_everything(SEED) print(f&quot;Using TensorFlow v{tf.__version__}&quot;) . Using TensorFlow v2.4.0 . if IS_COLAB: from google.colab import drive drive.mount(&#39;/content/gdrive&#39;, force_remount=True) . Mounted at /content/gdrive . root_path = &#39;/content/gdrive/MyDrive/&#39; if IS_COLAB else &#39;/&#39; input_path = f&#39;{root_path}kaggle/input/cassava-leaf-disease-classification/&#39; working_path = f&#39;{input_path}working/&#39; if IS_COLAB else &#39;/kaggle/working/&#39; os.makedirs(working_path, exist_ok=True) os.chdir(working_path) os.listdir(input_path) . [&#39;label_num_to_disease_map.json&#39;, &#39;sample_submission.csv&#39;, &#39;train.csv&#39;, &#39;cassava-leaf-disease-classification.zip&#39;, &#39;test_images&#39;, &#39;test_tfrecords&#39;, &#39;train_images&#39;, &#39;train_tfrecords&#39;, &#39;dump.tfcache.data-00000-of-00001&#39;, &#39;dump.tfcache.index&#39;, &#39;working&#39;] . EDA . df = pd.read_csv(f&#39;{input_path}train.csv&#39;) . df.head() . image_id label . 0 1000015157.jpg | 0 | . 1 1000201771.jpg | 3 | . 2 100042118.jpg | 1 | . 3 1000723321.jpg | 1 | . 4 1000812911.jpg | 3 | . Check how many images are available in the training dataset and also check if each item in the training set are unique . print(f&quot;There are {len(df)} train images&quot;) len(df.image_id) == len(df.image_id.unique()) . There are 21397 train images . True . (df.label.value_counts(normalize=True) * 100).plot.barh(figsize = (8, 5)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f264034a080&gt; . df[&#39;filename&#39;] = df[&#39;image_id&#39;].map(lambda x : f&#39;{input_path}train_images/{x}&#39;) df = df.drop(columns = [&#39;image_id&#39;]) df = df.sample(frac=1).reset_index(drop=True) . df.head() . label filename . 0 3 | /content/gdrive/MyDrive/kaggle/input/cassava-l... | . 1 3 | /content/gdrive/MyDrive/kaggle/input/cassava-l... | . 2 3 | /content/gdrive/MyDrive/kaggle/input/cassava-l... | . 3 3 | /content/gdrive/MyDrive/kaggle/input/cassava-l... | . 4 3 | /content/gdrive/MyDrive/kaggle/input/cassava-l... | . if DEBUG: _, df = train_test_split( df, test_size = 0.1, random_state=SEED, shuffle=True, stratify=df[&#39;label&#39;]) . with open(f&#39;{input_path}label_num_to_disease_map.json&#39;) as file: id2label = json.loads(file.read()) id2label . {&#39;0&#39;: &#39;Cassava Bacterial Blight (CBB)&#39;, &#39;1&#39;: &#39;Cassava Brown Streak Disease (CBSD)&#39;, &#39;2&#39;: &#39;Cassava Green Mottle (CGM)&#39;, &#39;3&#39;: &#39;Cassava Mosaic Disease (CMD)&#39;, &#39;4&#39;: &#39;Healthy&#39;} . In this case, we have 5 labels (4 diseases and healthy): . Cassava Bacterial Blight (CBB) | Cassava Brown Streak Disease (CBSD) | Cassava Green Mottle (CGM) | Cassava Mosaic Disease (CMD) | Healthy | In this case label 3, Cassava Mosaic Disease (CMD) is the most common label. This imbalance may have to be addressed with a weighted loss function or oversampling. I might try this in a future iteration of this kernel or in a new kernel. . Let&#39;s check an example image to see what it looks like . from PIL import Image img = Image.open(df[df.label==3][&#39;filename&#39;].iloc[0]) . width, height = img.size print(f&quot;Width: {width}, Height: {height}&quot;) . Width: 800, Height: 600 . img . EfficientNet . Configuration . BASE_MODEL, IMG_SIZE = (&#39;efficientnet_b3&#39;, 300) #@param [&quot;(&#39;efficientnet_b3&#39;, 300)&quot;, &quot;(&#39;efficientnet_b4&#39;, 380)&quot;, &quot;(&#39;efficientnet_b2&#39;, 260)&quot;] {type:&quot;raw&quot;, allow-input: true} BATCH_SIZE = 32 #@param {type:&quot;integer&quot;} IMG_SIZE = (IMG_SIZE, IMG_SIZE) #@param [&quot;(IMG_SIZE, IMG_SIZE)&quot;, &quot;(512,512)&quot;] {type:&quot;raw&quot;} print(&quot;Using {} with input size {}&quot;.format(BASE_MODEL, IMG_SIZE)) . Using efficientnet_b3 with input size (300, 300) . Loading data . After my quick and rough EDA, let&#39;s load the PIL Image to a Numpy array, so we can move on to data augmentation. . In fastai, they have item_tfms and batch_tfms defined for their data loader API. The item transforms performs a fairly large crop to 224 and also apply other standard augmentations (in aug_tranforms) at the batch level on the GPU. The batch size is set to 32 here. . Splitting . train_df, valid_df = train_test_split( df ,test_size = 0.2 ,random_state = SEED ,shuffle = True ,stratify = df[&#39;label&#39;]) . Constructing Dataset . train_ds = tf.data.Dataset.from_tensor_slices( (train_df.filename.values,train_df.label.values)) valid_ds = tf.data.Dataset.from_tensor_slices( (valid_df.filename.values, valid_df.label.values)) adapt_ds = tf.data.Dataset.from_tensor_slices( train_df.filename.values) . for x,y in valid_ds.take(3): print(x, y) . tf.Tensor(b&#39;/content/gdrive/MyDrive/kaggle/input/cassava-leaf-disease-classification/train_images/2484271873.jpg&#39;, shape=(), dtype=string) tf.Tensor(4, shape=(), dtype=int64) tf.Tensor(b&#39;/content/gdrive/MyDrive/kaggle/input/cassava-leaf-disease-classification/train_images/3704210007.jpg&#39;, shape=(), dtype=string) tf.Tensor(4, shape=(), dtype=int64) tf.Tensor(b&#39;/content/gdrive/MyDrive/kaggle/input/cassava-leaf-disease-classification/train_images/1655615998.jpg&#39;, shape=(), dtype=string) tf.Tensor(2, shape=(), dtype=int64) . . Important: At this point, you may have noticed that I have not used any kind of normalization or rescaling. I recently discovered that there is Normalization layer included in Keras&#8217; pretrained EfficientNet, as mentioned here. . Item transformation . Basically item transformations mainly make sure the input data is of the same size so that it can be collated in batches. . def decode_image(filename): img = tf.io.read_file(filename) img = tf.image.decode_jpeg(img, channels=3) return img def collate_train(filename, label): img = decode_image(filename) img = tf.image.random_brightness(img, 0.3) img = tf.image.random_flip_left_right(img, seed=None) img = tf.image.random_crop(img, size=[*IMG_SIZE, 3]) return img, label def process_adapt(filename): img = decode_image(filename) img = tf.keras.layers.experimental.preprocessing.Rescaling(1.0 / 255)(img) return img def collate_valid(filename, label): img = decode_image(filename) img = tf.image.resize(img, [*IMG_SIZE]) return img, label . train_ds = train_ds.map(collate_train, num_parallel_calls=AUTOTUNE) valid_ds = valid_ds.map(collate_valid, num_parallel_calls=AUTOTUNE) adapt_ds = adapt_ds.map(process_adapt, num_parallel_calls=AUTOTUNE) . def show_images(ds): _,axs = plt.subplots(4,6,figsize=(24,16)) for ((x, y), ax) in zip(ds.take(24), axs.flatten()): ax.imshow(x.numpy().astype(np.uint8)) ax.set_title(np.argmax(y)) ax.axis(&#39;off&#39;) . show_images(train_ds) . show_images(valid_ds) . Batching Dataset . . Note: I was shuffing the validation set which is a bug . train_ds_batch = (train_ds .cache(&#39;dump.tfcache&#39;) .shuffle(buffer_size=1000) .batch(BATCH_SIZE) .prefetch(buffer_size=AUTOTUNE)) valid_ds_batch = (valid_ds #.shuffle(buffer_size=1000) .batch(BATCH_SIZE*2) .prefetch(buffer_size=AUTOTUNE)) adapt_ds_batch = (adapt_ds .shuffle(buffer_size=1000) .batch(BATCH_SIZE) .prefetch(buffer_size=AUTOTUNE)) . Batch augmentation . data_augmentation = tf.keras.Sequential( [ tf.keras.layers.experimental.preprocessing.RandomCrop(*IMG_SIZE), tf.keras.layers.experimental.preprocessing.RandomFlip(&quot;horizontal_and_vertical&quot;), tf.keras.layers.experimental.preprocessing.RandomRotation(0.25), tf.keras.layers.experimental.preprocessing.RandomZoom((-0.2, 0)), tf.keras.layers.experimental.preprocessing.RandomContrast((0.2,0.2)) ] ) . func = lambda x,y: (data_augmentation(x), y) x = (train_ds .batch(BATCH_SIZE) .take(1) .map(func, num_parallel_calls=AUTOTUNE)) . show_images(x.unbatch()) . Building a model . I am using an EfficientNetB3 on top of which I add some output layers to predict our 5 disease classes. I decided to load the imagenet pretrained weights locally to keep the internet off (part of the requirements to submit a kernal to this competition). . from tensorflow.keras.applications import EfficientNetB3 . efficientnet = EfficientNetB3( weights = &#39;imagenet&#39; if TRAIN else None, include_top = False, input_shape = (*IMG_SIZE, 3), pooling=&#39;avg&#39;) . def build_model(base_model, num_class): inputs = tf.keras.layers.Input(shape=(*IMG_SIZE, 3)) x = data_augmentation(inputs) x = base_model(x) x = tf.keras.layers.Dropout(0.4)(x) outputs = tf.keras.layers.Dense(num_class, activation=&quot;softmax&quot;, name=&quot;pred&quot;)(x) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) return model . model = build_model(base_model=efficientnet, num_class=len(id2label)) . model.summary() . Model: &#34;model_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_5 (InputLayer) [(None, 300, 300, 3)] 0 _________________________________________________________________ sequential_2 (Sequential) (None, 300, 300, 3) 0 _________________________________________________________________ efficientnetb3 (Functional) (None, 1536) 10783535 _________________________________________________________________ dropout_1 (Dropout) (None, 1536) 0 _________________________________________________________________ pred (Dense) (None, 5) 7685 ================================================================= Total params: 10,791,220 Trainable params: 10,703,917 Non-trainable params: 87,303 _________________________________________________________________ . Fine tune . The 3rd layer of the Efficient is the Normalization layer, which can be tuned to our new dataset instead of imagenet. Be patient on this one, it does take a bit of time as we&#39;re going through the entire training set. . %%time if TRAIN: if not os.path.exists(f&quot;{working_path}000_normalization.h5&quot;): model.get_layer(&#39;efficientnetb3&#39;).get_layer(&#39;normalization&#39;).adapt(adapt_ds_batch) model.save_weights(&quot;000_normalization.h5&quot;) else: model.load_weights(&quot;000_normalization.h5&quot;) . CPU times: user 4 µs, sys: 0 ns, total: 4 µs Wall time: 6.68 µs . Optimizer . CosineDecay . . Important: I always wanted to try the new CosineDecayRestarts function implemented in tf.keras as it seemed promising and I struggled to find the right settings (if there were any) for the ReduceLROnPlateau . EPOCHS = 8 STEPS = int(round(len(train_df)/BATCH_SIZE)) * EPOCHS schedule = tf.keras.experimental.CosineDecayRestarts( initial_learning_rate=1e-4, first_decay_steps=300 ) . schedule.get_config() . {&#39;alpha&#39;: 0.0, &#39;first_decay_steps&#39;: 300, &#39;initial_learning_rate&#39;: 0.0001, &#39;m_mul&#39;: 1.0, &#39;name&#39;: None, &#39;t_mul&#39;: 2.0} . x = [i for i in range(STEPS)] y = [schedule(s) for s in range(STEPS)] plt.plot(x, y) . [&lt;matplotlib.lines.Line2D at 0x7f264156fc18&gt;] . . Warning: There is a gap between what I had expected and the acutal LearningRateScheduler that tensorflow gives us. The LearningRateScheduler update the lr on_epoch_begin while it makes more sense to do it on_batch_end or on_batch_begin. . Callbacks . callbacks = [ tf.keras.callbacks.ModelCheckpoint( filepath=&#39;001_best_model.h5&#39;, monitor=&#39;val_loss&#39;, save_best_only=True), ] model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(schedule), metrics=[&quot;accuracy&quot;]) . Training . if TRAIN: history = model.fit(train_ds_batch, epochs = EPOCHS, validation_data=valid_ds_batch, callbacks=callbacks) . Epoch 1/8 12/54 [=====&gt;........................] - ETA: 21:15 - loss: 0.4106 - accuracy: 0.8684 . Evaluating . def plot_hist(hist): plt.plot(history.history[&#39;loss&#39;]) plt.plot(history.history[&#39;val_loss&#39;]) plt.title(&#39;Loss over epochs&#39;) plt.ylabel(&#39;loss&#39;) plt.xlabel(&#39;epoch&#39;) plt.legend([&#39;train&#39;, &#39;valid&#39;], loc=&#39;best&#39;) plt.show() . if TRAIN: plot_hist(history) . We load the best weight that were kept from the training phase. Just to check how our model is performing, we will attempt predictions over the validation set. This can help to highlight any classes that will be consistently miscategorised. . model.load_weights(&#39;001_best_model.h5&#39;) . Prediction . x = train_df.sample(1).filename.values[0] img = decode_image(x) . %%time imgs = [tf.image.random_crop(img, size=[*IMG_SIZE, 3]) for _ in range(4)] _,axs = plt.subplots(1,4,figsize=(16,4)) for (x, ax) in zip(imgs, axs.flatten()): ax.imshow(x.numpy().astype(np.uint8)) ax.axis(&#39;off&#39;) . CPU times: user 57.3 ms, sys: 870 µs, total: 58.2 ms Wall time: 62.1 ms . I apply some very basic test time augmentation to every local image extracted from the original 600-by-800 images. We know we can do some fancy augmentation with albumentations but I wanted to do that exclusively with Keras preprocessing layers to keep the cleanest pipeline possible. . tta = tf.keras.Sequential( [ tf.keras.layers.experimental.preprocessing.RandomCrop((*IMG_SIZE)), tf.keras.layers.experimental.preprocessing.RandomFlip(&quot;horizontal_and_vertical&quot;), tf.keras.layers.experimental.preprocessing.RandomZoom((-0.2, 0.2)), tf.keras.layers.experimental.preprocessing.RandomContrast((0.2,0.2)) ] ) . def predict_tta(filename, num_tta=4): img = decode_image(filename) img = tf.expand_dims(img, 0) imgs = tf.concat([tta(img) for _ in range(num_tta)], 0) preds = model.predict(imgs) return preds.sum(0).argmax() . pred = predict_tta(df.sample(1).filename.values[0]) print(pred) . 3 . if INFERENCE: from tqdm import tqdm preds = [] with tqdm(total=len(valid_df)) as pbar: for filename in valid_df.filename: pbar.update() preds.append(predict_tta(filename, num_tta=4)) . 100%|██████████| 4280/4280 [25:34&lt;00:00, 2.79it/s] . if INFERENCE: cm = tf.math.confusion_matrix(valid_df.label.values, np.array(preds)) plt.figure(figsize=(10, 8)) sns.heatmap(cm, xticklabels=id2label.values(), yticklabels=id2label.values(), annot=True, fmt=&#39;g&#39;, cmap=&quot;Blues&quot;) plt.xlabel(&#39;Prediction&#39;) plt.ylabel(&#39;Label&#39;) plt.show() . test_folder = input_path + &#39;/test_images/&#39; submission_df = pd.DataFrame(columns={&quot;image_id&quot;,&quot;label&quot;}) submission_df[&quot;image_id&quot;] = os.listdir(test_folder) submission_df[&quot;label&quot;] = 0 . submission_df[&#39;label&#39;] = (submission_df[&#39;image_id&#39;] .map(lambda x : predict_tta(test_folder+x))) . submission_df . image_id label . 0 2216849948.jpg | 4 | . submission_df.to_csv(&quot;submission.csv&quot;, index=False) . 1% Better Everyday . reference . https://www.kaggle.com/c/cassava-leaf-disease-classification | https://www.kaggle.com/dimitreoliveira/cassava-leaf-disease-training-with-tpu-v2-pods/notebook#Training-data-samples-(with-augmentation) | https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/#keras-implementation-of-efficientnet | https://www.tensorflow.org/guide/gpu_performance_analysis | https://www.tensorflow.org/guide/data_performance#prefetching | https://www.tensorflow.org/guide/data_performance_analysis | . . todos . See if I can integrate the Cutmix/Mixup augmentations in the appendix into our existing notebook. This is an excellent example | Still want to figure out some intuition of item aug and batch aug. I don&#39;t know, maybe there is some limitation or how to do so to help to speed up. | Learn more about the adapt function that being used to retrain the normalization layer of the EfficientNetB3. | . . done . Predict in batch to speed up | Add a cell for checkbox parameter to select between kaggle and colab, default is Kaggle. | Try out the data_generator and the data_frame_iterator | Removing normalizaiton step in generator since in EfficientNet, normalization is done within the model itself and the model expects input in the range of [0,255] | Find out the intuition and the difference between item_tfm and batch_tfm . In fastai, item_tfm defines the transforms that are done on the CPU and batch_tfm defines those done on the GPU. . | Customize my own data generator as fastai creates their Dataloader . No need, things are much easier than what I was originally expecting. Please refer to the Loading data section in this notebook. . | The 3rd layer of the Efficientnet is the Normalization layer, which can be tuned to our new dataset instead of imagenet. Be patient on this one, it does take a bit of time we&#39;re going through the entire training set. . | Add seed_everything function | . Appendix . The albumentation is primarily used for resizing and normalization. . def albu_transforms_train(data_resize): return A.Compose([ A.ToFloat(), A.Resize(data_resize, data_resize), ], p=1.) # For Validation def albu_transforms_valid(data_resize): return A.Compose([ A.ToFloat(), A.Resize(data_resize, data_resize), ], p=1.) . def CutMix(image, label, DIM, PROBABILITY = 1.0): # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3] # output - a batch of images with cutmix applied CLASSES = 5 imgs = []; labs = [] for j in range(len(image)): # DO CUTMIX WITH PROBABILITY DEFINED ABOVE P = tf.cast( tf.random.uniform([],0,1)&lt;=PROBABILITY, tf.int32) # CHOOSE RANDOM IMAGE TO CUTMIX WITH k = tf.cast( tf.random.uniform([],0,len(image)),tf.int32) # CHOOSE RANDOM LOCATION x = tf.cast( tf.random.uniform([],0,DIM),tf.int32) y = tf.cast( tf.random.uniform([],0,DIM),tf.int32) b = tf.random.uniform([],0,1) # this is beta dist with alpha=1.0 WIDTH = tf.cast( DIM * tf.math.sqrt(1-b),tf.int32) * P ya = tf.math.maximum(0,y-WIDTH//2) yb = tf.math.minimum(DIM,y+WIDTH//2) xa = tf.math.maximum(0,x-WIDTH//2) xb = tf.math.minimum(DIM,x+WIDTH//2) # MAKE CUTMIX IMAGE one = image[j,ya:yb,0:xa,:] two = image[k,ya:yb,xa:xb,:] three = image[j,ya:yb,xb:DIM,:] middle = tf.concat([one,two,three],axis=1) img = tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM,:,:]],axis=0) imgs.append(img) # MAKE CUTMIX LABEL a = tf.cast(WIDTH*WIDTH/DIM/DIM,tf.float32) labs.append((1-a)*label[j] + a*label[k]) # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?) image2 = tf.reshape(tf.stack(imgs),(len(image),DIM,DIM,3)) label2 = tf.reshape(tf.stack(labs),(len(image),CLASSES)) return image2,label2 . def MixUp(image, label, DIM, PROBABILITY = 1.0): # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3] # output - a batch of images with mixup applied CLASSES = 5 imgs = []; labs = [] for j in range(len(image)): # DO MIXUP WITH PROBABILITY DEFINED ABOVE P = tf.cast( tf.random.uniform([],0,1)&lt;=PROBABILITY, tf.float32) # CHOOSE RANDOM k = tf.cast( tf.random.uniform([],0,len(image)),tf.int32) a = tf.random.uniform([],0,1)*P # this is beta dist with alpha=1.0 # MAKE MIXUP IMAGE img1 = image[j,] img2 = image[k,] imgs.append((1-a)*img1 + a*img2) # MAKE CUTMIX LABEL labs.append((1-a)*label[j] + a*label[k]) # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?) image2 = tf.reshape(tf.stack(imgs),(len(image),DIM,DIM,3)) label2 = tf.reshape(tf.stack(labs),(len(image),CLASSES)) return image2,label2 . .",
            "url": "https://austinyhc.github.io/blog/plant/disease/classification/efficientnet/2020/12/24/cassava-leaf-disease-classification.html",
            "relUrl": "/plant/disease/classification/efficientnet/2020/12/24/cassava-leaf-disease-classification.html",
            "date": " • Dec 24, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am Austin; after years plunging into Smartphone Industry and IC Design, I have found data science is crucial in every aspect possible. Thanks to Jeremy Howard, the founder of Fast.ai, he re-enlightens my passion for Machine Learning, especially Deep Learning and Transfer Learning. I have freelanced with different organizations and continuously strive to apply DL to any project, team, or goal. Hope to meet a ton of you in the sphere of AI and to contribute as best as I can to your community. .",
          "url": "https://austinyhc.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://austinyhc.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}