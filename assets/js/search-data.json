{
  
    
        "post0": {
            "title": "The Real World is not a Kaggle Competition",
            "content": "Outline . introduction | over-engineering | the best performance | be mindful of your surroundings | the trade-off | getting it just right | minimalism | . . Collecting voices in my brain . to me Kaggle competetion is only a part of a real-lift data science project. In real life there are multiple other aspects that Kaggle doesn&#39;t touch on. Kagggle is a perfect platform for hosting and learning specific skills, but a real-life science problems are usually much bigger challengesfso the distinction is worth making. | . . Reference . https://www.quora.com/How-similar-are-Kaggle-competitions-to-what-data-scientists-do | . Note: Draft 1: In my experience, the most challenging part is to convince the Business Users (Board of Directors and other C-level executives) and merchants to follow through your solution. You really need to have a data backed strong analysis result to persuade them that your solution is indeed better than their business acumen, which they have inculcated over the years. And in such a scenario, black-box type models involving cool stacking and blending which is used to climb up the leader-board, often fail. Business Users can get quite defensive and be reluctant to take ahead your analysis, just because it achieves 95% accuracy! So most of the time, what your want is a white-box algorithm with a reasonable accuracy, say 90%, is easy to follow and explainable in business terms. . . Note: Draft 2 for over-engineering: To give a concrete example, let&#8217;s follow through the Promotion Effectiveness Analysis which is a classic problem for any Retailer. Usually what htey want out of the exercise is some way to quantify the effectiveness of Promotion X which they ran and a list of products which are likely to perform well under it. Now let&#8217;s assume two data scientist are given with this task. You being a pro in Kaggle, use a lot of ensembling, blending and stacking and now the new trend of incorporating data leaks and other cool hacks up your sleeve to make your model most accurate in terms of perdicting which products are expected to perform well under Promotion X. But you should mentally prepare yourself before hand, for you might be turned down by the Business because these usually influence million-dollar decisions and they have a hard time relying on a black-box! . Introduction . Machine Learning and Deep Learning are evolving at a faster pace than ever since early 2010s. . . Note: Draft 1 for Introduction, the do not get me wrong tone which is inspired by this Quora reply: I am in no way undermining the capabiliteis you can develop by participanting in these excellent Data Science competitions. In fact, kaggle teaches you feature engineering really well! But what I want you know, is that these problems are subset of what you will face in real-life as a data scientist. There will be cases when you will benefit from all the cool hacks you learn in these platforms for a solution that doesn&#8217;t require a peek into what&#8217;s going on inside the bos. There will be some which will require you to discover, gain confidence and slowly move to black-box. The key is in understanding the situation, business problem at hand, expectation of stakeholders and some how balancing all these, yet rendering out a reasonable accuracy. . The Best Performance . Be Mindful of Your Surroundings . The Trade-Off . Getting the Size Just Right . Minimal Model .",
            "url": "https://austinyhc.github.io/blog/deep%20learning/2021/01/06/the-real-world-is-not-a-kaggle-competition.html",
            "relUrl": "/deep%20learning/2021/01/06/the-real-world-is-not-a-kaggle-competition.html",
            "date": " • Jan 6, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Jane Street Market Prediction",
            "content": "Dependencies . import numpy as np import pandas as pd import seaborn as sns import albumentations as A import matplotlib.pyplot as plt import os, gc, cv2, random, warnings, math, sys, json, pprint, pdb import tensorflow as tf from tensorflow.keras import backend as K import tensorflow_hub as hub from sklearn.model_selection import train_test_split import dabl import datatable as dt warnings.simplefilter(&#39;ignore&#39;) print(f&quot;Using TensorFlow v{tf.__version__}&quot;) . . Using TensorFlow v2.4.0 . def seed_everything(seed=0): random.seed(seed) np.random.seed(seed) tf.random.set_seed(seed) os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) os.environ[&#39;TF_DETERMINISTIC_OPS&#39;] = &#39;1&#39; GOOGLE = &#39;google.colab&#39; in str(get_ipython()) KAGGLE = not GOOGLE print(&quot;Running on {}!&quot;.format( &quot;Google Colab&quot; if GOOGLE else &quot;Kaggle Kernel&quot; )) . . Running on Google Colab! . We can observe that the train.csv is large: 6GB and it has 2390492 rows in the file. . 2390492 /content/gdrive/MyDrive/kaggle/input/jane-street-market-prediction/train.csv . To speed things up here, let&#39;s use datatable to read the data, and then convert to a pandas dataframe. . %%time train_dt = dt.fread(f&quot;{input_path}train.csv&quot;) . CPU times: user 28.6 s, sys: 5.02 s, total: 33.6 s Wall time: 4min 41s . %%time train_df = train_dt.to_pandas() . CPU times: user 5.37 s, sys: 3.98 s, total: 9.35 s Wall time: 7.07 s . Exploration . resp . fig,ax = plt.subplots(figsize=(15,5), facecolor=&quot;#F0F0F0&quot;) balance = pd.Series(train_df[&#39;resp&#39;].cumsum()) ax.set_xlabel(&quot;Trade&quot;, fontsize=18) ax.set_ylabel(&quot;Cumulative resp&quot;, fontsize=18) balance.plot(lw=3) del balance gc.collect() . . 2487 . as well as four time horizons . The longer the Time Horizon, the more aggressive, or riskier portfolio, ans investor can build. The shorter the Time Horizon, the more conservative, or less risky, the investor may want to adopt. . fig,ax = plt.subplots(figsize=(15,5), facecolor=&quot;#F0F0F0&quot;) resps = [] for colName, colData in train_df.iteritems(): if (&#39;resp&#39; in colName): resps.append(colData.cumsum()) ax.set_xlabel(&quot;Trade&quot;, fontsize=18) ax.set_title(&quot;Cumulative resp and time horizons 1,2,3,4 (500 days)&quot;, fontsize=18) for r in resps: r.plot(lw=3) plt.legend(loc=&quot;upper left&quot;) del resps; gc.collect() . . 3382 . We can see that resp (in purple) most closely follows time horizon 4 (resp_4 is the uppermost curve, in red). . In the notebook Jane Street: time horizons and volatilities by @pcarta, if I understand correctly, by using maximum likelihood estimation it is calculated that if the time horizon ($T_j$) for resp_1 (i.e. $T_1$ is 1, then . $T_j$(resp_2) $ approx 1.4T_1$ | $T_j$(resp_3) $ approx 3.9T_1$ | $T_j$(resp_4) $ approx 11.1T_1$ where $T_1$ could correspond to 5 trading days. Let&#39;s now plot a histogram of all of the resp values (here only shown for values between -0.05 and 0.05) | . fig,ax = plt.subplots(figsize=(12,5), facecolor=&quot;#F0F0F0&quot;) sns.histplot( ax=ax, x=train_df[&#39;resp&#39;], bins=3000, kde_kws={&quot;clip&quot;:(-0.05,0.05)}, binrange=(-0.05,0.05), color=&#39;darkcyan&#39;, kde=False) values = np.array([rec.get_height() for rec in ax.patches]) norm = plt.Normalize(values.min(), values.max()) colors = plt.cm.jet(norm(values)) for rec, col in zip(ax.patches, colors): rec.set_color(col) plt.xlabel(&quot;Histogram of the resp values&quot;, size=14) plt.show(); del values gc.collect(); . . This distribution has very long tails . print(&#39;The minimum value for resp is : %.5f&#39; % train_df[&#39;resp&#39;].min()) print(&#39;The minimum value for resp is : %.5f&#39; % train_df[&#39;resp&#39;].max()) . . The minimum value for resp is : -0.54938 The minimum value for resp is : 0.44846 . Also calculate its&#39; skew and kurtosis of this distribution . print(&quot;Skew of resp is: %.2f&quot; % train_df[&#39;resp&#39;].skew() ) print(&quot;Kurtosis of resp is: %.2f&quot; % train_df[&#39;resp&#39;].kurtosis() ) . . Skew of resp is: 0.10 Kurtosis of resp is: 17.36 . weight . Each trade has an associated weight and resp, which together represents a return on the trade. Trades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation. . percent_zeros = (100/train_df.shape[0])*((train_df.weight.values == 0).sum()) print(&#39;Percentage of zero weights is: %i&#39; % percent_zeros +&quot;%&quot;) . Percentage of zero weights is: 0% . Let us see if there are any negative weights. A negative weight would be meaningless, but you never know. . min_weight = train_df[&#39;weight&#39;].min() print(&#39;The minimum weight is: %.2f&#39; % min_weight) . The minimum weight is: 0.01 . An now to find the maximum weight used . max_weight = train_df[&#39;weight&#39;].max() print(&#39;The maximum weight was: %.2f&#39; % max_weight) . The maximum weight was: 167.29 . which occured on day 446 . train_df[train_df[&#39;weight&#39;]==train_df[&#39;weight&#39;].max()] . date weight resp_1 resp_2 resp_3 resp_4 resp feature_0 feature_1 feature_2 feature_3 feature_4 feature_5 feature_6 feature_7 feature_8 feature_9 feature_10 feature_11 feature_12 feature_13 feature_14 feature_15 feature_16 feature_17 feature_18 feature_19 feature_20 feature_21 feature_22 feature_23 feature_24 feature_25 feature_26 feature_27 feature_28 feature_29 feature_30 feature_31 feature_32 ... feature_92 feature_93 feature_94 feature_95 feature_96 feature_97 feature_98 feature_99 feature_100 feature_101 feature_102 feature_103 feature_104 feature_105 feature_106 feature_107 feature_108 feature_109 feature_110 feature_111 feature_112 feature_113 feature_114 feature_115 feature_116 feature_117 feature_118 feature_119 feature_120 feature_121 feature_122 feature_123 feature_124 feature_125 feature_126 feature_127 feature_128 feature_129 ts_id action . 1322733 446 | 167.293716 | 0.000281 | 0.001213 | 0.00138 | -0.000427 | -0.001215 | -1 | -0.735754 | -0.048433 | -0.175366 | -0.20698 | 0.150967 | 0.23681 | 0.035755 | 0.026819 | -0.981118 | -0.188376 | 0.069351 | 0.060235 | -0.361906 | 0.09651 | -0.789638 | -0.582599 | 0.110738 | 0.114945 | -0.455587 | -0.844032 | 0.167681 | 0.177815 | -0.3927 | -0.804331 | -0.527014 | -0.93592 | 0.12584 | 0.160809 | 0.560677 | 1.263861 | 0.207275 | 0.252047 | ... | 0.314832 | 1.342298 | 1.391578 | 0.900093 | 0.401429 | -1.338859 | 0.420475 | -0.509267 | -1.194013 | -0.420336 | 0.244751 | -0.94019 | 0.320808 | -0.237071 | -1.781693 | 0.616443 | 0.392753 | -0.484772 | 0.417847 | -0.253349 | -0.843852 | -0.468821 | 0.262442 | 0.935519 | 0.288209 | 0.828683 | 0.438545 | 0.57467 | -0.92586 | 0.948026 | -1.094062 | 0.326287 | -0.715126 | 1.490866 | -1.111595 | 1.083793 | -0.979801 | 0.913979 | 2097681 | 0 | . 1 rows × 139 columns . Let us take a look at a histogram of the non-zero weights . fig,ax = plt.subplots(figsize = (12,5), facecolor=&quot;#F0F0F0&quot;) sns.histplot( ax=ax, x=train_df[&#39;weight&#39;], bins=1400, kde_kws={&quot;clip&quot;:(0.001,1.4)}, binrange=(0.001,1.4), color=&#39;darkcyan&#39;, kde=False); values = np.array([rec.get_height() for rec in ax.patches]) norm = plt.Normalize(values.min(), values.max()) colors = plt.cm.jet(norm(values)) for rec, col in zip(ax.patches, colors): rec.set_color(col) plt.xlabel(&quot;Histogram of non-zero weights&quot;, size=14) plt.show(); del values gc.collect(); . . https://www.kaggle.com/carlmcbrideellis/jane-street-eda-of-day-0-and-feature-importance . HyperParameters . Data . Loading training data . train_df = train_df.query(&#39;date &gt; 85&#39;).reset_index(drop=True) # limit memory usage train_df = train_df.astype({c: np.float32 for c in train_df.select_dtypes(include=&#39;float64&#39;).columns}) train_df.fillna(train_df.mean(), inplace=True) train_df = train_df.query(&#39;weight &gt; 0&#39;).reset_index(drop = True) train_df[&#39;action&#39;] = ((train_df[&#39;resp_1&#39;] &gt; 0.00001) &amp; (train_df[&#39;resp_2&#39;] &gt; 0.00001) &amp; (train_df[&#39;resp_3&#39;] &gt; 0.00001 ) &amp; (train_df[&#39;resp_4&#39;] &gt; 0.00001 ) &amp; (train_df[&#39;resp&#39;] &gt; 0.00001 )).astype(&#39;int&#39;) features = [c for c in train_df.columns if &#39;feature&#39; in c] resp_cols = [&#39;resp&#39;, &#39;resp_1&#39;, &#39;resp_2&#39;, &#39;resp_3&#39;, &#39;resp_4&#39;] x_train = train_df[features].values y_train = np.stack([(train_df[col] &gt; 0.000001).astype(&#39;int&#39;) for col in resp_cols]).T # Multitarget f_mean = np.mean(train_df[features[1:]].values, axis=0) . Model . The idea of using an encoder is the denoise the data. After many attempts at using a unsupervised autoencoder, the choice landed on a bottleneck encoder as this will preserve the intra-feature relations. . from sklearn.model_selection import KFold from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples from sklearn.utils.validation import _deprecate_positional_args . # modified code for group gaps; source # https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243 class PurgedGroupTimeSeriesSplit(_BaseKFold): &quot;&quot;&quot;Time Series cross-validator variant with non-overlapping groups. Allows for a gap in groups to avoid potentially leaking info from train into test if the model has windowed or lag features. Provides train/test indices to split time series data samples that are observed at fixed time intervals according to a third-party provided group. In each split, test indices must be higher than before, and thus shuffling in cross validator is inappropriate. This cross-validation object is a variation of :class:`KFold`. In the kth split, it returns first k folds as train set and the (k+1)th fold as test set. The same group will not appear in two different folds (the number of distinct groups has to be at least equal to the number of folds). Note that unlike standard cross-validation methods, successive training sets are supersets of those that come before them. Read more in the :ref:`User Guide &lt;cross_validation&gt;`. Parameters - n_splits : int, default=5 Number of splits. Must be at least 2. max_train_group_size : int, default=Inf Maximum group size for a single training set. group_gap : int, default=None Gap between train and test max_test_group_size : int, default=Inf We discard this number of groups from the end of each train split &quot;&quot;&quot; @_deprecate_positional_args def __init__(self, n_splits=5, *, max_train_group_size=np.inf, max_test_group_size=np.inf, group_gap=None, verbose=False ): super().__init__(n_splits, shuffle=False, random_state=None) self.max_train_group_size = max_train_group_size self.group_gap = group_gap self.max_test_group_size = max_test_group_size self.verbose = verbose def split(self, X, y=None, groups=None): &quot;&quot;&quot;Generate indices to split data into training and test set. Parameters - X : array-like of shape (n_samples, n_features) Training data, where n_samples is the number of samples and n_features is the number of features. y : array-like of shape (n_samples,) Always ignored, exists for compatibility. groups : array-like of shape (n_samples,) Group labels for the samples used while splitting the dataset into train/test set. Yields train : ndarray The training set indices for that split. test : ndarray The testing set indices for that split. &quot;&quot;&quot; if groups is None: raise ValueError( &quot;The &#39;groups&#39; parameter should not be None&quot;) X, y, groups = indexable(X, y, groups) n_samples = _num_samples(X) n_splits = self.n_splits group_gap = self.group_gap max_test_group_size = self.max_test_group_size max_train_group_size = self.max_train_group_size n_folds = n_splits + 1 group_dict = {} u, ind = np.unique(groups, return_index=True) unique_groups = u[np.argsort(ind)] n_samples = _num_samples(X) n_groups = _num_samples(unique_groups) for idx in np.arange(n_samples): if (groups[idx] in group_dict): group_dict[groups[idx]].append(idx) else: group_dict[groups[idx]] = [idx] if n_folds &gt; n_groups: raise ValueError( (&quot;Cannot have number of folds={0} greater than&quot; &quot; the number of groups={1}&quot;).format(n_folds, n_groups)) group_test_size = min(n_groups // n_folds, max_test_group_size) group_test_starts = range(n_groups - n_splits * group_test_size, n_groups, group_test_size) for group_test_start in group_test_starts: train_array = [] test_array = [] group_st = max(0, group_test_start - group_gap - max_train_group_size) for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]: train_array_tmp = group_dict[train_group_idx] train_array = np.sort(np.unique( np.concatenate((train_array, train_array_tmp)), axis=None), axis=None) train_end = train_array.size for test_group_idx in unique_groups[group_test_start: group_test_start + group_test_size]: test_array_tmp = group_dict[test_group_idx] test_array = np.sort(np.unique( np.concatenate((test_array, test_array_tmp)), axis=None), axis=None) test_array = test_array[group_gap:] if self.verbose &gt; 0: pass yield [int(i) for i in train_array], [int(i) for i in test_array] . . class CVTuner(kt.engine.tuner.Tuner): def run_trial(self, trial, X, y, splits, batch_size=32, epochs=1,callbacks=None): val_losses = [] for train_indices, test_indices in splits: X_train, X_test = [x[train_indices] for x in X], [x[test_indices] for x in X] y_train, y_test = [a[train_indices] for a in y], [a[test_indices] for a in y] if len(X_train) &lt; 2: X_train = X_train[0] X_test = X_test[0] if len(y_train) &lt; 2: y_train = y_train[0] y_test = y_test[0] model = self.hypermodel.build(trial.hyperparameters) hist = model.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=epochs, batch_size=batch_size, callbacks=callbacks) val_losses.append([hist.history[k][-1] for k in hist.history]) val_losses = np.asarray(val_losses) self.oracle.update_trial(trial.trial_id, {k:np.mean(val_losses[:,i]) for i,k in enumerate(hist.history.keys())}) self.save_model(trial.trial_id, model) . . Building the autoencoder . The autoencoder should aid in denoising the data based on this paper. . def build_autoencoder(input_dim, output_dim, noise=.05): inputs = tf.keras.layers.Input(input_dim) encoded = tf.keras.layers.BatchNormalization()(inputs) encoded = tf.keras.layers.GaussianNoise(noise)(encoded) encoded = tf.keras.layers.Dense(640, activation=&#39;relu&#39;)(encoded) decoded = tf.keras.layers.Dropout(0.2)(encoded) decoded = tf.keras.layers.Dense(input_dim, name=&#39;decoded&#39;)(decoded) x = tf.keras.layers.Dense(320, activation=&#39;relu&#39;)(decoded) x = tf.keras.layers.BatchNormalization()(x) x = tf.keras.layers.Dropout(0.2)(x) x = tf.keras.layers.Dense(output_dim, activation=&#39;sigmoid&#39;, name=&#39;label_output&#39;)(x) encoder = tf.keras.models.Model(inputs=inputs, outputs=encoded) autoencoder = tf.keras.models.Model(inputs=inputs, outputs=[decoded,x]) autoencoder.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss={&#39;decoded&#39;:&#39;mse&#39;, &#39;label_output&#39;:&#39;binary_crossentropy&#39;}) return autoencoder, encoder . Building the MLP . def build_model(hp, input_dim, output_dim, encoder): inputs = tf.keras.layers.Input(input_dim) x = encoder(inputs) x = tf.keras.layers.Concatenate()([x,inputs]) #use both raw and encoded features x = tf.keras.layers.BatchNormalization()(x) x = tf.keras.layers.Dropout(hp.Float(&#39;init_dropout&#39;,0.0,0.5))(x) for i in range(hp.Int(&#39;num_layers&#39;,1,5)): x = tf.keras.layers.Dense(hp.Int(&#39;num_units_{i}&#39;,128,256))(x) x = tf.keras.layers.BatchNormalization()(x) x = tf.keras.layers.Lambda(tf.keras.activations.swish)(x) x = tf.keras.layers.Dropout(hp.Float(f&#39;dropout_{i}&#39;,0.0,0.5))(x) x = tf.keras.layers.Dense(output_dim,activation=&#39;sigmoid&#39;)(x) model = tf.keras.models.Model(inputs=inputs, outputs=x) model.compile(optimizer = tf.keras.optimizers.Adam( hp.Float(&#39;lr&#39;,0.00001,0.1,default=0.001)), loss = tf.keras.losses.BinaryCrossentropy( label_smoothing = hp.Float(&#39;label_smoothing&#39;,0.0,0.1)), metrics = [tf.keras.metrics.AUC(name = &#39;auc&#39;)]) return model . Building a model . We add gaussian noise with mean and std from training datea. After training we lock the layersfin the encoder from further training. . Optimizer . Callbacks . callbacks = [ tf.keras.callbacks.EarlyStopping( monitor=&#39;val_loss&#39;, patience=10, restore_best_weights=True, verbose=1) ] . Training . autoencoder,encoder = build_autoencoder( x_train.shape[-1], y_train.shape[-1], noise=.1) . autoencoder.summary() . Model: &#34;model_5&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_3 (InputLayer) [(None, 130)] 0 _________________________________________________________________ batch_normalization_4 (Batch (None, 130) 520 _________________________________________________________________ gaussian_noise_2 (GaussianNo (None, 130) 0 _________________________________________________________________ dense_4 (Dense) (None, 640) 83840 _________________________________________________________________ dropout_4 (Dropout) (None, 640) 0 _________________________________________________________________ decoded (Dense) (None, 130) 83330 _________________________________________________________________ dense_5 (Dense) (None, 320) 41920 _________________________________________________________________ batch_normalization_5 (Batch (None, 320) 1280 _________________________________________________________________ dropout_5 (Dropout) (None, 320) 0 _________________________________________________________________ label_output (Dense) (None, 5) 1605 ================================================================= Total params: 212,495 Trainable params: 211,595 Non-trainable params: 900 _________________________________________________________________ . history = autoencoder.fit( x_train, (x_train, y_train), epochs=1002, batch_size=16384, validation_split=0.1, callbacks=callbacks ) . Epoch 1/1002 87/87 [==============================] - 6s 37ms/step - loss: 3.2765 - decoded_loss: 2.5013 - label_output_loss: 0.7752 - val_loss: 1.1174 - val_decoded_loss: 0.4225 - val_label_output_loss: 0.6949 Epoch 2/1002 87/87 [==============================] - 3s 30ms/step - loss: 1.1679 - decoded_loss: 0.4596 - label_output_loss: 0.7083 - val_loss: 0.8400 - val_decoded_loss: 0.1500 - val_label_output_loss: 0.6899 Epoch 3/1002 87/87 [==============================] - 3s 29ms/step - loss: 1.0435 - decoded_loss: 0.3474 - label_output_loss: 0.6961 - val_loss: 0.7875 - val_decoded_loss: 0.0986 - val_label_output_loss: 0.6889 Epoch 4/1002 87/87 [==============================] - 2s 29ms/step - loss: 0.9867 - decoded_loss: 0.2949 - label_output_loss: 0.6918 - val_loss: 0.7706 - val_decoded_loss: 0.0816 - val_label_output_loss: 0.6890 Epoch 5/1002 87/87 [==============================] - 3s 29ms/step - loss: 1.0088 - decoded_loss: 0.3185 - label_output_loss: 0.6903 - val_loss: 0.7628 - val_decoded_loss: 0.0742 - val_label_output_loss: 0.6886 Epoch 6/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.9435 - decoded_loss: 0.2538 - label_output_loss: 0.6898 - val_loss: 0.7582 - val_decoded_loss: 0.0695 - val_label_output_loss: 0.6887 Epoch 7/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9328 - decoded_loss: 0.2433 - label_output_loss: 0.6895 - val_loss: 0.7504 - val_decoded_loss: 0.0622 - val_label_output_loss: 0.6882 Epoch 8/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9254 - decoded_loss: 0.2361 - label_output_loss: 0.6893 - val_loss: 0.7489 - val_decoded_loss: 0.0604 - val_label_output_loss: 0.6885 Epoch 9/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9405 - decoded_loss: 0.2513 - label_output_loss: 0.6892 - val_loss: 0.7476 - val_decoded_loss: 0.0591 - val_label_output_loss: 0.6884 Epoch 10/1002 87/87 [==============================] - 2s 29ms/step - loss: 0.9278 - decoded_loss: 0.2386 - label_output_loss: 0.6892 - val_loss: 0.7449 - val_decoded_loss: 0.0567 - val_label_output_loss: 0.6882 Epoch 11/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9307 - decoded_loss: 0.2416 - label_output_loss: 0.6891 - val_loss: 0.7427 - val_decoded_loss: 0.0545 - val_label_output_loss: 0.6883 Epoch 12/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9113 - decoded_loss: 0.2225 - label_output_loss: 0.6888 - val_loss: 0.7406 - val_decoded_loss: 0.0520 - val_label_output_loss: 0.6885 Epoch 13/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9149 - decoded_loss: 0.2260 - label_output_loss: 0.6890 - val_loss: 0.7387 - val_decoded_loss: 0.0505 - val_label_output_loss: 0.6882 Epoch 14/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9261 - decoded_loss: 0.2374 - label_output_loss: 0.6887 - val_loss: 0.7384 - val_decoded_loss: 0.0503 - val_label_output_loss: 0.6881 Epoch 15/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9222 - decoded_loss: 0.2335 - label_output_loss: 0.6886 - val_loss: 0.7387 - val_decoded_loss: 0.0503 - val_label_output_loss: 0.6885 Epoch 16/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9061 - decoded_loss: 0.2176 - label_output_loss: 0.6885 - val_loss: 0.7372 - val_decoded_loss: 0.0490 - val_label_output_loss: 0.6882 Epoch 17/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.9178 - decoded_loss: 0.2292 - label_output_loss: 0.6886 - val_loss: 0.7405 - val_decoded_loss: 0.0526 - val_label_output_loss: 0.6879 Epoch 18/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8941 - decoded_loss: 0.2057 - label_output_loss: 0.6884 - val_loss: 0.7379 - val_decoded_loss: 0.0499 - val_label_output_loss: 0.6880 Epoch 19/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9067 - decoded_loss: 0.2183 - label_output_loss: 0.6884 - val_loss: 0.7360 - val_decoded_loss: 0.0482 - val_label_output_loss: 0.6878 Epoch 20/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.9197 - decoded_loss: 0.2313 - label_output_loss: 0.6884 - val_loss: 0.7385 - val_decoded_loss: 0.0505 - val_label_output_loss: 0.6880 Epoch 21/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.9045 - decoded_loss: 0.2163 - label_output_loss: 0.6882 - val_loss: 0.7391 - val_decoded_loss: 0.0511 - val_label_output_loss: 0.6880 Epoch 22/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8970 - decoded_loss: 0.2088 - label_output_loss: 0.6882 - val_loss: 0.7384 - val_decoded_loss: 0.0504 - val_label_output_loss: 0.6880 Epoch 23/1002 87/87 [==============================] - 2s 29ms/step - loss: 0.9360 - decoded_loss: 0.2479 - label_output_loss: 0.6881 - val_loss: 0.7339 - val_decoded_loss: 0.0461 - val_label_output_loss: 0.6878 Epoch 24/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8909 - decoded_loss: 0.2028 - label_output_loss: 0.6881 - val_loss: 0.7393 - val_decoded_loss: 0.0510 - val_label_output_loss: 0.6882 Epoch 25/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8966 - decoded_loss: 0.2086 - label_output_loss: 0.6880 - val_loss: 0.7351 - val_decoded_loss: 0.0471 - val_label_output_loss: 0.6880 Epoch 26/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9049 - decoded_loss: 0.2171 - label_output_loss: 0.6879 - val_loss: 0.7315 - val_decoded_loss: 0.0435 - val_label_output_loss: 0.6880 Epoch 27/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8959 - decoded_loss: 0.2081 - label_output_loss: 0.6878 - val_loss: 0.7348 - val_decoded_loss: 0.0468 - val_label_output_loss: 0.6880 Epoch 28/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8936 - decoded_loss: 0.2058 - label_output_loss: 0.6878 - val_loss: 0.7347 - val_decoded_loss: 0.0468 - val_label_output_loss: 0.6879 Epoch 29/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8993 - decoded_loss: 0.2116 - label_output_loss: 0.6877 - val_loss: 0.7346 - val_decoded_loss: 0.0467 - val_label_output_loss: 0.6878 Epoch 30/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.9043 - decoded_loss: 0.2166 - label_output_loss: 0.6876 - val_loss: 0.7349 - val_decoded_loss: 0.0470 - val_label_output_loss: 0.6878 Epoch 31/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8943 - decoded_loss: 0.2067 - label_output_loss: 0.6877 - val_loss: 0.7352 - val_decoded_loss: 0.0471 - val_label_output_loss: 0.6880 Epoch 32/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.9031 - decoded_loss: 0.2156 - label_output_loss: 0.6875 - val_loss: 0.7362 - val_decoded_loss: 0.0482 - val_label_output_loss: 0.6880 Epoch 33/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9025 - decoded_loss: 0.2151 - label_output_loss: 0.6874 - val_loss: 0.7322 - val_decoded_loss: 0.0438 - val_label_output_loss: 0.6884 Epoch 34/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8991 - decoded_loss: 0.2117 - label_output_loss: 0.6874 - val_loss: 0.7318 - val_decoded_loss: 0.0439 - val_label_output_loss: 0.6879 Epoch 35/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8955 - decoded_loss: 0.2081 - label_output_loss: 0.6873 - val_loss: 0.7316 - val_decoded_loss: 0.0437 - val_label_output_loss: 0.6879 Epoch 36/1002 87/87 [==============================] - 3s 31ms/step - loss: 0.9228 - decoded_loss: 0.2355 - label_output_loss: 0.6874 - val_loss: 0.7311 - val_decoded_loss: 0.0429 - val_label_output_loss: 0.6883 Epoch 37/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8863 - decoded_loss: 0.1990 - label_output_loss: 0.6873 - val_loss: 0.7334 - val_decoded_loss: 0.0454 - val_label_output_loss: 0.6880 Epoch 38/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8938 - decoded_loss: 0.2067 - label_output_loss: 0.6871 - val_loss: 0.7317 - val_decoded_loss: 0.0436 - val_label_output_loss: 0.6881 Epoch 39/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8955 - decoded_loss: 0.2082 - label_output_loss: 0.6873 - val_loss: 0.7321 - val_decoded_loss: 0.0441 - val_label_output_loss: 0.6880 Epoch 40/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8989 - decoded_loss: 0.2117 - label_output_loss: 0.6872 - val_loss: 0.7318 - val_decoded_loss: 0.0436 - val_label_output_loss: 0.6883 Epoch 41/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8904 - decoded_loss: 0.2033 - label_output_loss: 0.6872 - val_loss: 0.7285 - val_decoded_loss: 0.0404 - val_label_output_loss: 0.6881 Epoch 42/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8889 - decoded_loss: 0.2018 - label_output_loss: 0.6870 - val_loss: 0.7344 - val_decoded_loss: 0.0460 - val_label_output_loss: 0.6884 Epoch 43/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8896 - decoded_loss: 0.2025 - label_output_loss: 0.6871 - val_loss: 0.7299 - val_decoded_loss: 0.0418 - val_label_output_loss: 0.6881 Epoch 44/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.9085 - decoded_loss: 0.2213 - label_output_loss: 0.6871 - val_loss: 0.7323 - val_decoded_loss: 0.0441 - val_label_output_loss: 0.6882 Epoch 45/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8972 - decoded_loss: 0.2104 - label_output_loss: 0.6867 - val_loss: 0.7300 - val_decoded_loss: 0.0421 - val_label_output_loss: 0.6879 Epoch 46/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.9084 - decoded_loss: 0.2216 - label_output_loss: 0.6868 - val_loss: 0.7313 - val_decoded_loss: 0.0433 - val_label_output_loss: 0.6880 Epoch 47/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8876 - decoded_loss: 0.2007 - label_output_loss: 0.6869 - val_loss: 0.7336 - val_decoded_loss: 0.0452 - val_label_output_loss: 0.6885 Epoch 48/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8860 - decoded_loss: 0.1993 - label_output_loss: 0.6867 - val_loss: 0.7272 - val_decoded_loss: 0.0390 - val_label_output_loss: 0.6881 Epoch 49/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8836 - decoded_loss: 0.1968 - label_output_loss: 0.6868 - val_loss: 0.7306 - val_decoded_loss: 0.0423 - val_label_output_loss: 0.6883 Epoch 50/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8836 - decoded_loss: 0.1970 - label_output_loss: 0.6866 - val_loss: 0.7289 - val_decoded_loss: 0.0407 - val_label_output_loss: 0.6882 Epoch 51/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8905 - decoded_loss: 0.2038 - label_output_loss: 0.6866 - val_loss: 0.7317 - val_decoded_loss: 0.0433 - val_label_output_loss: 0.6884 Epoch 52/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8814 - decoded_loss: 0.1949 - label_output_loss: 0.6865 - val_loss: 0.7290 - val_decoded_loss: 0.0409 - val_label_output_loss: 0.6880 Epoch 53/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8897 - decoded_loss: 0.2029 - label_output_loss: 0.6868 - val_loss: 0.7281 - val_decoded_loss: 0.0399 - val_label_output_loss: 0.6883 Epoch 54/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8829 - decoded_loss: 0.1964 - label_output_loss: 0.6865 - val_loss: 0.7303 - val_decoded_loss: 0.0419 - val_label_output_loss: 0.6883 Epoch 55/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8815 - decoded_loss: 0.1951 - label_output_loss: 0.6864 - val_loss: 0.7294 - val_decoded_loss: 0.0413 - val_label_output_loss: 0.6881 Epoch 56/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8809 - decoded_loss: 0.1945 - label_output_loss: 0.6864 - val_loss: 0.7286 - val_decoded_loss: 0.0406 - val_label_output_loss: 0.6880 Epoch 57/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8834 - decoded_loss: 0.1970 - label_output_loss: 0.6864 - val_loss: 0.7286 - val_decoded_loss: 0.0405 - val_label_output_loss: 0.6880 Epoch 58/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.9081 - decoded_loss: 0.2218 - label_output_loss: 0.6864 - val_loss: 0.7275 - val_decoded_loss: 0.0396 - val_label_output_loss: 0.6879 Restoring model weights from the end of the best epoch. Epoch 00058: early stopping . . encoder.save_weights(&#39;encoder.hdf5&#39;) . encoder.trainable = False . Running CV . Following this notebook which use 5 PurgedGroupTimeSeriesSplit split on the dates in the training data. . We add the locked encoder as the first layer of the MLP. This seems to help in speeding up the submission rather than first predicting using the encoder then using the MLP. . We use a Baysian Optimizer to find the optimal HPs for out model. 20 trials take about 2 hours on GPU. . model_fn = lambda hp: build_model( hp, x_train.shape[-1], y_train.shape[-1], encoder) .",
            "url": "https://austinyhc.github.io/blog/time%20series/stock/2021/01/06/jane_street_market_prediction.html",
            "relUrl": "/time%20series/stock/2021/01/06/jane_street_market_prediction.html",
            "date": " • Jan 6, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Learning Rate Finder",
            "content": "The learning rate is arguably the most important hyperparameter that controls how much we are adjusting the weights of our network with respect to the loss gradient. It stands for how much a model can learn from a new mini-batch of training data. The higher the learning rate, the bigger the steps we take along the trajectory to the minimum of the loss function, where the best model parameters are. . . Why is it hard? . The learning rate is a tricky hyperparameter to tune for a number of reasons: . In most cases, domain knowledge or previous studies are of little help, for a learning rate that worked well for one problem might not be even half as good for another, even a closely-related one. | Tuning learning rates via a grid search or a random search is typically costly, both in terms of time and computing power, especially for large networks. | The optimal learning rate is tightly coupled with other hyperparameters. Hence, each time your change the amount of regularization or the networks architecture, you should re-tune the learning rate. | . import numpy as np import pandas as pd import seaborn as sns import albumentations as A import matplotlib.pyplot as plt import os, gc, cv2, random, warnings import math, sys, json, pprint, pdb import tensorflow as tf from tensorflow.keras import backend as K from tensorflow.keras.callbacks import Callback from sklearn.model_selection import train_test_split warnings.simplefilter(&#39;ignore&#39;) print(f&quot;Using TensorFlow v{tf.__version__}&quot;) . . Using TensorFlow v2.4.0 . Implement LRFinder . class MultiplicativeLearningRate(tf.keras.callbacks.Callback): def __init__(self, factor): self.factor = factor self.losses = [] self.lrs = [] def on_batch_end(self, batch, logs): self.lrs.append(K.get_value(self.model.optimizer.lr)) self.losses.append(logs[&quot;loss&quot;]) K.set_value(self.model.optimizer.lr, self.model.optimizer.lr*self.factor) . min_lr = 1e-6 max_lr = 1e1 num_iter = 1000 lr_factor = np.exp(np.log(max_lr / min_lr) / num_iter) lrs = [min_lr * (lr_factor)**i for i in range(num_iter)] fig,axs = plt.subplots(1,2,figsize=(12,4),facecolor=&quot;#F0F0F0&quot;) axs[0].plot(lrs) axs[0].set_yscale(&quot;log&quot;) axs[0].set_ylabel(&quot;learning rate&quot;) axs[0].set_xlabel(&quot;iteration&quot;) axs[1].plot(lrs) axs[1].set_ylabel(&quot;learning rate&quot;) axs[1].set_xlabel(&quot;iteration&quot;) . Text(0.5, 0, &#39;iteration&#39;) . def find_lr(model, x, y, batch_size, min_lr=1e-6, max_lr=1e1): num_iter = len(x) // batch_size lr_factor = np.exp(np.log(max_lr / min_lr) / num_iter) # Train for 1 epoch, starting with minimum learning rate and increase it K.set_value(model.optimizer.lr, min_lr) lr_callback = MultiplicativeLearningRate(lr_factor) model.fit(x, y, epochs=1, batch_size=batch_size, callbacks=[lr_callback]) # Plot loss vs log-scaled learning rate plot = sns.lineplot(lr_callback.lrs, lr_callback.losses) plot.set(xscale=&quot;log&quot;, xlabel=&quot;Learning Rate (log-scale)&quot;, ylabel=&quot;Training Loss&quot;, title=&quot;Optimal learning rate is slightly below minimum&quot;, facecolor=&quot;#F0F0F0&quot;) . class LRFinder(Callback): &quot;&quot;&quot;Callback that exponentially adjusts the learning rate after each training batch between start_lr and end_lr for a maximum number of batches: max_step. The loss and learning rate are recorded at each step allowing visually finding a good learning rate as per https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html via the plot method. &quot;&quot;&quot; def __init__(self, start_lr: float = 1e-7, end_lr: float = 10, max_steps: int = 100, smoothing=0.9): super(LRFinder, self).__init__() self.start_lr, self.end_lr = start_lr, end_lr self.max_steps = max_steps self.smoothing = smoothing self.step, self.best_loss, self.avg_loss, self.lr = 0, 0, 0, 0 self.lrs, self.losses = [], [] def on_train_begin(self, logs=None): self.step, self.best_loss, self.avg_loss, self.lr = 0, 0, 0, 0 self.lrs, self.losses = [], [] def on_train_batch_begin(self, batch, logs=None): self.lr = self.exp_annealing(self.step) K.set_value(self.model.optimizer.lr, self.lr) def on_train_batch_end(self, batch, logs=None): logs = logs or {} loss = logs.get(&#39;loss&#39;) step = self.step if loss: self.avg_loss = self.smoothing * self.avg_loss + (1 - self.smoothing) * loss smooth_loss = self.avg_loss / (1 - self.smoothing ** (self.step + 1)) self.losses.append(smooth_loss) self.lrs.append(self.lr) if step == 0 or loss &lt; self.best_loss: self.best_loss = loss if smooth_loss &gt; 4 * self.best_loss or tf.math.is_nan(smooth_loss): self.model.stop_training = True if step == self.max_steps: self.model.stop_training = True self.step += 1 def exp_annealing(self, step): return self.start_lr * (self.end_lr / self.start_lr) ** (step * 1. / self.max_steps) def plot(self): fig, ax = plt.subplots(1,1,facecolor=&quot;#F0F0F0&quot;) ax.set_ylabel(&#39;Loss&#39;) ax.set_xlabel(&#39;Learning Rate&#39;) ax.set_xscale(&#39;log&#39;) ax.xaxis.set_major_formatter(plt.FormatStrFormatter(&#39;%.0e&#39;)) ax.plot(self.lrs, self.losses) . MNIST Dataset . (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() print(x_train.shape, x_test.shape, y_train.shape, y_test.shape) . Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11493376/11490434 [==============================] - 0s 0us/step (60000, 28, 28) (10000, 28, 28) (60000,) (10000,) . IMG_SIZE = (28, 28, 1) NCLASSES = 10 . x_train = np.expand_dims(x_train.astype(&#39;float32&#39;) / 255.0, axis=-1) x_test = np.expand_dims(x_test.astype(&#39;float32&#39;) / 255.0, axis=-1) print(&#39;x_train shape:&#39;, x_train.shape) print(&#39;x_test shape:&#39;, x_test.shape) . x_train shape: (60000, 28, 28, 1) x_test shape: (10000, 28, 28, 1) . y_train = tf.keras.utils.to_categorical(y_train, NCLASSES) y_test = tf.keras.utils.to_categorical(y_test, NCLASSES) print(&#39;y_train shape:&#39;, y_train.shape) print(&#39;y_test shape:&#39;, y_test.shape) . y_train shape: (60000, 10) y_test shape: (10000, 10) . Simple Model . def build_simple_model(input_shape, lr=None): model = tf.keras.models.Sequential() model.add(tf.keras.layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, input_shape=input_shape)) model.add(tf.keras.layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;)) model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2))) model.add(tf.keras.layers.Dropout(0.25)) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(128, activation=&#39;relu&#39;)) model.add(tf.keras.layers.Dropout(0.5)) model.add(tf.keras.layers.Dense(10, activation=&#39;softmax&#39;)) opt = tf.keras.optimizers.SGD(learning_rate=lr if lr else 1e-2) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=opt, metrics=[&#39;accuracy&#39;]) return model . BATCH_SIZE = 64 EPOCHS = 5 STEPS = len(x_train) // BATCH_SIZE . Train without optimal lr . Let us train our model without searching for an optimal learning rate. We will be using the default learning rate for the optimizer . model = build_simple_model(IMG_SIZE) model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE) . Epoch 1/5 938/938 [==============================] - 5s 5ms/step - loss: 1.1890 - accuracy: 0.6130 Epoch 2/5 938/938 [==============================] - 5s 5ms/step - loss: 0.3800 - accuracy: 0.8839 Epoch 3/5 938/938 [==============================] - 5s 5ms/step - loss: 0.2949 - accuracy: 0.9112 Epoch 4/5 938/938 [==============================] - 5s 5ms/step - loss: 0.2673 - accuracy: 0.9187 Epoch 5/5 938/938 [==============================] - 5s 5ms/step - loss: 0.2363 - accuracy: 0.9295 . &lt;tensorflow.python.keras.callbacks.History at 0x7faaf2b84390&gt; . score = model.evaluate(x_test, y_test, verbose=0,batch_size=BATCH_SIZE) print(&#39;Test loss:&#39;, score[0]) print(&#39;Test accuracy:&#39;, score[1]) . Test loss: 0.11825986206531525 Test accuracy: 0.9642999768257141 . Train with optimal lr . We will create an instance of the class we built above and pass it as a callback to our model. The LR finder is very cheap in terms of compute and it hardly takes an epoch or less to complete. We will keep the default values of base_lr and max_lr but you can change it if you want to. . model = build_simple_model(IMG_SIZE) find_lr(model, x_train, y_train, batch_size=BATCH_SIZE) . 1/938 [..............................] - ETA: 5:44 - loss: 2.3144 - accuracy: 0.0312WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.0053s). Check your callbacks. 938/938 [==============================] - 6s 6ms/step - loss: 2.2018 - accuracy: 0.1523 . The recommended minimum learning rate is the value where the loss decreases the fatest (minimum negative gradient), while the recommended maximum learning rate is 10 times less than the learning rate wher the loss is minimum. Why not just the very minimum of the loss? Why 10 times less? Because what we actually plot is a smoothed version of the loss, and taking the learning rate corresponding to the minimum loss is likely to be too large and make the loss diverge during training. . model3 = build_simple_model(IMG_SIZE, lr=8e-2) model3.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE) . Epoch 1/5 938/938 [==============================] - 5s 5ms/step - loss: 0.6557 - accuracy: 0.7870 Epoch 2/5 938/938 [==============================] - 5s 5ms/step - loss: 0.1406 - accuracy: 0.9579 Epoch 3/5 938/938 [==============================] - 5s 5ms/step - loss: 0.1012 - accuracy: 0.9690 Epoch 4/5 938/938 [==============================] - 5s 5ms/step - loss: 0.0823 - accuracy: 0.9751 Epoch 5/5 938/938 [==============================] - 5s 5ms/step - loss: 0.0690 - accuracy: 0.9790 . &lt;tensorflow.python.keras.callbacks.History at 0x7faae42e75c0&gt; . score = model3.evaluate(x_test, y_test, verbose=0, batch_size=BATCH_SIZE) print(&#39;Test loss:&#39;, score[0]) print(&#39;Test accuracy:&#39;, score[1]) . Test loss: 0.0344100221991539 Test accuracy: 0.9896000027656555 . You can see that if we start with an optimal learning rate, we can coverge much faster. A good start always pays off! . Conclusion . In this notebook, we saw how we can implement a simple LR finder in keras. Keras gives you the hooks to implement almost anything seamlessly. Before writing anything from scratch, you should always check how can you use a hook to implement it in Keras first. . References . https://arxiv.org/abs/1708.07120 | https://arxiv.org/abs/1506.01186 | https://arxiv.org/abs/1803.09820 | https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html | . 1% Better Everyday . Maybe we don&#39;t really need to create a class for learning rate finder. Perhaps we can achieve the same goal by using the tf.keras.optimizers.schedules.ExponentialDecay plus tensor board. . https://blog.dataiku.com/the-learning-rate-finder-technique-how-reliable-is-it | https://medium.com/octavian-ai/how-to-use-the-learning-rate-finder-in-tensorflow-126210de9489 | .",
            "url": "https://austinyhc.github.io/blog/optimizer/learning%20rate/2021/01/06/implementation-of-learning-rate-finder.html",
            "relUrl": "/optimizer/learning%20rate/2021/01/06/implementation-of-learning-rate-finder.html",
            "date": " • Jan 6, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "How To Create TFRecords",
            "content": "1% Better Everyday . Move the TFRecords basics from the notebook, Petals to the Metal. | . Cassava Leaf Disease-Stratified TFRecords 256x256 . !pip install -q imagehash &gt; /dev/null . import numpy as np import pandas as pd import seaborn as sns import albumentations as A import matplotlib.pyplot as plt import os, gc, cv2, random, re import warnings, math, sys, json import subprocess, pprint, pdb import tensorflow_hub as hub import tensorflow as tf from tensorflow.keras import backend as K from sklearn.model_selection import train_test_split, StratifiedKFold from sklearn.metrics import f1_score,precision_score, recall_score, confusion_matrix import glob, imagehash, copy import concurrent.futures,time from tqdm.auto import tqdm from PIL import Image warnings.simplefilter(&#39;ignore&#39;) print(f&quot;Using TensorFlow v{tf.__version__}&quot;) . Using TensorFlow v2.4.0 . def seed_everything(seed=0): random.seed(seed) np.random.seed(seed) tf.random.set_seed(seed) os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) os.environ[&#39;TF_DETERMINISTIC_OPS&#39;] = &#39;1&#39; SEED = 10120919 seed_everything(SEED) . from google.colab import drive drive.mount(&#39;/content/gdrive&#39;, force_remount=True) . Mounted at /content/gdrive . project_name = &#39;cassava-leaf-disease-classification&#39; root_path = &#39;/content/gdrive/MyDrive/&#39; input_path = f&#39;{root_path}kaggle/input/{project_name}/&#39; working_path = f&#39;{input_path}working/&#39; os.makedirs(working_path, exist_ok=True) os.chdir(working_path) os.listdir(input_path) . [&#39;label_num_to_disease_map.json&#39;, &#39;sample_submission.csv&#39;, &#39;train.csv&#39;, &#39;cassava-leaf-disease-classification.zip&#39;, &#39;test_images&#39;, &#39;test_tfrecords&#39;, &#39;train_images&#39;, &#39;train_tfrecords&#39;, &#39;dump.tfcache.data-00000-of-00001&#39;, &#39;dump.tfcache.index&#39;, &#39;working&#39;] . Find Duplicates . My strategy will pre-generate the four different hash values to represet each image. Therefore instead of comparing 480000 pixels between images, we use 256 integers for comparison. . funcs = [ imagehash.average_hash, imagehash.phash, imagehash.dhash, imagehash.whash ] . def hash(fname): im = Image.open(fname) id = os.path.basename(fname) hash = np.array([f(im).hash for f in funcs]).reshape(256) return id,hash . filenames = glob.glob(f&#39;{input_path}train_images/*.jpg&#39;) . im = Image.open(filenames[0]) im . id,hs = hash(filenames[0]) . res = 1 . res = res &lt;&lt; 1 . for h in hs: print(h) . False False True True False False False False True True False True True False False False False False True True True True True True False False False True True True True True False False False True True True True True False False False True False False True True False False False False False False True True True True True False False False True True True False True False True True False True True True False True True False False False False True True False False True True False False False False False True True False True True True True True True False False True False False False False False False True True False False False False False True True False True True True True False True True False True True False True False False True False True False True True False False True False True True True True True True False False True False True True True False True True False False True True False False True True False True True True False True True True True True False False False True True True True False False False False True True True False True True True False False False False False True False True True False False False False False True True True True True True False False False True True True True True False False False True True True True True False False False True False False True True False False False False False False True True True True True False False False True True . fnames = filenames[:500].copy() . ids = [] hashes = [] for fname in tqdm(fnames): im = Image.open(fname) id = os.path.basename(fname) ids.append(id) hashes.append(np.array([f(im).hash for f in funcs]).reshape(256)) . . . Tip: The above preprocssing can be paralellize. the only chunk of memory they share is the global list that each job append their output to. . def hash(fname): im = Image.open(fname) id = os.path.basename(fname) hash = np.array([f(im).hash for f in funcs]).reshape(256) return id,hash . with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor: results = list(tqdm(executor.map(hash, fnames), total=len(fnames))) . . Find if there is any duplicated images . img_ids,hashes = zip(*results) . hashes = np.asarray(hashes) similarity_matrix = np.array([(hashes[i] == hashes).sum(axis=1)/256 for i in range(hashes.shape[0])]) . idx,idy = np.where(similarity_matrix == 1) dup_pos = np.where(idx != idy) truncate = int(dup_pos[0].shape[0]/2) ids_to_delete = list(iy[dup_pos])[:truncate] print(f&quot;Number of duplicates: {len(ids_to_delete)}&quot;) . Number of duplicates: 0 . def decode_image(image_data): image = tf.image.decode_jpeg(image_data, channels=3) image = tf.cast(image, tf.float32) / 255.0 image = tf.image.resize(image, [HEIGHT, WIDTH]) image = tf.reshape(image, [HEIGHT, WIDTH, 3]) return image def read_tfrecord(example): TFREC_FORMAT = { &#39;image&#39;: tf.io.FixedLenFeature([], tf.string), &#39;target&#39;: tf.io.FixedLenFeature([], tf.int64), &#39;image_name&#39;: tf.io.FixedLenFeature([], tf.string), } example = tf.io.parse_single_example(example, TFREC_FORMAT) image = decode_image(example[&#39;image&#39;]) target = example[&#39;target&#39;] name = example[&#39;image_name&#39;] return image, target, name def load_dataset(filenames, HEIGHT, WIDTH, CHANNELS=3): dataset = tf.data.TFRecordDataset(filenames) dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO) return dataset def display_samples(ds, row, col): ds_iter = iter(ds) plt.figure(figsize=(15, int(15*row/col))) for j in range(row*col): image, label, name = next(ds_iter) plt.subplot(row,col,j+1) plt.axis(&#39;off&#39;) plt.imshow(image[0]) plt.title(f&quot;{label[0]}: {name[0].numpy().decode(&#39;utf-8&#39;)}&quot;, fontsize=12) plt.show() def count_data_items(filenames): n = [int(re.compile(r&quot;-([0-9]*) .&quot;).search(filename).group(1)) for filename in filenames] return np.sum(n) # Create TF Records def _bytes_feature(value): &quot;&quot;&quot;Returns a bytes_list from a string / byte.&quot;&quot;&quot; if isinstance(value, type(tf.constant(0))): value = value.numpy() # BytesList won&#39;t unpack a string from an EagerTensor. return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value])) def _int64_feature(value): &quot;&quot;&quot;Returns an int64_list from a bool / enum / int / uint.&quot;&quot;&quot; return tf.train.Feature(int64_list=tf.train.Int64List(value=[value])) def serialize_example(image, target, image_name): feature = { &#39;image&#39;: _bytes_feature(image), &#39;target&#39;: _int64_feature(target), &#39;image_name&#39;: _bytes_feature(image_name), } example_proto = tf.train.Example(features=tf.train.Features(feature=feature)) return example_proto.SerializeToString() . IMGS = os.listdir(f&#39;{input_path}train_images/&#39;) NFILES = 15 HEIGHT = 256 WIDTH = 256 IMG_SIZE = (HEIGHT, WIDTH, 3) IMG_QUALITY = 100 . train_df = pd.read_csv(f&#39;{input_path}train.csv&#39;) . train_df.head() . image_id label . 0 1000015157.jpg | 0 | . 1 1000201771.jpg | 3 | . 2 100042118.jpg | 1 | . 3 1000723321.jpg | 1 | . 4 1000812911.jpg | 3 | . train_df = train_df[~train_df.image_id.isin(pd.Series(img_ids[0]))].reset_index() print(f&#39;Number of training samples: {len(train_df)}&#39;) . Number of training samples: 21396 . stratify = StratifiedKFold(n_splits=NFILES, shuffle=True, random_state=SEED) train_df[&#39;file&#39;] = -1 . for i,(train, valid) in enumerate(stratify.split(train_df, train_df[&#39;label&#39;])): print(f&quot;File #{i}: {len(valid)}&quot;) train_df[&#39;file&#39;].loc[valid] = i . File #0: 1427 File #1: 1427 File #2: 1427 File #3: 1427 File #4: 1427 File #5: 1427 File #6: 1426 File #7: 1426 File #8: 1426 File #9: 1426 File #10: 1426 File #11: 1426 File #12: 1426 File #13: 1426 File #14: 1426 . train_df.head() . index image_id label file . 0 0 | 1000015157.jpg | 0 | 5 | . 1 1 | 1000201771.jpg | 3 | 13 | . 2 2 | 100042118.jpg | 1 | 10 | . 3 3 | 1000723321.jpg | 1 | 8 | . 4 4 | 1000812911.jpg | 3 | 12 | . train_df.to_csv(&#39;train.csv&#39;, index=False) . Generate TF records . for i in range(NFILES): samples = train_df[train_df[&#39;file&#39;] == i] n_samples = len(samples) print(f&#39;Writing TFRecord {i} of {NFILES}: {n_samples} samples&#39;) #print(&#39;Id_train%.2i-%i.tfrec&#39;%(i, n_samples)) with tf.io.TFRecordWriter(&#39;Id_train%.2i-%i.tfrec&#39; % (i, n_samples)) as writer: for row in samples.itertuples(): label = row.label image_name = row.image_id img_path = f&#39;{input_path}train_images/{image_name}&#39; img = cv2.imread(img_path) img = cv2.resize(img, (HEIGHT, WIDTH)) img = cv2.imencode(&#39;.jpg&#39;, img, (cv2.IMWRITE_JPEG_QUALITY, IMG_QUALITY))[1].tostring() example = serialize_example(img, label, str.encode(image_name)) writer.write(example) .",
            "url": "https://austinyhc.github.io/blog/tfrecord/dataset%20garden/2021/01/06/how_to_create_tfrecords.html",
            "relUrl": "/tfrecord/dataset%20garden/2021/01/06/how_to_create_tfrecords.html",
            "date": " • Jan 6, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Continuous Self Motivation",
            "content": "The Four Big Ideas . Habits are the compound interest of self-improvement. | If you want better results, then forget about setting goals. Focus on your system instead. | The most effective way to change your habits is to focus not on what you want to achieve, but on who you wish to become. | The Four Laws of Behavior Change are a simple set of rules we can use to build better habits. They are make it obvious | make it attractive | make it easy | make it satisfying. | . | . I. Customize 1cycle . This learning rate scheduler allows us to easily train a network using Leslie Smith&#39;s 1cycle policy. To learn more about the 1cycle technique for training neural networks check out Leslie Smith&#39;s paper and for more graphical and intuitive explanation checkout out Sylvain Gugger&#39;s post. . To use 1cycle policy we will need an optimum learning rate. We can find this learning rate by using a learning finder which can be called by using lr_finder as fastai does. It will do a mock training by going over a large range of learning rates, then plot them against the losses. We will then pick a value a bit before the minimum, where the loss still improves. Our graph would something like this: . . There is somthing to add, if we are transfer learning, we do not want to start off with too large a learning rate, or we will erase the intelligence of the model already contained in its weights. Instead, we begin with a very small learning rate and increase it gradually before lowering it again to fine-tune the weights. . . Important: After digging into the rabbit hole, I found there are two different learning rate schedule utility in tensorflow, the naming is very confusing, keras.optimizers.schedules.LearningRateSchedule and keras.callbacks.LearningRateScheduler. Although the naming is very similar, they are different in some senses. - The former is subclassing from tf.keras.optimizers while the latter is from tf.keras.Callback . The former schedule the learning rate per iteration while the former is per epoch. | . II. EfficientNet . (Read the EfficientNet paper and summarize in one of the section of this notebook) . EfficientNet, first introduced in Tan and Le, 2019 is among the most efficient models (i.e. requiring least FLOPS for inference) that reaches state-of-the-art accracy on both imagenet and common image classification transfer learning tasks. . The smallest base model is similar to MnasNet, which reached near-SOTA with a significantly smaller model. By introducing a heuristic way to scale the model, EfficientNet provides a family of models (B0 to B7) that represents a good combination of efficiency and accuracy on a variety of scales. Such a scaling heuristics (compound-scaling, details see Tan and Le, 2019) allows the efficiency-oriented base model (B0) to surpass models at every scale, while avoiding extensive grid-search of hyperparameters. . A summary of the latest updates on the model is available at here, where various augmentation schemes and semi-supervised learning approaches are applied to further improve the imagenet performance of the models. These extensions of the model can be used by updating weights without changing model topology . B0 to B7 variats of EfficientNet . Keras implementation of EfficientNet . An implementation of EfficientNet B0 to B7 has been shipped with tf.keras since TF2.3. To use EfficientNetB0 for classifying 1000 classes of images from imagenet, run: . from tensorflow.keras.applications import EfficientNetB0 model = EfficientNetB0(weights=&#39;imagenet&#39;) . The B0 model takes input images of shape (224,224,3), and the input data should range [0,255]. Normailzation is included as part of the model. . Because training EfficientNet on imagenet takes a tremendous amount of resources and several techniques that are not a part of the model architecture itself. Hence the Keras implementations by default loads pre-trained weights obtained via training with AutoAugment. . From B0 to B7 base model, the input shapes are different. Here is a list of input shpae expected for each model: . Base model resolution . EfficientNetB0 | 224 | . EfficientNetB1 | 240 | . EfficientNetB2 | 260 | . EfficientNetB3 | 300 | . EfficientNetB4 | 380 | . EfficientNetB5 | 456 | . EfficientNetB6 | 528 | . EfficientNetB7 | 600 | . When the model is intended for transfer learning, the Keras implementation provides a option to remove the top layers: . model = EfficientNetB0(include_top=False, weights=&#39;imagenet&#39;) . This option excludes the final Dense layer that turns 1280 features on the penultimate layer into prediction of the 1000 ImageNet classes. Replacing the top layer with custom layers allows using EfficientNet as a feature extractor in a transfer learning workflow. . Another argument in the model constructor worth noticing is drop_connect_rate which controls the dropout rate responsible for stochastic depth. This parameter serves as a toggle for extra regularization in finetuning, but does not affect loaded weights. For example, when stronger regularization is desired, try: . model = EfficientNetB0(weights=&#39;imagenet&#39;, drop_connect_rate=0.4) . The default value for drop_connect_rate is 0. . Clarification . AutoAugment . In this article, in section Keras implementation of EfficientNet, it says . Because training EfficientNet on ImageNet takes a tremendous amount of resources and several techniques that are not a part of the model architecture itself. Hence the Keras implementation by default loads pre-trained weights obtained via training with AutoAugment . It means the weights of keras EfficientNet are trained on the pre-trained from AutoAugment. My follow-up question is what dataset does the AutoAugment trained on? .",
            "url": "https://austinyhc.github.io/blog/personal%20development/motivation/habit/2021/01/06/continuous-self-motivation.html",
            "relUrl": "/personal%20development/motivation/habit/2021/01/06/continuous-self-motivation.html",
            "date": " • Jan 6, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Cassava Leaf Disease Classification",
            "content": "This notebook is a simple training pipeline in TensorFlow for the Cassava Leaf Competition where we are given 21,397 labeled images of cassava leaves classified as 5 different groups (4 diseases and a healthy group) and asked to predict on unseen images of cassava leaves. As with most image classification problems, we can use and experiment with many different forms of augmentation and we can explore transfer learning. . . Note: I am using Dimitre&#8217;s TFRecords that can be found here. He also has 128x128, 256x256, and 384x384 sized images that I added for experimental purposes. Please give his datasets an upvote (and his work in general, it is excellent). . import numpy as np import pandas as pd import seaborn as sns import albumentations as A import matplotlib.pyplot as plt import os, gc, cv2, random, warnings, math, sys, json, pprint, pdb import tensorflow as tf from tensorflow.keras import backend as K import tensorflow_hub as hub from sklearn.model_selection import train_test_split warnings.simplefilter(&#39;ignore&#39;) print(f&quot;Using TensorFlow v{tf.__version__}&quot;) . . Using TensorFlow v2.4.0 . . Tip: Adding seed helps reproduce results. Setting debug parameter wil run the model on smaller number of epochs to validate the architecture. . #@title Notebook type { run: &quot;auto&quot;, display-mode:&quot;form&quot; } SEED = 16 DEBUG = False #@param {type:&quot;boolean&quot;} TRAIN = True #@param {type:&quot;boolean&quot;} def seed_everything(seed=0): random.seed(seed) np.random.seed(seed) tf.random.set_seed(seed) os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) os.environ[&#39;TF_DETERMINISTIC_OPS&#39;] = &#39;1&#39; GOOGLE = &#39;google.colab&#39; in str(get_ipython()) KAGGLE = not GOOGLE seed_everything(SEED) print(&quot;Running on {}!&quot;.format( &quot;Google Colab&quot; if GOOGLE else &quot;Kaggle Kernel&quot; )) . Running on Google Colab! . Hyperparameters . #@title {run: &quot;auto&quot;, display-mode: &quot;form&quot; } BASE_MODEL= &#39;efficientnet_b3&#39; #@param [&quot;&#39;efficientnet_b3&#39;&quot;, &quot;&#39;efficientnet_b4&#39;&quot;, &quot;&#39;efficientnet_b2&#39;&quot;] {type:&quot;raw&quot;, allow-input: true} BATCH_SIZE = 32 #@param {type:&quot;integer&quot;} HEIGHT = 300#@param {type:&quot;number&quot;} WIDTH = 300#@param {type:&quot;number&quot;} CHANNELS = 3#@param {type:&quot;number&quot;} IMG_SIZE = (HEIGHT, WIDTH, CHANNELS) EPOCHS = 8#@param {type:&quot;number&quot;} print(&quot;Using {} with input size {}&quot;.format(BASE_MODEL, IMG_SIZE)) . Using efficientnet_b3 with input size (300, 300, 3) . Data . Exploring data . df = pd.read_csv(f&#39;{input_path}train.csv&#39;) df.head() . image_id label . 0 1000015157.jpg | 0 | . 1 1000201771.jpg | 3 | . 2 100042118.jpg | 1 | . 3 1000723321.jpg | 1 | . 4 1000812911.jpg | 3 | . Check how many images are available in the training dataset and also check if each item in the training set are unique . Number of training images: 21397 . True . The distribution of labels is obviously unbalanced as can be observed in the figure below. . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7efc0095a550&gt; . Let&#39;s preprocess to add the directory string to the filename and rename the column to filename . df[&#39;filename&#39;] = df[&#39;image_id&#39;].map(lambda x : f&#39;{input_path}train_images/{x}&#39;) df = df.drop(columns = [&#39;image_id&#39;]) df = df.sample(frac=1).reset_index(drop=True) . df.head() . label filename . 0 3 | /content/gdrive/MyDrive/kaggle/input/cassava-l... | . 1 3 | /content/gdrive/MyDrive/kaggle/input/cassava-l... | . 2 3 | /content/gdrive/MyDrive/kaggle/input/cassava-l... | . 3 1 | /content/gdrive/MyDrive/kaggle/input/cassava-l... | . 4 3 | /content/gdrive/MyDrive/kaggle/input/cassava-l... | . Let&#39;s find out what labels do we have for the 5 categories. . {&#39;0&#39;: &#39;Cassava Bacterial Blight (CBB)&#39;, &#39;1&#39;: &#39;Cassava Brown Streak Disease (CBSD)&#39;, &#39;2&#39;: &#39;Cassava Green Mottle (CGM)&#39;, &#39;3&#39;: &#39;Cassava Mosaic Disease (CMD)&#39;, &#39;4&#39;: &#39;Healthy&#39;} . From the bar chart shown earlier, the label 3, Cassava Mosaic Disease (CMD) is the most common one. This imbalance may have to be addressed with a weighted loss function or oversampling. I might try this in a future iteration of this kernel or in a new kernel. . Let&#39;s check an example image to see what it looks like . The size of the image is W800 x H600 . Loading data . After my quick and rough EDA, let&#39;s load the PIL Image to a Numpy array, so we can move on to data augmentation. . In fastai, they have item_tfms and batch_tfms defined for their data loader API. The item transforms performs a fairly large crop to 224 and also apply other standard augmentations (in aug_tranforms) at the batch level on the GPU. The batch size is set to 32 here. . Split the dataset into training set and validation set . train_df, valid_df = train_test_split( df ,test_size = 0.2 ,random_state = SEED ,shuffle = True ,stratify = df[&#39;label&#39;]) . train_ds = tf.data.Dataset.from_tensor_slices( (train_df.filename.values,train_df.label.values)) valid_ds = tf.data.Dataset.from_tensor_slices( (valid_df.filename.values, valid_df.label.values)) adapt_ds = tf.data.Dataset.from_tensor_slices( train_df.filename.values) . for x,y in valid_ds.take(3): print(x, y) . tf.Tensor(b&#39;/content/gdrive/MyDrive/kaggle/input/cassava-leaf-disease-classification/train_images/3227289141.jpg&#39;, shape=(), dtype=string) tf.Tensor(4, shape=(), dtype=int64) tf.Tensor(b&#39;/content/gdrive/MyDrive/kaggle/input/cassava-leaf-disease-classification/train_images/1494523424.jpg&#39;, shape=(), dtype=string) tf.Tensor(4, shape=(), dtype=int64) tf.Tensor(b&#39;/content/gdrive/MyDrive/kaggle/input/cassava-leaf-disease-classification/train_images/3290333742.jpg&#39;, shape=(), dtype=string) tf.Tensor(2, shape=(), dtype=int64) . Data transformation . In this stage we will collating the data and the label, and then do some basic data transformation so the image size can fit to the input size of the model. . Basically item transformations mainly make sure the input data is of the same size so that it can be collated in batches. . . Important: You may have noticed that I had not used any kind of normalization or rescaling. I recently discovered that there is Normalization layer included in Keras&#8217; pretrained EfficientNet, as mentioned here. . def decode_image(filename): img = tf.io.read_file(filename) img = tf.image.decode_jpeg(img, channels=3) return img def collate_train(filename, label): img = decode_image(filename) img = tf.image.random_brightness(img, 0.3) img = tf.image.random_flip_left_right(img, seed=None) img = tf.image.random_crop(img, IMG_SIZE) return img, label def process_adapt(filename): img = decode_image(filename) img = tf.keras.layers.experimental.preprocessing.Rescaling(1.0 / 255)(img) return img def collate_valid(filename, label): img = decode_image(filename) img = tf.image.random_crop(img, IMG_SIZE) return img, label . train_ds = train_ds.map(collate_train, num_parallel_calls=AUTOTUNE) valid_ds = valid_ds.map(collate_valid, num_parallel_calls=AUTOTUNE) adapt_ds = adapt_ds.map(process_adapt, num_parallel_calls=AUTOTUNE) . train_ds_batch = (train_ds .cache(&#39;dump.tfcache&#39;) .shuffle(buffer_size=1000) .batch(BATCH_SIZE) .prefetch(buffer_size=AUTOTUNE)) valid_ds_batch = (valid_ds #.shuffle(buffer_size=1000) .batch(BATCH_SIZE*2) .prefetch(buffer_size=AUTOTUNE)) adapt_ds_batch = (adapt_ds .shuffle(buffer_size=1000) .batch(BATCH_SIZE) .prefetch(buffer_size=AUTOTUNE)) . def show_images(ds): _,axs = plt.subplots(3,3,figsize=(16,16)) for ((x, y), ax) in zip(ds.take(9), axs.flatten()): ax.imshow(x.numpy().astype(np.uint8)) ax.set_title(np.argmax(y)) ax.axis(&#39;off&#39;) . Show some training images . Show some validation images . Model . Batch augmentation . data_augmentation = tf.keras.Sequential( [ tf.keras.layers.experimental.preprocessing.RandomCrop(HEIGHT, WIDTH), tf.keras.layers.experimental.preprocessing.RandomFlip(&quot;horizontal_and_vertical&quot;), tf.keras.layers.experimental.preprocessing.RandomRotation(0.25), tf.keras.layers.experimental.preprocessing.RandomZoom((-0.2, 0)), tf.keras.layers.experimental.preprocessing.RandomContrast((0.2,0.2)) ] ) . func = lambda x,y: (data_augmentation(x), y) x = (train_ds .batch(BATCH_SIZE) .take(1) .map(func, num_parallel_calls=AUTOTUNE)) . show_images(x.unbatch()) . Building a model . I am using an EfficientNetB3 on top of which I add some output layers to predict our 5 disease classes. I decided to load the imagenet pretrained weights locally to keep the internet off (part of the requirements to submit a kernal to this competition). . %%run_if {GOOGLE} from tensorflow.keras.applications import EfficientNetB3 from tensorflow.keras.applications import VGG16 . def build_model(base_model, num_class): inputs = tf.keras.layers.Input(shape=IMG_SIZE) x = data_augmentation(inputs) x = base_model(x) x = tf.keras.layers.Dropout(0.4)(x) outputs = tf.keras.layers.Dense(num_class, activation=&quot;softmax&quot;, name=&quot;pred&quot;)(x) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) return model . efficientnet = EfficientNetB3( weights = &#39;imagenet&#39; if TRAIN else None, include_top = False, input_shape = IMG_SIZE, pooling=&#39;avg&#39;) efficientnet.trainable = True model = build_model(base_model=efficientnet, num_class=len(id2label)) . Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5 43941888/43941136 [==============================] - 0s 0us/step . model.summary() . Model: &#34;model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 300, 300, 3)] 0 _________________________________________________________________ sequential (Sequential) (None, 300, 300, 3) 0 _________________________________________________________________ efficientnetb3 (Functional) (None, 1536) 10783535 _________________________________________________________________ dropout (Dropout) (None, 1536) 0 _________________________________________________________________ pred (Dense) (None, 5) 7685 ================================================================= Total params: 10,791,220 Trainable params: 10,703,917 Non-trainable params: 87,303 _________________________________________________________________ . Fine tune . The 3rd layer of the Efficient is the Normalization layer, which can be tuned to our new dataset instead of imagenet. Be patient on this one, it does take a bit of time as we&#39;re going through the entire training set. . %%run_if {GOOGLE and TRAIN} if not os.path.exists(&quot;000_normalization.h5&quot;): model.get_layer(&#39;efficientnetb3&#39;).get_layer(&#39;normalization&#39;).adapt(adapt_ds_batch) model.save_weights(&quot;000_normalization.h5&quot;) else: model.load_weights(&quot;000_normalization.h5&quot;) . Optimizer : CosineDecay . . Important: I always wanted to try the new CosineDecayRestarts function implemented in tf.keras as it seemed promising and I struggled to find the right settings (if there were any) for the ReduceLROnPlateau . %%run_if {TRAIN} #@title { run: &quot;auto&quot;, display-mode: &quot;form&quot; } STEPS = math.ceil(len(train_df) / BATCH_SIZE) * EPOCHS LR_START = 9e-3 #@param {type: &quot;number&quot;} LR_START *= strategy.num_replicas_in_sync LR_MIN = 3e-4 #@param {type: &quot;number&quot;} N_RESTARTS = 5#@param {type: &quot;number&quot;} T_MUL = 2.0 #@param {type: &quot;number&quot;} M_MUL = 1#@param {type: &quot;number&quot;} STEPS_START = math.ceil((T_MUL-1)/(T_MUL**(N_RESTARTS+1)-1) * STEPS) schedule = tf.keras.experimental.CosineDecayRestarts( first_decay_steps=STEPS_START, initial_learning_rate=LR_START, alpha=LR_MIN, m_mul=M_MUL, t_mul=T_MUL) x = [i for i in range(STEPS)] y = [schedule(s) for s in range(STEPS)] _,ax = plt.subplots(1,1,figsize=(8,5),facecolor=&#39;#F0F0F0&#39;) ax.plot(x, y) ax.set_facecolor(&#39;#F8F8F8&#39;) ax.set_xlabel(&#39;iteration&#39;) ax.set_ylabel(&#39;learning rate&#39;) print(&#39;{:d} total epochs and {:d} steps per epoch&#39; .format(EPOCHS, STEPS // EPOCHS)) print(schedule.get_config()) . 8 total epochs and 535 steps per epoch {&#39;initial_learning_rate&#39;: 0.009, &#39;first_decay_steps&#39;: 68, &#39;t_mul&#39;: 2.0, &#39;m_mul&#39;: 1, &#39;alpha&#39;: 0.0003, &#39;name&#39;: None} . . Warning: There is a gap between what I had expected and the acutal LearningRateScheduler that tensorflow gives us. The LearningRateScheduler update the lr on_epoch_begin while it makes more sense to do it on_batch_end or on_batch_begin. . Callbacks . LR finder . %%run_if {GOOGLE and TRAIN} from tensorflow.keras.callbacks import Callback class LRFinder(Callback): &quot;&quot;&quot;`Callback` that exponentially adjusts the learning rate after each training batch between `start_lr` and `end_lr` for a maximum number of batches: `max_step`. The loss and learning rate are recorded at each step allowing visually finding a good learning rate as https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html suggested. &quot;&quot;&quot; def __init__(self, start_lr: float = 1e-7, end_lr: float = 10, max_steps: int = 100, smoothing=0.9): super(LRFinder, self).__init__() self.start_lr, self.end_lr = start_lr, end_lr self.max_steps = max_steps self.smoothing = smoothing self.step, self.best_loss, self.avg_loss, self.lr = 0, 0, 0, 0 self.lrs, self.losses = [], [] def on_train_begin(self, logs=None): self.step, self.best_loss, self.avg_loss, self.lr = 0, 0, 0, 0 self.lrs, self.losses = [], [] def on_train_batch_begin(self, batch, logs=None): self.lr = self.exp_annealing(self.step) tf.keras.backend.set_value(self.model.optimizer.lr, self.lr) def on_train_batch_end(self, batch, logs=None): logs = logs or {} loss = logs.get(&#39;loss&#39;) step = self.step if loss: self.avg_loss = self.smoothing * self.avg_loss + (1 - self.smoothing) * loss smooth_loss = self.avg_loss / (1 - self.smoothing ** (self.step + 1)) self.losses.append(smooth_loss) self.lrs.append(self.lr) if step == 0 or loss &lt; self.best_loss: self.best_loss = loss if smooth_loss &gt; 4 * self.best_loss or tf.math.is_nan(smooth_loss): self.model.stop_training = True if step == self.max_steps: self.model.stop_training = True self.step += 1 def exp_annealing(self, step): return self.start_lr * (self.end_lr / self.start_lr) ** (step * 1. / self.max_steps) def plot(self, skip_end=None): lrs = self.lrs[:-skip_end] if skip_end else self.lrs[:-5] losses = self.losses[:-skip_end] if skip_end else self.losses[:-5] fig, ax = plt.subplots(1, 1, facecolor=&quot;#F0F0F0&quot;) ax.set_ylabel(&#39;Loss&#39;) ax.set_xlabel(&#39;Learning Rate&#39;) ax.set_xscale(&#39;log&#39;) ax.xaxis.set_major_formatter(plt.FormatStrFormatter(&#39;%.0e&#39;)) ax.plot(lrs, losses) . %%run_if {GOOGLE and TRAIN} model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;]) lr_finder = LRFinder() _ = model.fit(train_ds_batch, epochs=1, callbacks=[lr_finder]) . 6/535 [..............................] - ETA: 9:38 - loss: 1.7046 - accuracy: 0.2086WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.4558s vs `on_train_batch_end` time: 0.6337s). Check your callbacks. 535/535 [==============================] - 122s 200ms/step - loss: 14.9670 - accuracy: 0.3355 . %%run_if {GOOGLE and TRAIN} lr_finder.plot(skip_end=20) . As can be observed from the curve, we can pinpoint the lr_max to be 9e-3 and the lr_min to be 3e-4. Let&#39;s feed these hyperparams back to the optimizer schedule and retrain the model. . Before retraining, don&#39;t forget to reset the model so it can be trained from the 000_normalization.h5 rather than 1 epoch after it because executing the lr_finder . . Tip: I will create a repo, tflearner and have this implemented as a .reset method of a learner class. . %%run_if {GOOGLE and TRAIN} efficientnet = EfficientNetB3( weights = &#39;imagenet&#39;, include_top = False, input_shape = IMG_SIZE, pooling=&#39;avg&#39;) efficientnet.trainable = True model = build_model(base_model=efficientnet, num_class=len(id2label)) model.load_weights(&quot;000_normalization.h5&quot;) . Others . %%run_if {TRAIN} callbacks = [ tf.keras.callbacks.ModelCheckpoint( filepath=&#39;001_best_model.h5&#39;, monitor=&#39;val_loss&#39;, save_best_only=True), ] . %%run_if {TRAIN} model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(schedule), metrics=[&quot;accuracy&quot;]) . Training . %%run_if {TRAIN} history = model.fit(train_ds_batch, epochs = EPOCHS, validation_data=valid_ds_batch, callbacks=callbacks) . Epoch 1/8 535/535 [==============================] - 1104s 2s/step - loss: 1.3144 - accuracy: 0.5887 - val_loss: 2.8411 - val_accuracy: 0.6185 Epoch 2/8 535/535 [==============================] - 642s 1s/step - loss: 0.9649 - accuracy: 0.6520 - val_loss: 2.7552 - val_accuracy: 0.6407 Epoch 3/8 535/535 [==============================] - 641s 1s/step - loss: 0.9434 - accuracy: 0.6511 - val_loss: 0.8542 - val_accuracy: 0.6979 Epoch 4/8 535/535 [==============================] - 642s 1s/step - loss: 0.7773 - accuracy: 0.7152 - val_loss: 1.6173 - val_accuracy: 0.6336 Epoch 5/8 535/535 [==============================] - 643s 1s/step - loss: 0.8693 - accuracy: 0.6780 - val_loss: 2.3116 - val_accuracy: 0.6271 Epoch 6/8 535/535 [==============================] - 644s 1s/step - loss: 0.7839 - accuracy: 0.7174 - val_loss: 0.8465 - val_accuracy: 0.6956 Epoch 7/8 535/535 [==============================] - 645s 1s/step - loss: 0.6877 - accuracy: 0.7509 - val_loss: 0.6638 - val_accuracy: 0.7584 Epoch 8/8 535/535 [==============================] - 644s 1s/step - loss: 0.6153 - accuracy: 0.7741 - val_loss: 0.5781 - val_accuracy: 0.7902 . Evaluating . def show_history(history): topics = [&#39;loss&#39;, &#39;accuracy&#39;] groups = [{k:v for (k,v) in history.items() if topic in k} for topic in topics] _,axs = plt.subplots(1,2,figsize=(15,6),facecolor=&#39;#F0F0F0&#39;) for topic,group,ax in zip(topics,groups,axs.flatten()): for (_,v) in group.items(): ax.plot(v) ax.set_facecolor(&#39;#F8F8F8&#39;) ax.set_title(f&#39;{topic} over epochs&#39;) ax.set_xlabel(&#39;epoch&#39;) ax.set_ylabel(topic) ax.legend([&#39;train&#39;, &#39;valid&#39;], loc=&#39;best&#39;) . %%run_if {TRAIN} show_history(history.history) . We load the best weight that were kept from the training phase. Just to check how our model is performing, we will attempt predictions over the validation set. This can help to highlight any classes that will be consistently miscategorised. . model.load_weights(&#39;{}001_best_model.h5&#39;.format( &#39;&#39; if TRAIN else &#39;../input/cassava-leaf-disease-classification-models/&#39;)) . Prediction . x = train_df.sample(1).filename.values[0] img = decode_image(x) . %%time imgs = [tf.image.random_crop(img, size=IMG_SIZE) for _ in range(4)] _,axs = plt.subplots(1,4,figsize=(16,4)) for (x, ax) in zip(imgs, axs.flatten()): ax.imshow(x.numpy().astype(np.uint8)) ax.axis(&#39;off&#39;) . CPU times: user 61.7 ms, sys: 20 µs, total: 61.7 ms Wall time: 60.5 ms . I apply some very basic test time augmentation to every local image extracted from the original 600-by-800 images. We know we can do some fancy augmentation with albumentations but I wanted to do that exclusively with Keras preprocessing layers to keep the cleanest pipeline possible. . tta = tf.keras.Sequential( [ tf.keras.layers.experimental.preprocessing.RandomCrop(HEIGHT, WIDTH), tf.keras.layers.experimental.preprocessing.RandomFlip(&quot;horizontal_and_vertical&quot;), tf.keras.layers.experimental.preprocessing.RandomZoom((-0.2, 0.2)), tf.keras.layers.experimental.preprocessing.RandomContrast((0.2,0.2)) ] ) . def predict_tta(filename, num_tta=4): img = decode_image(filename) img = tf.expand_dims(img, 0) imgs = tf.concat([tta(img) for _ in range(num_tta)], 0) preds = model.predict(imgs) return preds.sum(0).argmax() . pred = predict_tta(df.sample(1).filename.values[0]) print(pred) . from tqdm import tqdm preds = [] with tqdm(total=len(valid_df)) as pbar: for filename in valid_df.filename: pbar.update() preds.append(predict_tta(filename, num_tta=4)) . cm = tf.math.confusion_matrix(valid_df.label.values, np.array(preds)) plt.figure(figsize=(10, 8)) sns.heatmap(cm, xticklabels=id2label.values(), yticklabels=id2label.values(), annot=True, fmt=&#39;g&#39;, cmap=&quot;Blues&quot;) plt.xlabel(&#39;Prediction&#39;) plt.ylabel(&#39;Label&#39;) plt.show() . test_folder = input_path + &#39;/test_images/&#39; submission_df = pd.DataFrame(columns={&quot;image_id&quot;,&quot;label&quot;}) submission_df[&quot;image_id&quot;] = os.listdir(test_folder) submission_df[&quot;label&quot;] = 0 . submission_df[&#39;label&#39;] = (submission_df[&#39;image_id&#39;] .map(lambda x : predict_tta(test_folder+x))) . submission_df . submission_df.to_csv(&quot;submission.csv&quot;, index=False) . 1% Better Everyday . reference . . todos . . done .",
            "url": "https://austinyhc.github.io/blog/plant/disease/classification/efficientnet/2021/01/06/cassava-leaf-disease-classification.html",
            "relUrl": "/plant/disease/classification/efficientnet/2021/01/06/cassava-leaf-disease-classification.html",
            "date": " • Jan 6, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Petals to the Metal",
            "content": "import numpy as np import pandas as pd import seaborn as sns import albumentations as A import matplotlib.pyplot as plt import os, gc, cv2, random, re import warnings, math, sys, json import subprocess, pprint, pdb import tensorflow as tf from tensorflow.keras import backend as K import tensorflow_hub as hub from sklearn.model_selection import train_test_split from sklearn.metrics import f1_score,precision_score, recall_score, confusion_matrix warnings.simplefilter(&#39;ignore&#39;) print(f&quot;Using TensorFlow v{tf.__version__}&quot;) . . Using TensorFlow v2.4.0 . . Tip: Adding seed helps reproduce results. Setting debug parameter wil run the model on smaller number of epochs to validate the architecture. . def seed_everything(seed=0): random.seed(seed) np.random.seed(seed) tf.random.set_seed(seed) os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) os.environ[&#39;TF_DETERMINISTIC_OPS&#39;] = &#39;1&#39; GOOGLE = &#39;google.colab&#39; in str(get_ipython()) KAGGLE = not GOOGLE print(&quot;Running on {}!&quot;.format( &quot;Google Colab&quot; if GOOGLE else &quot;Kaggle Kernel&quot; )) . . Running on Google Colab! . TFRecord basics . GCS_PATTERN = &#39;gs://flowers-public/*/*.jpg&#39; GCS_OUTPUT = &#39;gs://flowers-public/tfrecords-jpeg-192x192-2/flowers&#39; SHARDS = 16 TARGET_SIZE = [192, 192] CLASSES = [b&#39;daisy&#39;, b&#39;dandelion&#39;, b&#39;roses&#39;, b&#39;sunflowers&#39;, b&#39;tulips&#39;] . . Read images and labels . def decode_image_and_label(filename): bits = tf.io.read_file(filename) image = tf.image.decode_jpeg(bits) label = tf.strings.split(tf.expand_dims(filename, axis=-1), sep=&#39;/&#39;) #label = tf.strings.split(filename, sep=&#39;/&#39;) label = label.values[-2] label = tf.cast((CLASSES==label), tf.int8) return image, label . . filenames = tf.data.Dataset.list_files(GCS_PATTERN, seed=16) for x in filenames.take(3): print(x) . tf.Tensor(b&#39;gs://flowers-public/tulips/251811158_75fa3034ff.jpg&#39;, shape=(), dtype=string) tf.Tensor(b&#39;gs://flowers-public/daisy/506348009_9ecff8b6ef.jpg&#39;, shape=(), dtype=string) tf.Tensor(b&#39;gs://flowers-public/daisy/2019064575_7656b9340f_m.jpg&#39;, shape=(), dtype=string) . def show_images(ds): _,axs = plt.subplots(3,3,figsize=(16,16)) for ((x, y), ax) in zip(ds.take(9), axs.flatten()): ax.imshow(x.numpy().astype(np.uint8)) ax.set_title(np.argmax(y)) ax.axis(&#39;off&#39;) . . ds0 = filenames.map(decode_image_and_label, num_parallel_calls=AUTOTUNE) show_images(ds0) . Resize and crop images to common size . No need to study the code in this cell. It&#39;s only image resizing. . def resize_and_crop_image(image, label): # Resize and crop using &quot;fill&quot; algorithm: # always make sure the resulting image # is cut out from the source image so that # it fills the TARGET_SIZE entirely with no # black bars and a preserved aspect ratio. w = tf.shape(image)[0] h = tf.shape(image)[1] tw = TARGET_SIZE[1] th = TARGET_SIZE[0] resize_crit = (w * th) / (h * tw) image = tf.cond(resize_crit &lt; 1, lambda: tf.image.resize(image, [w*tw/w, h*tw/w]), # if true lambda: tf.image.resize(image, [w*th/h, h*th/h]) # if false ) nw = tf.shape(image)[0] nh = tf.shape(image)[1] image = tf.image.crop_to_bounding_box(image, (nw - tw) // 2, (nh - th) // 2, tw, th) return image, label . . ds1 = ds0.map(resize_and_crop_image, num_parallel_calls=AUTOTUNE) show_images(ds1) . Speed test: too slow . Google Cloud Storage is capable of great throughput but has a per-file access penalty. Run the cell below and see that throughput is around 8 images per second. That is too slow. Training on thousands of individual files will not work. We have to use the TFRecord format to group files together. . %%time for image,label in ds1.batch(8).take(10): print(&quot;Image batch shape {} {}&quot;.format( image.numpy().shape, [np.argmax(lbl) for lbl in label.numpy()])) . Image batch shape (8, 192, 192, 3) [0, 1, 0, 0, 1, 3, 2, 1] Image batch shape (8, 192, 192, 3) [3, 4, 4, 0, 3, 4, 3, 0] Image batch shape (8, 192, 192, 3) [0, 3, 0, 4, 2, 4, 2, 4] Image batch shape (8, 192, 192, 3) [3, 4, 4, 0, 2, 3, 2, 3] Image batch shape (8, 192, 192, 3) [1, 3, 4, 3, 0, 3, 1, 3] Image batch shape (8, 192, 192, 3) [4, 4, 3, 0, 0, 4, 4, 1] Image batch shape (8, 192, 192, 3) [1, 3, 1, 3, 1, 2, 4, 2] Image batch shape (8, 192, 192, 3) [1, 4, 2, 4, 2, 2, 1, 0] Image batch shape (8, 192, 192, 3) [0, 3, 2, 2, 3, 4, 0, 1] Image batch shape (8, 192, 192, 3) [1, 2, 0, 1, 0, 3, 4, 1] CPU times: user 60 ms, sys: 54.4 ms, total: 114 ms Wall time: 5.62 s . Recompress the images . The bandwidth savings outweight the decoding CPU cost . def recompress_image(image, label): height = tf.shape(image)[0] width = tf.shape(image)[1] image = tf.cast(image, tf.uint8) image = tf.image.encode_jpeg(image, optimize_size=True, chroma_downsampling=False) return image, label, height, width . . IMAGE_SIZE = len(tf.io.gfile.glob(GCS_PATTERN)) SHARD_SIZE = math.ceil(1.0 * IMAGE_SIZE / SHARDS) . ds2 = ds1.map(recompress_image, num_parallel_calls=AUTOTUNE) ds2 = ds2.batch(SHARD_SIZE) . Why TFRecords? . TPUs have eight cores which act as eight independent workers. We can get data to each core more efficiently by splitting the dataset into multiple files or shards. This way, each core can grab an independent part of the data as it needs. The most convenient kind of file to use for sharding in TensorFlow is a TFRecord. A TFRecord is a binary file that contains sequences of byte-strings. Data needs to be serialized (encoded as a byte-string) before being written into a TFRecord. The most convenient way of serializing data in TensorFlow is to wrap the data with tf.Example. This is a record format based on Google&#39;s protobufs but designed for TensorFlow. It&#39;s more or less like a dict with some type annotations . x = tf.constant([[1,2], [3, 4]], dtype=tf.uint8) print(x) . tf.Tensor( [[1 2] [3 4]], shape=(2, 2), dtype=uint8) . x_in_bytes = tf.io.serialize_tensor(x) print(x_in_bytes) . tf.Tensor(b&#39; x08 x04 x12 x08 x12 x02 x08 x02 x12 x02 x08 x02&#34; x04 x01 x02 x03 x04&#39;, shape=(), dtype=string) . print(tf.io.parse_tensor(x_in_bytes, out_type=tf.uint8)) . tf.Tensor( [[1 2] [3 4]], shape=(2, 2), dtype=uint8) . A TFRecord is a sequence of bytes, so we have to turn our data into byte-strings before it can go into a TFRecord. We can use tf.io.serialize_tensor to turn a tensor into a byte-string and tf.io.parse_tensor to turn it back. It&#39;s important to keep track of your tensor&#39;s datatype (in this case tf.uint8) since you have to specify it when parsing the string back to a tensor again . Write dataset to TFRecord files . . Note: Will uncomment the cells in this section when I find a gs:// domain to write to. . Read from TFRecord Dataset . def read_tfrecord(example): features = { &quot;image&quot;: tf.io.FixedLenFeature([], tf.string), # tf.string = bytestring (not text string) &quot;class&quot;: tf.io.FixedLenFeature([], tf.int64), # shape [] means scalar # additional (not very useful) fields to demonstrate TFRecord writing/reading of different types of data &quot;label&quot;: tf.io.FixedLenFeature([], tf.string), # one bytestring &quot;size&quot;: tf.io.FixedLenFeature([2], tf.int64), # two integers &quot;one_hot_class&quot;: tf.io.VarLenFeature(tf.float32) # a certain number of floats } # decode the TFRecord example = tf.io.parse_single_example(example, features) # FixedLenFeature fields are now ready to use: exmple[&#39;size&#39;] # VarLenFeature fields require additional sparse_to_dense decoding image = tf.image.decode_jpeg(example[&#39;image&#39;], channels=3) image = tf.reshape(image, [*TARGET_SIZE, 3]) class_num = example[&#39;class&#39;] label = example[&#39;label&#39;] height = example[&#39;size&#39;][0] width = example[&#39;size&#39;][1] one_hot_class = tf.sparse.to_dense(example[&#39;one_hot_class&#39;]) return image, class_num, label, height, width, one_hot_class . . option_no_order = tf.data.Options() option_no_order.experimental_deterministic = False filenames = tf.io.gfile.glob(GCS_OUTPUT + &quot;*tfrec&quot;) ds3 = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) ds3 = (ds3.with_options(option_no_order) .map(read_tfrecord, num_parallel_calls=AUTOTUNE) .shuffle(30)) . ds3_to_show = ds3.map(lambda image, id, label, height, width, one_hot: (image, label)) show_images(ds3_to_show) . Speed test: fast . Loading form TFRecords is almost 10x time faster than loading from JPEGs. . %%time for image, class_num, label, height, width, one_hot_class in ds3.batch(8).take(10): print(&quot;Image batch shape {} {}&quot;.format( image.numpy().shape, [lbl.decode(&#39;utf8&#39;) for lbl in label.numpy()])) . Image batch shape (8, 192, 192, 3) [&#39;roses&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;, &#39;tulips&#39;, &#39;daisy&#39;, &#39;dandelion&#39;, &#39;tulips&#39;] Image batch shape (8, 192, 192, 3) [&#39;roses&#39;, &#39;dandelion&#39;, &#39;sunflowers&#39;, &#39;tulips&#39;, &#39;sunflowers&#39;, &#39;dandelion&#39;, &#39;roses&#39;, &#39;dandelion&#39;] Image batch shape (8, 192, 192, 3) [&#39;daisy&#39;, &#39;daisy&#39;, &#39;sunflowers&#39;, &#39;roses&#39;, &#39;roses&#39;, &#39;daisy&#39;, &#39;sunflowers&#39;, &#39;tulips&#39;] Image batch shape (8, 192, 192, 3) [&#39;roses&#39;, &#39;roses&#39;, &#39;roses&#39;, &#39;tulips&#39;, &#39;sunflowers&#39;, &#39;tulips&#39;, &#39;tulips&#39;, &#39;tulips&#39;] Image batch shape (8, 192, 192, 3) [&#39;sunflowers&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;, &#39;daisy&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;, &#39;sunflowers&#39;, &#39;roses&#39;] Image batch shape (8, 192, 192, 3) [&#39;roses&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;, &#39;sunflowers&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;] Image batch shape (8, 192, 192, 3) [&#39;dandelion&#39;, &#39;sunflowers&#39;, &#39;daisy&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;, &#39;dandelion&#39;, &#39;daisy&#39;] Image batch shape (8, 192, 192, 3) [&#39;roses&#39;, &#39;daisy&#39;, &#39;dandelion&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;, &#39;daisy&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;] Image batch shape (8, 192, 192, 3) [&#39;tulips&#39;, &#39;tulips&#39;, &#39;roses&#39;, &#39;dandelion&#39;, &#39;sunflowers&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;, &#39;tulips&#39;] Image batch shape (8, 192, 192, 3) [&#39;daisy&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;, &#39;dandelion&#39;, &#39;daisy&#39;, &#39;roses&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;] CPU times: user 32.5 ms, sys: 12.8 ms, total: 45.3 ms Wall time: 184 ms . Hyperparameters . BASE_MODEL = &#39;efficientnet_b3&#39; #@param [&quot;&#39;efficientnet_b3&#39;&quot;, &quot;&#39;efficientnet_b4&#39;&quot;, &quot;&#39;efficientnet_b2&#39;&quot;] {type:&quot;raw&quot;, allow-input: true} HEIGHT = 300#@param {type:&quot;number&quot;} WIDTH = 300#@param {type:&quot;number&quot;} CHANNELS = 3#@param {type:&quot;number&quot;} IMG_SIZE = (HEIGHT, WIDTH, CHANNELS) EPOCHS = 50#@param {type:&quot;number&quot;} BATCH_SIZE = 32 * strategy.num_replicas_in_sync #@param {type:&quot;raw&quot;} print(&quot;Use {} with input size {}&quot;.format(BASE_MODEL, IMG_SIZE)) print(&quot;Train on batch size of {} for {} epochs&quot;.format(BATCH_SIZE, EPOCHS)) . Use efficientnet_b3 with input size (300, 300, 3) Train on batch size of 256 for 50 epochs . Data . Loading data . %%run_if {GOOGLE} #@title {run: &quot;auto&quot;, display-mode: &quot;form&quot;} GCS_PATH = &#39;gs://kds-c6b9829baa483a13a169c7cbe266341fb8c9b5ba36843af37a093a4c&#39; #@param {type: &quot;string&quot;} GCS_PATH += &#39;/tfrecords-jpeg-512x512&#39; #@param {type: &quot;string&quot;} print(f&quot;Sourcing images from {GCS_PATH}&quot;) . Sourcing images from gs://kds-c6b9829baa483a13a169c7cbe266341fb8c9b5ba36843af37a093a4c/tfrecords-jpeg-512x512 . CLASSES = [&#39;pink primrose&#39;, &#39;hard-leaved pocket orchid&#39;, &#39;canterbury bells&#39;, &#39;sweet pea&#39;, &#39;wild geranium&#39;, &#39;tiger lily&#39;, &#39;moon orchid&#39;, &#39;bird of paradise&#39;, &#39;monkshood&#39;, &#39;globe thistle&#39;, &#39;snapdragon&#39;, &quot;colt&#39;s foot&quot;, &#39;king protea&#39;, &#39;spear thistle&#39;, &#39;yellow iris&#39;, &#39;globe-flower&#39;, &#39;purple coneflower&#39;, &#39;peruvian lily&#39;, &#39;balloon flower&#39;,&#39;giant white arum lily&#39;, &#39;fire lily&#39;, &#39;pincushion flower&#39;, &#39;fritillary&#39;, &#39;red ginger&#39;, &#39;grape hyacinth&#39;, &#39;corn poppy&#39;, &#39;prince of wales feathers&#39;, &#39;stemless gentian&#39;, &#39;artichoke&#39;, &#39;sweet william&#39;, &#39;carnation&#39;, &#39;garden phlox&#39;, &#39;love in the mist&#39;, &#39;cosmos&#39;, &#39;alpine sea holly&#39;, &#39;ruby-lipped cattleya&#39;, &#39;cape flower&#39;, &#39;great masterwort&#39;, &#39;siam tulip&#39;, &#39;lenten rose&#39;, &#39;barberton daisy&#39;, &#39;daffodil&#39;, &#39;sword lily&#39;, &#39;poinsettia&#39;, &#39;bolero deep blue&#39;, &#39;wallflower&#39;, &#39;marigold&#39;, &#39;buttercup&#39;, &#39;daisy&#39;, &#39;common dandelion&#39;, &#39;petunia&#39;, &#39;wild pansy&#39;, &#39;primula&#39;, &#39;sunflower&#39;, &#39;lilac hibiscus&#39;, &#39;bishop of llandaff&#39;, &#39;gaura&#39;, &#39;geranium&#39;, &#39;orange dahlia&#39;, &#39;pink-yellow dahlia&#39;, &#39;cautleya spicata&#39;, &#39;japanese anemone&#39;, &#39;black-eyed susan&#39;, &#39;silverbush&#39;, &#39;californian poppy&#39;, &#39;osteospermum&#39;, &#39;spring crocus&#39;, &#39;iris&#39;, &#39;windflower&#39;, &#39;tree poppy&#39;, &#39;gazania&#39;, &#39;azalea&#39;, &#39;water lily&#39;, &#39;rose&#39;, &#39;thorn apple&#39;, &#39;morning glory&#39;, &#39;passion flower&#39;, &#39;lotus&#39;, &#39;toad lily&#39;, &#39;anthurium&#39;, &#39;frangipani&#39;, &#39;clematis&#39;, &#39;hibiscus&#39;, &#39;columbine&#39;, &#39;desert-rose&#39;, &#39;tree mallow&#39;, &#39;magnolia&#39;, &#39;cyclamen &#39;, &#39;watercress&#39;, &#39;canna lily&#39;, &#39;hippeastrum &#39;, &#39;bee balm&#39;, &#39;pink quill&#39;, &#39;foxglove&#39;, &#39;bougainvillea&#39;, &#39;camellia&#39;, &#39;mallow&#39;, &#39;mexican petunia&#39;, &#39;bromelia&#39;, &#39;blanket flower&#39;, &#39;trumpet creeper&#39;, &#39;blackberry lily&#39;, &#39;common tulip&#39;, &#39;wild rose&#39;] NCLASSES = len(CLASSES) print(f&quot;Number of labels: {NCLASSES}&quot;) . . Number of labels: 104 . def decode_image(image_data): image = tf.image.decode_jpeg(image_data, channels=CHANNELS) image = (tf.cast(image, tf.float32) if GOOGLE else tf.cast(image, tf.float32) / 255.0) image = tf.image.random_crop(image, IMG_SIZE) return image def collate_labeled_tfrecord(example): LABELED_TFREC_FORMAT = { &quot;image&quot;: tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring &quot;class&quot;: tf.io.FixedLenFeature([], tf.int64), # shape [] means single element } example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT) image = decode_image(example[&#39;image&#39;]) label = tf.cast(example[&#39;class&#39;], tf.int32) return image, label def process_unlabeled_tfrecord(example): UNLABELED_TFREC_FORMAT = { &quot;image&quot;: tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring &quot;id&quot;: tf.io.FixedLenFeature([], tf.string), # shape [] means single element } example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT) image = decode_image(example[&#39;image&#39;]) idnum = example[&#39;id&#39;] return image, idnum def count_data_items(filenames): n = [int(re.compile(r&quot;-([0-9]*) .&quot;).search(filename).group(1)) for filename in filenames] return np.sum(n) . . train_filenames = tf.io.gfile.glob(GCS_PATH + &#39;/train/*.tfrec&#39;) valid_filenames = tf.io.gfile.glob(GCS_PATH + &#39;/val/*.tfrec&#39;) test_filenames = tf.io.gfile.glob(GCS_PATH + &#39;/test/*.tfrec&#39;) . Number of train set: 12753 Number of valid set: 3712 Number of test set: 7382 . Data augmentation . . Note: The following data augmentation functions are referenced from Data Augmentation using GPU/TPU for Maximum Speed! by @cdeotte . def transform_shear(image, height, shear): # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3] # output - image randomly sheared DIM = height XDIM = DIM%2 #fix for size 331 shear = shear * tf.random.uniform([1],dtype=&#39;float32&#39;) shear = math.pi * shear / 180. # SHEAR MATRIX one = tf.constant([1],dtype=&#39;float32&#39;) zero = tf.constant([0],dtype=&#39;float32&#39;) c2 = tf.math.cos(shear) s2 = tf.math.sin(shear) shear_matrix = tf.reshape(tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3]) # LIST DESTINATION PIXEL INDICES x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM ) y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] ) z = tf.ones([DIM*DIM],dtype=&#39;int32&#39;) idx = tf.stack( [x,y,z] ) # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS idx2 = K.dot(shear_matrix,tf.cast(idx,dtype=&#39;float32&#39;)) idx2 = K.cast(idx2,dtype=&#39;int32&#39;) idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2) # FIND ORIGIN PIXEL VALUES idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] ) d = tf.gather_nd(image, tf.transpose(idx3)) return tf.reshape(d,[DIM,DIM,3]) . . def transform_shift(image, height, h_shift, w_shift): # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3] # output - image randomly shifted DIM = height XDIM = DIM%2 #fix for size 331 height_shift = h_shift * tf.random.uniform([1],dtype=&#39;float32&#39;) width_shift = w_shift * tf.random.uniform([1],dtype=&#39;float32&#39;) one = tf.constant([1],dtype=&#39;float32&#39;) zero = tf.constant([0],dtype=&#39;float32&#39;) # SHIFT MATRIX shift_matrix = tf.reshape(tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3]) # LIST DESTINATION PIXEL INDICES x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM ) y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] ) z = tf.ones([DIM*DIM],dtype=&#39;int32&#39;) idx = tf.stack( [x,y,z] ) # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS idx2 = K.dot(shift_matrix,tf.cast(idx,dtype=&#39;float32&#39;)) idx2 = K.cast(idx2,dtype=&#39;int32&#39;) idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2) # FIND ORIGIN PIXEL VALUES idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] ) d = tf.gather_nd(image, tf.transpose(idx3)) return tf.reshape(d,[DIM,DIM,3]) . . def transform_rotation(image, height, rotation): # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3] # output - image randomly rotated DIM = height XDIM = DIM%2 #fix for size 331 rotation = rotation * tf.random.uniform([1],dtype=&#39;float32&#39;) # CONVERT DEGREES TO RADIANS rotation = math.pi * rotation / 180. # ROTATION MATRIX c1 = tf.math.cos(rotation) s1 = tf.math.sin(rotation) one = tf.constant([1],dtype=&#39;float32&#39;) zero = tf.constant([0],dtype=&#39;float32&#39;) rotation_matrix = tf.reshape(tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3]) # LIST DESTINATION PIXEL INDICES x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM ) y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] ) z = tf.ones([DIM*DIM],dtype=&#39;int32&#39;) idx = tf.stack( [x,y,z] ) # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS idx2 = K.dot(rotation_matrix,tf.cast(idx,dtype=&#39;float32&#39;)) idx2 = K.cast(idx2,dtype=&#39;int32&#39;) idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2) # FIND ORIGIN PIXEL VALUES idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] ) d = tf.gather_nd(image, tf.transpose(idx3)) return tf.reshape(d,[DIM,DIM,3]) . . def data_augment(image, label): p_rotation = tf.random.uniform([], 0, 1.0, dtype=tf.float32) p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32) p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32) p_pixel = tf.random.uniform([], 0, 1.0, dtype=tf.float32) p_shear = tf.random.uniform([], 0, 1.0, dtype=tf.float32) p_shift = tf.random.uniform([], 0, 1.0, dtype=tf.float32) p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32) # Flips if p_spatial &gt;= .2: image = tf.image.random_flip_left_right(image) image = tf.image.random_flip_up_down(image) # Rotates if p_rotate &gt; .75: image = tf.image.rot90(image, k=3) # rotate 270º elif p_rotate &gt; .5: image = tf.image.rot90(image, k=2) # rotate 180º elif p_rotate &gt; .25: image = tf.image.rot90(image, k=1) # rotate 90º if p_rotation &gt;= .3: # Rotation image = transform_rotation(image, height=HEIGHT, rotation=45.) if p_shift &gt;= .3: # Shift image = transform_shift(image, height=HEIGHT, h_shift=15., w_shift=15.) if p_shear &gt;= .3: # Shear image = transform_shear(image, height=HEIGHT, shear=20.) # Crops if p_crop &gt; .4: crop_size = tf.random.uniform([], int(HEIGHT*.7), HEIGHT, dtype=tf.int32) image = tf.image.random_crop(image, size=[crop_size, crop_size, CHANNELS]) elif p_crop &gt; .7: if p_crop &gt; .9: image = tf.image.central_crop(image, central_fraction=.7) elif p_crop &gt; .8: image = tf.image.central_crop(image, central_fraction=.8) else: image = tf.image.central_crop(image, central_fraction=.9) image = tf.image.resize(image, size=[HEIGHT, WIDTH]) # Pixel-level transforms if p_pixel &gt;= .2: if p_pixel &gt;= .8: image = tf.image.random_saturation(image, lower=0, upper=2) elif p_pixel &gt;= .6: image = tf.image.random_contrast(image, lower=.8, upper=2) elif p_pixel &gt;= .4: image = tf.image.random_brightness(image, max_delta=.2) else: image = tf.image.adjust_gamma(image, gamma=.6) return image, label . . . Tip: experimental_deterministic is set to decide whether the outputs need to be produced in deterministic order. Default: True . option_no_order = tf.data.Options() option_no_order.experimental_deterministic = False . train_ds = tf.data.TFRecordDataset(train_filenames, num_parallel_reads=AUTOTUNE) train_ds = (train_ds .map(collate_labeled_tfrecord, num_parallel_calls=AUTOTUNE) .map(data_augment, num_parallel_calls=AUTOTUNE) .repeat() .shuffle(2048) .batch(BATCH_SIZE) .prefetch(AUTOTUNE)) . valid_ds = tf.data.TFRecordDataset(valid_filenames, num_parallel_reads=AUTOTUNE) valid_ds = (valid_ds .with_options(option_no_order) .map(collate_labeled_tfrecord, num_parallel_calls=AUTOTUNE) .batch(BATCH_SIZE) .cache() .prefetch(AUTOTUNE)) . test_ds = tf.data.TFRecordDataset(test_filenames, num_parallel_reads=AUTOTUNE) test_ds = (test_ds .with_options(option_no_order) .map(process_unlabeled_tfrecord, num_parallel_calls=AUTOTUNE) .batch(BATCH_SIZE) .prefetch(AUTOTUNE)) . Model . Batch augmentation . Augmentation can be applied in two ways. . Using the Keras Preprocessing Layers | Using the tf.image | . . Important: The Keras Preprocessing Layers are currently experimental so it seems it does not have supporting TPU OpKernel yet. . #batch_augment = tf.keras.Sequential( # [ # tf.keras.layers.experimental.preprocessing.RandomCrop(*IMG_SIZE), # tf.keras.layers.experimental.preprocessing.RandomFlip(&quot;horizontal_and_vertical&quot;), # tf.keras.layers.experimental.preprocessing.RandomRotation(0.25), # tf.keras.layers.experimental.preprocessing.RandomZoom((-0.2, 0)), # tf.keras.layers.experimental.preprocessing.RandomContrast((0.2,0.2)) # ] #) . . #func = lambda x,y: (batch_augment(x), y) #x = (train_ds # .take(1) # .map(func, num_parallel_calls=AUTOTUNE)) . . Building a model . Now we&#39;re ready to create a neural network for classifying images! We&#39;ll use what&#39;s known as transfer learning. With transfer learning, you reuse the body part of a pretrained model and replace its&#39; head or tail with custom layers depending on the problem that we are solving. . For this tutorial, we&#39;ll use EfficientNetb3 which is pretrained on ImageNet. Later, I might want to experiment with other models. (Xception wouldn&#39;t be a bad choice.) . . Important: The distribution strategy we created earilier contains a context manager, straategy.scope. This context manager tells TensorFlow how to divide the work of training among the eight TPU cores. When using TensorFlow with a TPU, it&#8217;s important to define your model in strategy.sceop() context. . %%run_if {GOOGLE} from tensorflow.keras.applications import EfficientNetB3 from tensorflow.keras.applications import VGG16 . def build_model(base_model, num_class): inputs = tf.keras.layers.Input(shape=IMG_SIZE) x = base_model(inputs) x = tf.keras.layers.Dropout(0.4)(x) outputs = tf.keras.layers.Dense(num_class, activation=&quot;softmax&quot;, name=&quot;pred&quot;)(x) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) return model . with strategy.scope(): efficientnet = EfficientNetB3( weights = &#39;imagenet&#39; if TRAIN else None, include_top = False, input_shape = IMG_SIZE, pooling=&#39;avg&#39;) efficientnet.trainable = True model = build_model(base_model=efficientnet, num_class=len(CLASSES)) . Optimizer . . Important: I always wanted to try the new CosineDecayRestarts function implemented in tf.keras as it seemed promising and I struggled to find the right settings (if there were any) for the ReduceLROnPlateau . STEPS = math.ceil(count_data_items(train_filenames) / BATCH_SIZE) * EPOCHS LR_START = 1e-4 #@param {type: &quot;number&quot;} LR_START *= strategy.num_replicas_in_sync LR_MIN = 1e-5 #@param {type: &quot;number&quot;} N_RESTARTS = 5#@param {type: &quot;number&quot;} T_MUL = 2.0 #@param {type: &quot;number&quot;} M_MUL = 1#@param {type: &quot;number&quot;} STEPS_START = math.ceil((T_MUL-1)/(T_MUL**(N_RESTARTS+1)-1) * STEPS) schedule = tf.keras.experimental.CosineDecayRestarts( first_decay_steps=STEPS_START, initial_learning_rate=LR_START, alpha=LR_MIN, m_mul=M_MUL, t_mul=T_MUL) x = [i for i in range(STEPS)] y = [schedule(s) for s in range(STEPS)] _,ax = plt.subplots(1,1,figsize=(8,5),facecolor=&#39;#F0F0F0&#39;) ax.plot(x, y) ax.set_facecolor(&#39;#F8F8F8&#39;) ax.set_xlabel(&#39;iteration&#39;) ax.set_ylabel(&#39;learning rate&#39;) print(&#39;{:d} total epochs and {:d} steps per epoch&#39; .format(EPOCHS, STEPS // EPOCHS)) print(schedule.get_config()) . 50 total epochs and 50 steps per epoch {&#39;initial_learning_rate&#39;: 0.0008, &#39;first_decay_steps&#39;: 40, &#39;t_mul&#39;: 2.0, &#39;m_mul&#39;: 1, &#39;alpha&#39;: 1e-05, &#39;name&#39;: None} . Callbacks . callbacks = [ tf.keras.callbacks.ModelCheckpoint( filepath=&#39;001_best_model.h5&#39;, monitor=&#39;val_loss&#39;, save_best_only=True), tf.keras.callbacks.EarlyStopping( monitor=&#39;val_loss&#39;, mode=&#39;min&#39;, patience=10, restore_best_weights=True, verbose=1) ] model.compile( optimizer=tf.keras.optimizers.Adam(schedule), loss = &#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;sparse_categorical_accuracy&#39;] ) . model.summary() . Model: &#34;model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 300, 300, 3)] 0 _________________________________________________________________ efficientnetb3 (Functional) (None, 1536) 10783535 _________________________________________________________________ dropout (Dropout) (None, 1536) 0 _________________________________________________________________ pred (Dense) (None, 104) 159848 ================================================================= Total params: 10,943,383 Trainable params: 10,856,080 Non-trainable params: 87,303 _________________________________________________________________ . Training . Train the normalization layer . %%run_if {GOOGLE} def generate_norm_image(example): LABELED_TFREC_FORMAT = { &quot;image&quot;: tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring &quot;class&quot;: tf.io.FixedLenFeature([], tf.int64), # shape [] means single element } example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT) image = decode_image(example[&#39;image&#39;]) image = image / 255.0 return image . %%run_if {GOOGLE} if os.path.exists(&quot;000_normalization.h5&quot;): model.load_weights(&quot;000_normalization.h5&quot;) else: adapt_ds = (tf.data.TFRecordDataset(train_filenames, num_parallel_reads=AUTOTUNE) .map(generate_norm_image, num_parallel_calls=AUTOTUNE) .shuffle(2048) .batch(BATCH_SIZE) .prefetch(AUTOTUNE)) model.get_layer(&#39;efficientnetb3&#39;).get_layer(&#39;normalization&#39;).adapt(adapt_ds) model.save_weights(&quot;000_normalization.h5&quot;) . Train all . history = model.fit( x=train_ds, validation_data=valid_ds, epochs=EPOCHS, steps_per_epoch=STEPS//BATCH_SIZE, callbacks=callbacks, verbose=2 ) . Epoch 1/50 9/9 - 156s - loss: 1.9010 - sparse_categorical_accuracy: 0.5043 - val_loss: 13.0768 - val_sparse_categorical_accuracy: 0.2594 Epoch 2/50 9/9 - 6s - loss: 1.6419 - sparse_categorical_accuracy: 0.5647 - val_loss: 6.8306 - val_sparse_categorical_accuracy: 0.3257 Epoch 3/50 9/9 - 6s - loss: 1.5901 - sparse_categorical_accuracy: 0.5864 - val_loss: 3.9058 - val_sparse_categorical_accuracy: 0.4119 Epoch 4/50 9/9 - 6s - loss: 1.5140 - sparse_categorical_accuracy: 0.5877 - val_loss: 2.9674 - val_sparse_categorical_accuracy: 0.4723 Epoch 5/50 9/9 - 10s - loss: 1.5115 - sparse_categorical_accuracy: 0.5959 - val_loss: 2.2275 - val_sparse_categorical_accuracy: 0.5275 Epoch 6/50 9/9 - 7s - loss: 1.4713 - sparse_categorical_accuracy: 0.6050 - val_loss: 1.6515 - val_sparse_categorical_accuracy: 0.6051 Epoch 7/50 9/9 - 7s - loss: 1.4460 - sparse_categorical_accuracy: 0.6155 - val_loss: 1.3832 - val_sparse_categorical_accuracy: 0.6360 Epoch 8/50 9/9 - 7s - loss: 1.3661 - sparse_categorical_accuracy: 0.6337 - val_loss: 1.3275 - val_sparse_categorical_accuracy: 0.6509 Epoch 9/50 9/9 - 6s - loss: 1.3910 - sparse_categorical_accuracy: 0.6215 - val_loss: 1.2825 - val_sparse_categorical_accuracy: 0.6608 Epoch 10/50 9/9 - 6s - loss: 1.2811 - sparse_categorical_accuracy: 0.6562 - val_loss: 1.2094 - val_sparse_categorical_accuracy: 0.6797 Epoch 11/50 9/9 - 6s - loss: 1.2926 - sparse_categorical_accuracy: 0.6484 - val_loss: 1.1464 - val_sparse_categorical_accuracy: 0.6950 Epoch 12/50 9/9 - 6s - loss: 1.2898 - sparse_categorical_accuracy: 0.6302 - val_loss: 1.1089 - val_sparse_categorical_accuracy: 0.7050 Epoch 13/50 9/9 - 6s - loss: 1.2204 - sparse_categorical_accuracy: 0.6784 - val_loss: 1.0893 - val_sparse_categorical_accuracy: 0.7096 Epoch 14/50 9/9 - 6s - loss: 1.2152 - sparse_categorical_accuracy: 0.6623 - val_loss: 1.0783 - val_sparse_categorical_accuracy: 0.7142 Epoch 15/50 9/9 - 6s - loss: 1.2462 - sparse_categorical_accuracy: 0.6576 - val_loss: 1.0587 - val_sparse_categorical_accuracy: 0.7217 Epoch 16/50 9/9 - 6s - loss: 1.2206 - sparse_categorical_accuracy: 0.6688 - val_loss: 0.9961 - val_sparse_categorical_accuracy: 0.7314 Epoch 17/50 9/9 - 7s - loss: 1.2518 - sparse_categorical_accuracy: 0.6636 - val_loss: 1.0047 - val_sparse_categorical_accuracy: 0.7368 Epoch 18/50 9/9 - 7s - loss: 1.1942 - sparse_categorical_accuracy: 0.6727 - val_loss: 0.9873 - val_sparse_categorical_accuracy: 0.7384 Epoch 19/50 9/9 - 9s - loss: 1.1119 - sparse_categorical_accuracy: 0.7010 - val_loss: 0.9598 - val_sparse_categorical_accuracy: 0.7395 Epoch 20/50 9/9 - 7s - loss: 1.1494 - sparse_categorical_accuracy: 0.6944 - val_loss: 0.9371 - val_sparse_categorical_accuracy: 0.7532 Epoch 21/50 9/9 - 7s - loss: 1.1256 - sparse_categorical_accuracy: 0.6879 - val_loss: 0.9200 - val_sparse_categorical_accuracy: 0.7627 Epoch 22/50 9/9 - 6s - loss: 1.0482 - sparse_categorical_accuracy: 0.7201 - val_loss: 0.8943 - val_sparse_categorical_accuracy: 0.7694 Epoch 23/50 9/9 - 6s - loss: 1.0809 - sparse_categorical_accuracy: 0.7153 - val_loss: 0.9161 - val_sparse_categorical_accuracy: 0.7648 Epoch 24/50 9/9 - 6s - loss: 1.0814 - sparse_categorical_accuracy: 0.7114 - val_loss: 0.9213 - val_sparse_categorical_accuracy: 0.7619 Epoch 25/50 9/9 - 6s - loss: 1.0078 - sparse_categorical_accuracy: 0.7214 - val_loss: 0.8947 - val_sparse_categorical_accuracy: 0.7675 Epoch 26/50 9/9 - 7s - loss: 0.9709 - sparse_categorical_accuracy: 0.7391 - val_loss: 0.8600 - val_sparse_categorical_accuracy: 0.7753 Epoch 27/50 9/9 - 6s - loss: 1.0287 - sparse_categorical_accuracy: 0.7270 - val_loss: 0.8354 - val_sparse_categorical_accuracy: 0.7872 Epoch 28/50 9/9 - 6s - loss: 0.9749 - sparse_categorical_accuracy: 0.7396 - val_loss: 0.8162 - val_sparse_categorical_accuracy: 0.7920 Epoch 29/50 9/9 - 6s - loss: 0.9431 - sparse_categorical_accuracy: 0.7383 - val_loss: 0.8065 - val_sparse_categorical_accuracy: 0.7939 Epoch 30/50 9/9 - 7s - loss: 0.9715 - sparse_categorical_accuracy: 0.7422 - val_loss: 0.7995 - val_sparse_categorical_accuracy: 0.7936 Epoch 31/50 9/9 - 7s - loss: 0.9164 - sparse_categorical_accuracy: 0.7530 - val_loss: 0.7948 - val_sparse_categorical_accuracy: 0.7947 Epoch 32/50 9/9 - 6s - loss: 0.9203 - sparse_categorical_accuracy: 0.7487 - val_loss: 0.8642 - val_sparse_categorical_accuracy: 0.7683 Epoch 33/50 9/9 - 7s - loss: 0.9915 - sparse_categorical_accuracy: 0.7313 - val_loss: 0.8313 - val_sparse_categorical_accuracy: 0.7823 Epoch 34/50 9/9 - 6s - loss: 1.0157 - sparse_categorical_accuracy: 0.7296 - val_loss: 0.8204 - val_sparse_categorical_accuracy: 0.7842 Epoch 35/50 9/9 - 7s - loss: 1.0600 - sparse_categorical_accuracy: 0.7188 - val_loss: 0.8281 - val_sparse_categorical_accuracy: 0.7866 Epoch 36/50 9/9 - 6s - loss: 0.9717 - sparse_categorical_accuracy: 0.7361 - val_loss: 0.8413 - val_sparse_categorical_accuracy: 0.7761 Epoch 37/50 9/9 - 7s - loss: 0.9517 - sparse_categorical_accuracy: 0.7326 - val_loss: 0.8223 - val_sparse_categorical_accuracy: 0.7839 Epoch 38/50 9/9 - 7s - loss: 0.8959 - sparse_categorical_accuracy: 0.7626 - val_loss: 0.7921 - val_sparse_categorical_accuracy: 0.7874 Epoch 39/50 9/9 - 7s - loss: 1.0345 - sparse_categorical_accuracy: 0.7196 - val_loss: 0.7694 - val_sparse_categorical_accuracy: 0.8036 Epoch 40/50 9/9 - 7s - loss: 0.9000 - sparse_categorical_accuracy: 0.7465 - val_loss: 0.7594 - val_sparse_categorical_accuracy: 0.8036 Epoch 41/50 9/9 - 7s - loss: 0.8503 - sparse_categorical_accuracy: 0.7626 - val_loss: 0.7567 - val_sparse_categorical_accuracy: 0.8012 Epoch 42/50 9/9 - 6s - loss: 0.8809 - sparse_categorical_accuracy: 0.7530 - val_loss: 0.7373 - val_sparse_categorical_accuracy: 0.8082 Epoch 43/50 9/9 - 6s - loss: 0.9019 - sparse_categorical_accuracy: 0.7539 - val_loss: 0.7463 - val_sparse_categorical_accuracy: 0.8106 Epoch 44/50 9/9 - 6s - loss: 0.8453 - sparse_categorical_accuracy: 0.7674 - val_loss: 0.7092 - val_sparse_categorical_accuracy: 0.8155 Epoch 45/50 9/9 - 7s - loss: 0.7987 - sparse_categorical_accuracy: 0.7687 - val_loss: 0.7131 - val_sparse_categorical_accuracy: 0.8152 Epoch 46/50 9/9 - 6s - loss: 0.8338 - sparse_categorical_accuracy: 0.7769 - val_loss: 0.7005 - val_sparse_categorical_accuracy: 0.8171 Epoch 47/50 9/9 - 7s - loss: 0.7969 - sparse_categorical_accuracy: 0.7799 - val_loss: 0.6921 - val_sparse_categorical_accuracy: 0.8230 Epoch 48/50 9/9 - 6s - loss: 0.7685 - sparse_categorical_accuracy: 0.7891 - val_loss: 0.6948 - val_sparse_categorical_accuracy: 0.8198 Epoch 49/50 9/9 - 7s - loss: 0.7290 - sparse_categorical_accuracy: 0.7986 - val_loss: 0.6804 - val_sparse_categorical_accuracy: 0.8254 Epoch 50/50 9/9 - 7s - loss: 0.7546 - sparse_categorical_accuracy: 0.7925 - val_loss: 0.6556 - val_sparse_categorical_accuracy: 0.8349 . . Training curve . def show_history(history): topics = [&#39;loss&#39;, &#39;accuracy&#39;] groups = [{k:v for (k,v) in history.items() if topic in k} for topic in topics] _,axs = plt.subplots(1,2,figsize=(15,6),facecolor=&#39;#F0F0F0&#39;) for topic,group,ax in zip(topics,groups,axs.flatten()): for (_,v) in group.items(): ax.plot(v) ax.set_facecolor(&#39;#F8F8F8&#39;) ax.set_title(f&#39;{topic} over epochs&#39;) ax.set_xlabel(&#39;epoch&#39;) ax.set_ylabel(topic) ax.legend([&#39;train&#39;, &#39;valid&#39;], loc=&#39;best&#39;) . . show_history(history.history) . def show_confusion_matrix(cmat, score, precision, recall): _,ax = plt.subplots(1,1,figsize=(12,12),facecolor=&#39;#F0F0F0&#39;) ax.matshow(cmat, cmap=&#39;Blues&#39;) if len(CLASSES) &lt;= 10: ax.set_xticks(range(len(CLASSES)),) ax.set_xticklabels(CLASSES, fontdict={&#39;fontsize&#39;: 7}) plt.setp(ax.get_xticklabels(), rotation=45, ha=&quot;left&quot;, rotation_mode=&quot;anchor&quot;) ax.set_yticks(range(len(CLASSES))) ax.set_yticklabels(CLASSES, fontdict={&#39;fontsize&#39;: 7}) plt.setp(ax.get_yticklabels(), rotation=45, ha=&quot;right&quot;, rotation_mode=&quot;anchor&quot;) else: ax.axis(&#39;off&#39;) textstr = &quot;&quot; if precision: textstr += &#39;precision = {:.3f} &#39;.format(precision) if recall: textstr += &#39; nrecall = {:.3f} &#39;.format(recall) if score: textstr += &#39; nf1 = {:.3f} &#39;.format(score) if len(textstr) &gt; 0: props = dict(boxstyle=&#39;round&#39;, facecolor=&#39;wheat&#39;, alpha=0.2) ax.text(0.75, 0.95, textstr, transform=ax.transAxes, fontsize=14, verticalalignment=&#39;top&#39;, bbox=props) plt.show() . . ordered_valid_ds = tf.data.TFRecordDataset(valid_filenames, num_parallel_reads=AUTOTUNE) ordered_valid_ds = (ordered_valid_ds .map(collate_labeled_tfrecord, num_parallel_calls=AUTOTUNE) .batch(BATCH_SIZE) .cache() .prefetch(AUTOTUNE)) x_valid_ds = ordered_valid_ds.map(lambda x,y : x, num_parallel_calls=AUTOTUNE) y_valid_ds = ordered_valid_ds.map(lambda x,y : y, num_parallel_calls=AUTOTUNE) . y_true = (y_valid_ds .unbatch() .batch(count_data_items(valid_filenames)) .as_numpy_iterator() .next()) y_probs = model.predict(x_valid_ds) y_preds = np.argmax(y_probs, axis=-1) . label_ids = range(len(CLASSES)) cmatrix = confusion_matrix(y_true, y_preds, labels=label_ids) cmatrix = (cmatrix.T / cmatrix.sum(axis=1)).T # normalize . You might be familiar with metrics like F1-score or precision and recall. This cell will compute these metrics and display them with a plot of the confusion matrix. (These metrics are defined in the Scikit-learn module sklearn.metrics; we&#39;ve imported them in the helper script for you.) . precision = precision_score(y_true, y_preds, labels=label_ids, average=&#39;macro&#39;) recall = recall_score(y_true, y_preds, labels=label_ids, average=&#39;macro&#39;) score = f1_score(y_true, y_preds, labels=label_ids,average=&#39;macro&#39;) . show_confusion_matrix(cmatrix, score, precision, recall) . Prediction . Once you&#39;re satisfied with everything, you&#39;re ready to make predictions on the test set. . test_ds = tf.data.TFRecordDataset(test_filenames, num_parallel_reads=AUTOTUNE) test_ds = (test_ds .map(process_unlabeled_tfrecord, num_parallel_calls=AUTOTUNE) .batch(BATCH_SIZE) .prefetch(AUTOTUNE)) x_test_ds = test_ds.map(lambda image,idnum: image) . y_probs = model.predict(x_test_ds) y_preds = np.argmax(y_probs, axis=-1) . Let&#39;s generate a file submission.csv. This file is what you&#39;ll submit to get your score on the leaderboard. . id_test_ds = test_ds.map(lambda image,idnum: idnum) . id_test_ds = (id_test_ds.unbatch() .batch(count_data_items(test_filenames)) .as_numpy_iterator() .next() .astype(&#39;U&#39;)) . np.savetxt(&#39;submission.csv&#39;, np.rec.fromarrays([id_test_ds, y_preds]), fmt=[&#39;%s&#39;, &#39;%d&#39;], delimiter=&#39;,&#39;, header=&#39;id,label&#39;, comments=&#39;&#39;) . !head submission.csv . id,label 252d840db,67 1c4736dea,28 c37a6f3e9,83 00e4f514e,103 59d1b6146,46 8d808a07b,53 aeb67eefb,52 53cfc6586,71 aaa580243,85 . 1% Better Everyday . reference . How to use my own data source? | TPU-speed data pipelines: tf.data.Dataset and TFRecords | . . todos . ~Finish the interpretation section~ | ~Comment out the 1/255.0 in the image preprocessing~ | ~Reorganize the notebook structure~ | .",
            "url": "https://austinyhc.github.io/blog/plant/classification/tpu/2020/12/17/petals-to-the-metal.html",
            "relUrl": "/plant/classification/tpu/2020/12/17/petals-to-the-metal.html",
            "date": " • Dec 17, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am Austin; An Embedded System Engineer by profession and passion with extensive experience in the areas of Deep Learning, Diagnostic Modeling, Predictive Modeling, Digital Signal/Image Processing by using Python and C++. . As an enthusiastic learner and practioner that involves making things intelligently work, I pursue my interest in real-life application and cutting-edge researches. Hope to meet a ton of you in the sphere of machine learning and to contribute as best as I can to your community. .",
          "url": "https://austinyhc.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://austinyhc.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}