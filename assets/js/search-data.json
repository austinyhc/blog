{
  
    
        "post0": {
            "title": "The Real World is not a Kaggle Competition",
            "content": "Outline . introduction | over-engineering | the best performance | be mindful of your surroundings | the trade-off | getting it just right | minimalism | . . Collecting voices in my brain . to me Kaggle competetion is only a part of a real-lift data science project. In real life there are multiple other aspects that Kaggle doesn&#39;t touch on. Kagggle is a perfect platform for hosting and learning specific skills, but a real-life science problems are usually much bigger challengesfso the distinction is worth making. | . . Reference . https://www.quora.com/How-similar-are-Kaggle-competitions-to-what-data-scientists-do | . Note: Draft 1: In my experience, the most challenging part is to convince the Business Users (Board of Directors and other C-level executives) and merchants to follow through your solution. You really need to have a data backed strong analysis result to persuade them that your solution is indeed better than their business acumen, which they have inculcated over the years. And in such a scenario, black-box type models involving cool stacking and blending which is used to climb up the leader-board, often fail. Business Users can get quite defensive and be reluctant to take ahead your analysis, just because it achieves 95% accuracy! So most of the time, what your want is a white-box algorithm with a reasonable accuracy, say 90%, is easy to follow and explainable in business terms. . . Note: Draft 2 for over-engineering: To give a concrete example, let&#8217;s follow through the Promotion Effectiveness Analysis which is a classic problem for any Retailer. Usually what htey want out of the exercise is some way to quantify the effectiveness of Promotion X which they ran and a list of products which are likely to perform well under it. Now let&#8217;s assume two data scientist are given with this task. You being a pro in Kaggle, use a lot of ensembling, blending and stacking and now the new trend of incorporating data leaks and other cool hacks up your sleeve to make your model most accurate in terms of perdicting which products are expected to perform well under Promotion X. But you should mentally prepare yourself before hand, for you might be turned down by the Business because these usually influence million-dollar decisions and they have a hard time relying on a black-box! . Introduction . Machine Learning and Deep Learning are evolving at a faster pace than ever since early 2010s. . . Note: Draft 1 for Introduction, the do not get me wrong tone which is inspired by this Quora reply: I am in no way undermining the capabiliteis you can develop by participanting in these excellent Data Science competitions. In fact, kaggle teaches you feature engineering really well! But what I want you know, is that these problems are subset of what you will face in real-life as a data scientist. There will be cases when you will benefit from all the cool hacks you learn in these platforms for a solution that doesn&#8217;t require a peek into what&#8217;s going on inside the bos. There will be some which will require you to discover, gain confidence and slowly move to black-box. The key is in understanding the situation, business problem at hand, expectation of stakeholders and some how balancing all these, yet rendering out a reasonable accuracy. . The Best Performance . Be Mindful of Your Surroundings . The Trade-Off . Getting the Size Just Right . Minimal Model .",
            "url": "https://austinyhc.github.io/blog/deep%20learning/2020/12/30/the-real-world-is-not-a-kaggle-competition.html",
            "relUrl": "/deep%20learning/2020/12/30/the-real-world-is-not-a-kaggle-competition.html",
            "date": " • Dec 30, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Petals to the Metal",
            "content": "%%writefile conditional_cell_extension.py def run_if(line, cell=None): &#39;&#39;&#39;Execute current line/cell if line evaluates to True.&#39;&#39;&#39; if not eval(line): return get_ipython().ex(cell) def load_ipython_extension(shell): &#39;&#39;&#39;Registers the run_if magic when the extension loads.&#39;&#39;&#39; shell.register_magic_function(run_if, &#39;line_cell&#39;) def unload_ipython_extension(shell): &#39;&#39;&#39;Unregisters the run_if magic when the extension unloads.&#39;&#39;&#39; del shell.magics_manager.magics[&#39;cell&#39;][&#39;run_if&#39;] . Overwriting conditional_cell_extension.py . %reload_ext conditional_cell_extension . import numpy as np import pandas as pd import seaborn as sns import albumentations as A import matplotlib.pyplot as plt import os, gc, cv2, random, re import warnings, math, sys, json import subprocess, pprint, pdb import tensorflow as tf from tensorflow.keras import backend as K import tensorflow_hub as hub from sklearn.model_selection import train_test_split from sklearn.metrics import f1_score,precision_score, recall_score, confusion_matrix warnings.simplefilter(&#39;ignore&#39;) print(f&quot;Using TensorFlow v{tf.__version__}&quot;) . . Using TensorFlow v2.4.0 . . Tip: Adding seed helps reproduce results. Setting debug parameter wil run the model on smaller number of epochs to validate the architecture. . def seed_everything(seed=0): random.seed(seed) np.random.seed(seed) tf.random.set_seed(seed) os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) os.environ[&#39;TF_DETERMINISTIC_OPS&#39;] = &#39;1&#39; GOOGLE = &#39;google.colab&#39; in str(get_ipython()) KAGGLE = not GOOGLE print(&quot;Running on {}!&quot;.format( &quot;Google Colab&quot; if GOOGLE else &quot;Kaggle Kernel&quot; )) . . Running on Google Colab! . TFRecord basics . GCS_PATTERN = &#39;gs://flowers-public/*/*.jpg&#39; GCS_OUTPUT = &#39;gs://flowers-public/tfrecords-jpeg-192x192-2/flowers&#39; SHARDS = 16 TARGET_SIZE = [192, 192] CLASSES = [b&#39;daisy&#39;, b&#39;dandelion&#39;, b&#39;roses&#39;, b&#39;sunflowers&#39;, b&#39;tulips&#39;] . . Read images and labels . def decode_image_and_label(filename): bits = tf.io.read_file(filename) image = tf.image.decode_jpeg(bits) label = tf.strings.split(tf.expand_dims(filename, axis=-1), sep=&#39;/&#39;) #label = tf.strings.split(filename, sep=&#39;/&#39;) label = label.values[-2] label = tf.cast((CLASSES==label), tf.int8) return image, label . . filenames = tf.data.Dataset.list_files(GCS_PATTERN, seed=16) for x in filenames.take(3): print(x) . tf.Tensor(b&#39;gs://flowers-public/tulips/251811158_75fa3034ff.jpg&#39;, shape=(), dtype=string) tf.Tensor(b&#39;gs://flowers-public/daisy/506348009_9ecff8b6ef.jpg&#39;, shape=(), dtype=string) tf.Tensor(b&#39;gs://flowers-public/daisy/2019064575_7656b9340f_m.jpg&#39;, shape=(), dtype=string) . def show_images(ds): _,axs = plt.subplots(3,3,figsize=(16,16)) for ((x, y), ax) in zip(ds.take(9), axs.flatten()): ax.imshow(x.numpy().astype(np.uint8)) ax.set_title(np.argmax(y)) ax.axis(&#39;off&#39;) . . ds0 = filenames.map(decode_image_and_label, num_parallel_calls=AUTOTUNE) show_images(ds0) . Resize and crop images to common size . No need to study the code in this cell. It&#39;s only image resizing. . def resize_and_crop_image(image, label): # Resize and crop using &quot;fill&quot; algorithm: # always make sure the resulting image # is cut out from the source image so that # it fills the TARGET_SIZE entirely with no # black bars and a preserved aspect ratio. w = tf.shape(image)[0] h = tf.shape(image)[1] tw = TARGET_SIZE[1] th = TARGET_SIZE[0] resize_crit = (w * th) / (h * tw) image = tf.cond(resize_crit &lt; 1, lambda: tf.image.resize(image, [w*tw/w, h*tw/w]), # if true lambda: tf.image.resize(image, [w*th/h, h*th/h]) # if false ) nw = tf.shape(image)[0] nh = tf.shape(image)[1] image = tf.image.crop_to_bounding_box(image, (nw - tw) // 2, (nh - th) // 2, tw, th) return image, label . . ds1 = ds0.map(resize_and_crop_image, num_parallel_calls=AUTOTUNE) show_images(ds1) . Speed test: too slow . Google Cloud Storage is capable of great throughput but has a per-file access penalty. Run the cell below and see that throughput is around 8 images per second. That is too slow. Training on thousands of individual files will not work. We have to use the TFRecord format to group files together. . %%time for image,label in ds1.batch(8).take(10): print(&quot;Image batch shape {} {}&quot;.format( image.numpy().shape, [np.argmax(lbl) for lbl in label.numpy()])) . Image batch shape (8, 192, 192, 3) [0, 1, 0, 0, 1, 3, 2, 1] Image batch shape (8, 192, 192, 3) [3, 4, 4, 0, 3, 4, 3, 0] Image batch shape (8, 192, 192, 3) [0, 3, 0, 4, 2, 4, 2, 4] Image batch shape (8, 192, 192, 3) [3, 4, 4, 0, 2, 3, 2, 3] Image batch shape (8, 192, 192, 3) [1, 3, 4, 3, 0, 3, 1, 3] Image batch shape (8, 192, 192, 3) [4, 4, 3, 0, 0, 4, 4, 1] Image batch shape (8, 192, 192, 3) [1, 3, 1, 3, 1, 2, 4, 2] Image batch shape (8, 192, 192, 3) [1, 4, 2, 4, 2, 2, 1, 0] Image batch shape (8, 192, 192, 3) [0, 3, 2, 2, 3, 4, 0, 1] Image batch shape (8, 192, 192, 3) [1, 2, 0, 1, 0, 3, 4, 1] CPU times: user 60 ms, sys: 54.4 ms, total: 114 ms Wall time: 5.62 s . Recompress the images . The bandwidth savings outweight the decoding CPU cost . def recompress_image(image, label): height = tf.shape(image)[0] width = tf.shape(image)[1] image = tf.cast(image, tf.uint8) image = tf.image.encode_jpeg(image, optimize_size=True, chroma_downsampling=False) return image, label, height, width . . IMAGE_SIZE = len(tf.io.gfile.glob(GCS_PATTERN)) SHARD_SIZE = math.ceil(1.0 * IMAGE_SIZE / SHARDS) . ds2 = ds1.map(recompress_image, num_parallel_calls=AUTOTUNE) ds2 = ds2.batch(SHARD_SIZE) . Why TFRecords? . TPUs have eight cores which act as eight independent workers. We can get data to each core more efficiently by splitting the dataset into multiple files or shards. This way, each core can grab an independent part of the data as it needs. The most convenient kind of file to use for sharding in TensorFlow is a TFRecord. A TFRecord is a binary file that contains sequences of byte-strings. Data needs to be serialized (encoded as a byte-string) before being written into a TFRecord. The most convenient way of serializing data in TensorFlow is to wrap the data with tf.Example. This is a record format based on Google&#39;s protobufs but designed for TensorFlow. It&#39;s more or less like a dict with some type annotations . x = tf.constant([[1,2], [3, 4]], dtype=tf.uint8) print(x) . tf.Tensor( [[1 2] [3 4]], shape=(2, 2), dtype=uint8) . x_in_bytes = tf.io.serialize_tensor(x) print(x_in_bytes) . tf.Tensor(b&#39; x08 x04 x12 x08 x12 x02 x08 x02 x12 x02 x08 x02&#34; x04 x01 x02 x03 x04&#39;, shape=(), dtype=string) . print(tf.io.parse_tensor(x_in_bytes, out_type=tf.uint8)) . tf.Tensor( [[1 2] [3 4]], shape=(2, 2), dtype=uint8) . A TFRecord is a sequence of bytes, so we have to turn our data into byte-strings before it can go into a TFRecord. We can use tf.io.serialize_tensor to turn a tensor into a byte-string and tf.io.parse_tensor to turn it back. It&#39;s important to keep track of your tensor&#39;s datatype (in this case tf.uint8) since you have to specify it when parsing the string back to a tensor again . Write dataset to TFRecord files . . Note: Will uncomment the cells in this section when I find a gs:// domain to write to. . Read from TFRecord Dataset . def read_tfrecord(example): features = { &quot;image&quot;: tf.io.FixedLenFeature([], tf.string), # tf.string = bytestring (not text string) &quot;class&quot;: tf.io.FixedLenFeature([], tf.int64), # shape [] means scalar # additional (not very useful) fields to demonstrate TFRecord writing/reading of different types of data &quot;label&quot;: tf.io.FixedLenFeature([], tf.string), # one bytestring &quot;size&quot;: tf.io.FixedLenFeature([2], tf.int64), # two integers &quot;one_hot_class&quot;: tf.io.VarLenFeature(tf.float32) # a certain number of floats } # decode the TFRecord example = tf.io.parse_single_example(example, features) # FixedLenFeature fields are now ready to use: exmple[&#39;size&#39;] # VarLenFeature fields require additional sparse_to_dense decoding image = tf.image.decode_jpeg(example[&#39;image&#39;], channels=3) image = tf.reshape(image, [*TARGET_SIZE, 3]) class_num = example[&#39;class&#39;] label = example[&#39;label&#39;] height = example[&#39;size&#39;][0] width = example[&#39;size&#39;][1] one_hot_class = tf.sparse.to_dense(example[&#39;one_hot_class&#39;]) return image, class_num, label, height, width, one_hot_class . . option_no_order = tf.data.Options() option_no_order.experimental_deterministic = False filenames = tf.io.gfile.glob(GCS_OUTPUT + &quot;*tfrec&quot;) ds3 = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) ds3 = (ds3.with_options(option_no_order) .map(read_tfrecord, num_parallel_calls=AUTOTUNE) .shuffle(30)) . ds3_to_show = ds3.map(lambda image, id, label, height, width, one_hot: (image, label)) show_images(ds3_to_show) . Speed test: fast . Loading form TFRecords is almost 10x time faster than loading from JPEGs. . %%time for image, class_num, label, height, width, one_hot_class in ds3.batch(8).take(10): print(&quot;Image batch shape {} {}&quot;.format( image.numpy().shape, [lbl.decode(&#39;utf8&#39;) for lbl in label.numpy()])) . Image batch shape (8, 192, 192, 3) [&#39;roses&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;, &#39;tulips&#39;, &#39;daisy&#39;, &#39;dandelion&#39;, &#39;tulips&#39;] Image batch shape (8, 192, 192, 3) [&#39;roses&#39;, &#39;dandelion&#39;, &#39;sunflowers&#39;, &#39;tulips&#39;, &#39;sunflowers&#39;, &#39;dandelion&#39;, &#39;roses&#39;, &#39;dandelion&#39;] Image batch shape (8, 192, 192, 3) [&#39;daisy&#39;, &#39;daisy&#39;, &#39;sunflowers&#39;, &#39;roses&#39;, &#39;roses&#39;, &#39;daisy&#39;, &#39;sunflowers&#39;, &#39;tulips&#39;] Image batch shape (8, 192, 192, 3) [&#39;roses&#39;, &#39;roses&#39;, &#39;roses&#39;, &#39;tulips&#39;, &#39;sunflowers&#39;, &#39;tulips&#39;, &#39;tulips&#39;, &#39;tulips&#39;] Image batch shape (8, 192, 192, 3) [&#39;sunflowers&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;, &#39;daisy&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;, &#39;sunflowers&#39;, &#39;roses&#39;] Image batch shape (8, 192, 192, 3) [&#39;roses&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;, &#39;sunflowers&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;] Image batch shape (8, 192, 192, 3) [&#39;dandelion&#39;, &#39;sunflowers&#39;, &#39;daisy&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;, &#39;dandelion&#39;, &#39;daisy&#39;] Image batch shape (8, 192, 192, 3) [&#39;roses&#39;, &#39;daisy&#39;, &#39;dandelion&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;, &#39;daisy&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;] Image batch shape (8, 192, 192, 3) [&#39;tulips&#39;, &#39;tulips&#39;, &#39;roses&#39;, &#39;dandelion&#39;, &#39;sunflowers&#39;, &#39;dandelion&#39;, &#39;dandelion&#39;, &#39;tulips&#39;] Image batch shape (8, 192, 192, 3) [&#39;daisy&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;, &#39;dandelion&#39;, &#39;daisy&#39;, &#39;roses&#39;, &#39;sunflowers&#39;, &#39;sunflowers&#39;] CPU times: user 32.5 ms, sys: 12.8 ms, total: 45.3 ms Wall time: 184 ms . Hyperparameters . BASE_MODEL = &#39;efficientnet_b3&#39; #@param [&quot;&#39;efficientnet_b3&#39;&quot;, &quot;&#39;efficientnet_b4&#39;&quot;, &quot;&#39;efficientnet_b2&#39;&quot;] {type:&quot;raw&quot;, allow-input: true} HEIGHT = 300#@param {type:&quot;number&quot;} WIDTH = 300#@param {type:&quot;number&quot;} CHANNELS = 3#@param {type:&quot;number&quot;} IMG_SIZE = (HEIGHT, WIDTH, CHANNELS) EPOCHS = 50#@param {type:&quot;number&quot;} BATCH_SIZE = 32 * strategy.num_replicas_in_sync #@param {type:&quot;raw&quot;} print(&quot;Use {} with input size {}&quot;.format(BASE_MODEL, IMG_SIZE)) print(&quot;Train on batch size of {} for {} epochs&quot;.format(BATCH_SIZE, EPOCHS)) . Use efficientnet_b3 with input size (300, 300, 3) Train on batch size of 256 for 50 epochs . Data . Loading data . %%run_if {KAGGLE} from kaggle_datasets import KaggleDatasets GCS_PATH = KaggleDatasets().get_gcs_path(project_name) GCS_PATH += &#39;/tfrecords-jpeg-512x512&#39; print(f&quot;Sourcing images from {GCS_PATH}&quot;) . %%run_if {GOOGLE} #@title {run: &quot;auto&quot;, display-mode: &quot;form&quot;} GCS_PATH = &#39;gs://kds-c6b9829baa483a13a169c7cbe266341fb8c9b5ba36843af37a093a4c&#39; #@param {type: &quot;string&quot;} GCS_PATH += &#39;/tfrecords-jpeg-512x512&#39; #@param {type: &quot;string&quot;} print(f&quot;Sourcing images from {GCS_PATH}&quot;) . Sourcing images from gs://kds-c6b9829baa483a13a169c7cbe266341fb8c9b5ba36843af37a093a4c/tfrecords-jpeg-512x512 . CLASSES = [&#39;pink primrose&#39;, &#39;hard-leaved pocket orchid&#39;, &#39;canterbury bells&#39;, &#39;sweet pea&#39;, &#39;wild geranium&#39;, &#39;tiger lily&#39;, &#39;moon orchid&#39;, &#39;bird of paradise&#39;, &#39;monkshood&#39;, &#39;globe thistle&#39;, &#39;snapdragon&#39;, &quot;colt&#39;s foot&quot;, &#39;king protea&#39;, &#39;spear thistle&#39;, &#39;yellow iris&#39;, &#39;globe-flower&#39;, &#39;purple coneflower&#39;, &#39;peruvian lily&#39;, &#39;balloon flower&#39;,&#39;giant white arum lily&#39;, &#39;fire lily&#39;, &#39;pincushion flower&#39;, &#39;fritillary&#39;, &#39;red ginger&#39;, &#39;grape hyacinth&#39;, &#39;corn poppy&#39;, &#39;prince of wales feathers&#39;, &#39;stemless gentian&#39;, &#39;artichoke&#39;, &#39;sweet william&#39;, &#39;carnation&#39;, &#39;garden phlox&#39;, &#39;love in the mist&#39;, &#39;cosmos&#39;, &#39;alpine sea holly&#39;, &#39;ruby-lipped cattleya&#39;, &#39;cape flower&#39;, &#39;great masterwort&#39;, &#39;siam tulip&#39;, &#39;lenten rose&#39;, &#39;barberton daisy&#39;, &#39;daffodil&#39;, &#39;sword lily&#39;, &#39;poinsettia&#39;, &#39;bolero deep blue&#39;, &#39;wallflower&#39;, &#39;marigold&#39;, &#39;buttercup&#39;, &#39;daisy&#39;, &#39;common dandelion&#39;, &#39;petunia&#39;, &#39;wild pansy&#39;, &#39;primula&#39;, &#39;sunflower&#39;, &#39;lilac hibiscus&#39;, &#39;bishop of llandaff&#39;, &#39;gaura&#39;, &#39;geranium&#39;, &#39;orange dahlia&#39;, &#39;pink-yellow dahlia&#39;, &#39;cautleya spicata&#39;, &#39;japanese anemone&#39;, &#39;black-eyed susan&#39;, &#39;silverbush&#39;, &#39;californian poppy&#39;, &#39;osteospermum&#39;, &#39;spring crocus&#39;, &#39;iris&#39;, &#39;windflower&#39;, &#39;tree poppy&#39;, &#39;gazania&#39;, &#39;azalea&#39;, &#39;water lily&#39;, &#39;rose&#39;, &#39;thorn apple&#39;, &#39;morning glory&#39;, &#39;passion flower&#39;, &#39;lotus&#39;, &#39;toad lily&#39;, &#39;anthurium&#39;, &#39;frangipani&#39;, &#39;clematis&#39;, &#39;hibiscus&#39;, &#39;columbine&#39;, &#39;desert-rose&#39;, &#39;tree mallow&#39;, &#39;magnolia&#39;, &#39;cyclamen &#39;, &#39;watercress&#39;, &#39;canna lily&#39;, &#39;hippeastrum &#39;, &#39;bee balm&#39;, &#39;pink quill&#39;, &#39;foxglove&#39;, &#39;bougainvillea&#39;, &#39;camellia&#39;, &#39;mallow&#39;, &#39;mexican petunia&#39;, &#39;bromelia&#39;, &#39;blanket flower&#39;, &#39;trumpet creeper&#39;, &#39;blackberry lily&#39;, &#39;common tulip&#39;, &#39;wild rose&#39;] NCLASSES = len(CLASSES) print(f&quot;Number of labels: {NCLASSES}&quot;) . . Number of labels: 104 . def decode_image(image_data): image = tf.image.decode_jpeg(image_data, channels=CHANNELS) image = (tf.cast(image, tf.float32) if GOOGLE else tf.cast(image, tf.float32) / 255.0) image = tf.image.random_crop(image, IMG_SIZE) return image def collate_labeled_tfrecord(example): LABELED_TFREC_FORMAT = { &quot;image&quot;: tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring &quot;class&quot;: tf.io.FixedLenFeature([], tf.int64), # shape [] means single element } example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT) image = decode_image(example[&#39;image&#39;]) label = tf.cast(example[&#39;class&#39;], tf.int32) return image, label def process_unlabeled_tfrecord(example): UNLABELED_TFREC_FORMAT = { &quot;image&quot;: tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring &quot;id&quot;: tf.io.FixedLenFeature([], tf.string), # shape [] means single element } example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT) image = decode_image(example[&#39;image&#39;]) idnum = example[&#39;id&#39;] return image, idnum def count_data_items(filenames): n = [int(re.compile(r&quot;-([0-9]*) .&quot;).search(filename).group(1)) for filename in filenames] return np.sum(n) . . train_filenames = tf.io.gfile.glob(GCS_PATH + &#39;/train/*.tfrec&#39;) valid_filenames = tf.io.gfile.glob(GCS_PATH + &#39;/val/*.tfrec&#39;) test_filenames = tf.io.gfile.glob(GCS_PATH + &#39;/test/*.tfrec&#39;) . print(&quot;Number of train set: {} n&quot; &quot;Number of valid set: {} n&quot; &quot;Number of test set: {} n&quot; .format(count_data_items(train_filenames), count_data_items(valid_filenames), count_data_items(test_filenames))) . Number of train set: 12753 Number of valid set: 3712 Number of test set: 7382 . Data augmentation . . Note: The following data augmentation functions are referenced from Data Augmentation using GPU/TPU for Maximum Speed! by @cdeotte . def transform_shear(image, height, shear): # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3] # output - image randomly sheared DIM = height XDIM = DIM%2 #fix for size 331 shear = shear * tf.random.uniform([1],dtype=&#39;float32&#39;) shear = math.pi * shear / 180. # SHEAR MATRIX one = tf.constant([1],dtype=&#39;float32&#39;) zero = tf.constant([0],dtype=&#39;float32&#39;) c2 = tf.math.cos(shear) s2 = tf.math.sin(shear) shear_matrix = tf.reshape(tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3]) # LIST DESTINATION PIXEL INDICES x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM ) y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] ) z = tf.ones([DIM*DIM],dtype=&#39;int32&#39;) idx = tf.stack( [x,y,z] ) # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS idx2 = K.dot(shear_matrix,tf.cast(idx,dtype=&#39;float32&#39;)) idx2 = K.cast(idx2,dtype=&#39;int32&#39;) idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2) # FIND ORIGIN PIXEL VALUES idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] ) d = tf.gather_nd(image, tf.transpose(idx3)) return tf.reshape(d,[DIM,DIM,3]) . def transform_shift(image, height, h_shift, w_shift): # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3] # output - image randomly shifted DIM = height XDIM = DIM%2 #fix for size 331 height_shift = h_shift * tf.random.uniform([1],dtype=&#39;float32&#39;) width_shift = w_shift * tf.random.uniform([1],dtype=&#39;float32&#39;) one = tf.constant([1],dtype=&#39;float32&#39;) zero = tf.constant([0],dtype=&#39;float32&#39;) # SHIFT MATRIX shift_matrix = tf.reshape(tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3]) # LIST DESTINATION PIXEL INDICES x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM ) y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] ) z = tf.ones([DIM*DIM],dtype=&#39;int32&#39;) idx = tf.stack( [x,y,z] ) # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS idx2 = K.dot(shift_matrix,tf.cast(idx,dtype=&#39;float32&#39;)) idx2 = K.cast(idx2,dtype=&#39;int32&#39;) idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2) # FIND ORIGIN PIXEL VALUES idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] ) d = tf.gather_nd(image, tf.transpose(idx3)) return tf.reshape(d,[DIM,DIM,3]) . def transform_rotation(image, height, rotation): # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3] # output - image randomly rotated DIM = height XDIM = DIM%2 #fix for size 331 rotation = rotation * tf.random.uniform([1],dtype=&#39;float32&#39;) # CONVERT DEGREES TO RADIANS rotation = math.pi * rotation / 180. # ROTATION MATRIX c1 = tf.math.cos(rotation) s1 = tf.math.sin(rotation) one = tf.constant([1],dtype=&#39;float32&#39;) zero = tf.constant([0],dtype=&#39;float32&#39;) rotation_matrix = tf.reshape(tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3]) # LIST DESTINATION PIXEL INDICES x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM ) y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] ) z = tf.ones([DIM*DIM],dtype=&#39;int32&#39;) idx = tf.stack( [x,y,z] ) # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS idx2 = K.dot(rotation_matrix,tf.cast(idx,dtype=&#39;float32&#39;)) idx2 = K.cast(idx2,dtype=&#39;int32&#39;) idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2) # FIND ORIGIN PIXEL VALUES idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] ) d = tf.gather_nd(image, tf.transpose(idx3)) return tf.reshape(d,[DIM,DIM,3]) . def data_augment(image, label): p_rotation = tf.random.uniform([], 0, 1.0, dtype=tf.float32) p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32) p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32) p_pixel = tf.random.uniform([], 0, 1.0, dtype=tf.float32) p_shear = tf.random.uniform([], 0, 1.0, dtype=tf.float32) p_shift = tf.random.uniform([], 0, 1.0, dtype=tf.float32) p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32) # Flips if p_spatial &gt;= .2: image = tf.image.random_flip_left_right(image) image = tf.image.random_flip_up_down(image) # Rotates if p_rotate &gt; .75: image = tf.image.rot90(image, k=3) # rotate 270º elif p_rotate &gt; .5: image = tf.image.rot90(image, k=2) # rotate 180º elif p_rotate &gt; .25: image = tf.image.rot90(image, k=1) # rotate 90º if p_rotation &gt;= .3: # Rotation image = transform_rotation(image, height=HEIGHT, rotation=45.) if p_shift &gt;= .3: # Shift image = transform_shift(image, height=HEIGHT, h_shift=15., w_shift=15.) if p_shear &gt;= .3: # Shear image = transform_shear(image, height=HEIGHT, shear=20.) # Crops if p_crop &gt; .4: crop_size = tf.random.uniform([], int(HEIGHT*.7), HEIGHT, dtype=tf.int32) image = tf.image.random_crop(image, size=[crop_size, crop_size, CHANNELS]) elif p_crop &gt; .7: if p_crop &gt; .9: image = tf.image.central_crop(image, central_fraction=.7) elif p_crop &gt; .8: image = tf.image.central_crop(image, central_fraction=.8) else: image = tf.image.central_crop(image, central_fraction=.9) image = tf.image.resize(image, size=[HEIGHT, WIDTH]) # Pixel-level transforms if p_pixel &gt;= .2: if p_pixel &gt;= .8: image = tf.image.random_saturation(image, lower=0, upper=2) elif p_pixel &gt;= .6: image = tf.image.random_contrast(image, lower=.8, upper=2) elif p_pixel &gt;= .4: image = tf.image.random_brightness(image, max_delta=.2) else: image = tf.image.adjust_gamma(image, gamma=.6) return image, label . . . Tip: experimental_deterministic is set to decide whether the outputs need to be produced in deterministic order. Default: True . option_no_order = tf.data.Options() option_no_order.experimental_deterministic = False . train_ds = tf.data.TFRecordDataset(train_filenames, num_parallel_reads=AUTOTUNE) train_ds = (train_ds .map(collate_labeled_tfrecord, num_parallel_calls=AUTOTUNE) .map(data_augment, num_parallel_calls=AUTOTUNE) .repeat() .shuffle(2048) .batch(BATCH_SIZE) .prefetch(AUTOTUNE)) . show_images(train_ds.take(1).unbatch()) . valid_ds = tf.data.TFRecordDataset(valid_filenames, num_parallel_reads=AUTOTUNE) valid_ds = (valid_ds .with_options(option_no_order) .map(collate_labeled_tfrecord, num_parallel_calls=AUTOTUNE) .batch(BATCH_SIZE) .cache() .prefetch(AUTOTUNE)) . show_images(valid_ds.take(1).unbatch()) . test_ds = tf.data.TFRecordDataset(test_filenames, num_parallel_reads=AUTOTUNE) test_ds = (test_ds .with_options(option_no_order) .map(process_unlabeled_tfrecord, num_parallel_calls=AUTOTUNE) .batch(BATCH_SIZE) .prefetch(AUTOTUNE)) . Model . Batch augmentation . Augmentation can be applied in two ways. . Using the Keras Preprocessing Layers | Using the tf.image | . . Important: The Keras Preprocessing Layers are currently experimental so it seems it does not have supporting TPU OpKernel yet. . #batch_augment = tf.keras.Sequential( # [ # tf.keras.layers.experimental.preprocessing.RandomCrop(*IMG_SIZE), # tf.keras.layers.experimental.preprocessing.RandomFlip(&quot;horizontal_and_vertical&quot;), # tf.keras.layers.experimental.preprocessing.RandomRotation(0.25), # tf.keras.layers.experimental.preprocessing.RandomZoom((-0.2, 0)), # tf.keras.layers.experimental.preprocessing.RandomContrast((0.2,0.2)) # ] #) . . #func = lambda x,y: (batch_augment(x), y) #x = (train_ds # .take(1) # .map(func, num_parallel_calls=AUTOTUNE)) . . Building a model . Now we&#39;re ready to create a neural network for classifying images! We&#39;ll use what&#39;s known as transfer learning. With transfer learning, you reuse the body part of a pretrained model and replace its&#39; head or tail with custom layers depending on the problem that we are solving. . For this tutorial, we&#39;ll use EfficientNetb3 which is pretrained on ImageNet. Later, I might want to experiment with other models. (Xception wouldn&#39;t be a bad choice.) . . Important: The distribution strategy we created earilier contains a context manager, straategy.scope. This context manager tells TensorFlow how to divide the work of training among the eight TPU cores. When using TensorFlow with a TPU, it&#8217;s important to define your model in strategy.sceop() context. . %%run_if {KAGGLE} subprocess.check_call([sys.executable, &#39;-m&#39;, &#39;pip&#39;, &#39;install&#39;,&#39;-q&#39;, &#39;efficientnet&#39;]) from efficientnet.tfkeras import EfficientNetB3 . %%run_if {GOOGLE} from tensorflow.keras.applications import EfficientNetB3 from tensorflow.keras.applications import VGG16 . def build_model(base_model, num_class): inputs = tf.keras.layers.Input(shape=IMG_SIZE) x = base_model(inputs) x = tf.keras.layers.Dropout(0.4)(x) outputs = tf.keras.layers.Dense(num_class, activation=&quot;softmax&quot;, name=&quot;pred&quot;)(x) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) return model . with strategy.scope(): efficientnet = EfficientNetB3( weights = &#39;imagenet&#39; if TRAIN else None, include_top = False, input_shape = IMG_SIZE, pooling=&#39;avg&#39;) efficientnet.trainable = True model = build_model(base_model=efficientnet, num_class=len(CLASSES)) . Optimizer . . Important: I always wanted to try the new CosineDecayRestarts function implemented in tf.keras as it seemed promising and I struggled to find the right settings (if there were any) for the ReduceLROnPlateau . STEPS = math.ceil(count_data_items(train_filenames) / BATCH_SIZE) * EPOCHS LR_START = 1e-4 #@param {type: &quot;number&quot;} LR_START *= strategy.num_replicas_in_sync LR_MIN = 1e-5 #@param {type: &quot;number&quot;} N_RESTARTS = 5#@param {type: &quot;number&quot;} T_MUL = 2.0 #@param {type: &quot;number&quot;} M_MUL = 1#@param {type: &quot;number&quot;} STEPS_START = math.ceil((T_MUL-1)/(T_MUL**(N_RESTARTS+1)-1) * STEPS) schedule = tf.keras.experimental.CosineDecayRestarts( first_decay_steps=STEPS_START, initial_learning_rate=LR_START, alpha=LR_MIN, m_mul=M_MUL, t_mul=T_MUL) x = [i for i in range(STEPS)] y = [schedule(s) for s in range(STEPS)] _,ax = plt.subplots(1,1,figsize=(8,5),facecolor=&#39;#F0F0F0&#39;) ax.plot(x, y) ax.set_facecolor(&#39;#F8F8F8&#39;) ax.set_xlabel(&#39;iteration&#39;) ax.set_ylabel(&#39;learning rate&#39;) print(&#39;{:d} total epochs and {:d} steps per epoch&#39; .format(EPOCHS, STEPS // EPOCHS)) print(schedule.get_config()) . 50 total epochs and 50 steps per epoch {&#39;initial_learning_rate&#39;: 0.0008, &#39;first_decay_steps&#39;: 40, &#39;t_mul&#39;: 2.0, &#39;m_mul&#39;: 1, &#39;alpha&#39;: 1e-05, &#39;name&#39;: None} . Callbacks . callbacks = [ tf.keras.callbacks.ModelCheckpoint( filepath=&#39;001_best_model.h5&#39;, monitor=&#39;val_loss&#39;, save_best_only=True), tf.keras.callbacks.EarlyStopping( monitor=&#39;val_loss&#39;, mode=&#39;min&#39;, patience=10, restore_best_weights=True, verbose=1) ] model.compile( optimizer=tf.keras.optimizers.Adam(schedule), loss = &#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;sparse_categorical_accuracy&#39;] ) . model.summary() . Model: &#34;model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 300, 300, 3)] 0 _________________________________________________________________ efficientnetb3 (Functional) (None, 1536) 10783535 _________________________________________________________________ dropout (Dropout) (None, 1536) 0 _________________________________________________________________ pred (Dense) (None, 104) 159848 ================================================================= Total params: 10,943,383 Trainable params: 10,856,080 Non-trainable params: 87,303 _________________________________________________________________ . Training . Train the normalization layer . %%run_if {GOOGLE} def generate_norm_image(example): LABELED_TFREC_FORMAT = { &quot;image&quot;: tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring &quot;class&quot;: tf.io.FixedLenFeature([], tf.int64), # shape [] means single element } example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT) image = decode_image(example[&#39;image&#39;]) image = image / 255.0 return image . %%run_if {GOOGLE} if os.path.exists(&quot;000_normalization.h5&quot;): model.load_weights(&quot;000_normalization.h5&quot;) else: adapt_ds = (tf.data.TFRecordDataset(train_filenames, num_parallel_reads=AUTOTUNE) .map(generate_norm_image, num_parallel_calls=AUTOTUNE) .shuffle(2048) .batch(BATCH_SIZE) .prefetch(AUTOTUNE)) model.get_layer(&#39;efficientnetb3&#39;).get_layer(&#39;normalization&#39;).adapt(adapt_ds) model.save_weights(&quot;000_normalization.h5&quot;) . Train all . history = model.fit( x=train_ds, validation_data=valid_ds, epochs=EPOCHS, steps_per_epoch=STEPS//BATCH_SIZE, callbacks=callbacks, verbose=2 ) . Epoch 1/50 9/9 - 156s - loss: 1.9010 - sparse_categorical_accuracy: 0.5043 - val_loss: 13.0768 - val_sparse_categorical_accuracy: 0.2594 Epoch 2/50 9/9 - 6s - loss: 1.6419 - sparse_categorical_accuracy: 0.5647 - val_loss: 6.8306 - val_sparse_categorical_accuracy: 0.3257 Epoch 3/50 9/9 - 6s - loss: 1.5901 - sparse_categorical_accuracy: 0.5864 - val_loss: 3.9058 - val_sparse_categorical_accuracy: 0.4119 Epoch 4/50 9/9 - 6s - loss: 1.5140 - sparse_categorical_accuracy: 0.5877 - val_loss: 2.9674 - val_sparse_categorical_accuracy: 0.4723 Epoch 5/50 9/9 - 10s - loss: 1.5115 - sparse_categorical_accuracy: 0.5959 - val_loss: 2.2275 - val_sparse_categorical_accuracy: 0.5275 Epoch 6/50 9/9 - 7s - loss: 1.4713 - sparse_categorical_accuracy: 0.6050 - val_loss: 1.6515 - val_sparse_categorical_accuracy: 0.6051 Epoch 7/50 9/9 - 7s - loss: 1.4460 - sparse_categorical_accuracy: 0.6155 - val_loss: 1.3832 - val_sparse_categorical_accuracy: 0.6360 Epoch 8/50 9/9 - 7s - loss: 1.3661 - sparse_categorical_accuracy: 0.6337 - val_loss: 1.3275 - val_sparse_categorical_accuracy: 0.6509 Epoch 9/50 9/9 - 6s - loss: 1.3910 - sparse_categorical_accuracy: 0.6215 - val_loss: 1.2825 - val_sparse_categorical_accuracy: 0.6608 Epoch 10/50 9/9 - 6s - loss: 1.2811 - sparse_categorical_accuracy: 0.6562 - val_loss: 1.2094 - val_sparse_categorical_accuracy: 0.6797 Epoch 11/50 9/9 - 6s - loss: 1.2926 - sparse_categorical_accuracy: 0.6484 - val_loss: 1.1464 - val_sparse_categorical_accuracy: 0.6950 Epoch 12/50 9/9 - 6s - loss: 1.2898 - sparse_categorical_accuracy: 0.6302 - val_loss: 1.1089 - val_sparse_categorical_accuracy: 0.7050 Epoch 13/50 9/9 - 6s - loss: 1.2204 - sparse_categorical_accuracy: 0.6784 - val_loss: 1.0893 - val_sparse_categorical_accuracy: 0.7096 Epoch 14/50 9/9 - 6s - loss: 1.2152 - sparse_categorical_accuracy: 0.6623 - val_loss: 1.0783 - val_sparse_categorical_accuracy: 0.7142 Epoch 15/50 9/9 - 6s - loss: 1.2462 - sparse_categorical_accuracy: 0.6576 - val_loss: 1.0587 - val_sparse_categorical_accuracy: 0.7217 Epoch 16/50 9/9 - 6s - loss: 1.2206 - sparse_categorical_accuracy: 0.6688 - val_loss: 0.9961 - val_sparse_categorical_accuracy: 0.7314 Epoch 17/50 9/9 - 7s - loss: 1.2518 - sparse_categorical_accuracy: 0.6636 - val_loss: 1.0047 - val_sparse_categorical_accuracy: 0.7368 Epoch 18/50 9/9 - 7s - loss: 1.1942 - sparse_categorical_accuracy: 0.6727 - val_loss: 0.9873 - val_sparse_categorical_accuracy: 0.7384 Epoch 19/50 9/9 - 9s - loss: 1.1119 - sparse_categorical_accuracy: 0.7010 - val_loss: 0.9598 - val_sparse_categorical_accuracy: 0.7395 Epoch 20/50 9/9 - 7s - loss: 1.1494 - sparse_categorical_accuracy: 0.6944 - val_loss: 0.9371 - val_sparse_categorical_accuracy: 0.7532 Epoch 21/50 9/9 - 7s - loss: 1.1256 - sparse_categorical_accuracy: 0.6879 - val_loss: 0.9200 - val_sparse_categorical_accuracy: 0.7627 Epoch 22/50 9/9 - 6s - loss: 1.0482 - sparse_categorical_accuracy: 0.7201 - val_loss: 0.8943 - val_sparse_categorical_accuracy: 0.7694 Epoch 23/50 9/9 - 6s - loss: 1.0809 - sparse_categorical_accuracy: 0.7153 - val_loss: 0.9161 - val_sparse_categorical_accuracy: 0.7648 Epoch 24/50 9/9 - 6s - loss: 1.0814 - sparse_categorical_accuracy: 0.7114 - val_loss: 0.9213 - val_sparse_categorical_accuracy: 0.7619 Epoch 25/50 9/9 - 6s - loss: 1.0078 - sparse_categorical_accuracy: 0.7214 - val_loss: 0.8947 - val_sparse_categorical_accuracy: 0.7675 Epoch 26/50 9/9 - 7s - loss: 0.9709 - sparse_categorical_accuracy: 0.7391 - val_loss: 0.8600 - val_sparse_categorical_accuracy: 0.7753 Epoch 27/50 9/9 - 6s - loss: 1.0287 - sparse_categorical_accuracy: 0.7270 - val_loss: 0.8354 - val_sparse_categorical_accuracy: 0.7872 Epoch 28/50 9/9 - 6s - loss: 0.9749 - sparse_categorical_accuracy: 0.7396 - val_loss: 0.8162 - val_sparse_categorical_accuracy: 0.7920 Epoch 29/50 9/9 - 6s - loss: 0.9431 - sparse_categorical_accuracy: 0.7383 - val_loss: 0.8065 - val_sparse_categorical_accuracy: 0.7939 Epoch 30/50 9/9 - 7s - loss: 0.9715 - sparse_categorical_accuracy: 0.7422 - val_loss: 0.7995 - val_sparse_categorical_accuracy: 0.7936 Epoch 31/50 9/9 - 7s - loss: 0.9164 - sparse_categorical_accuracy: 0.7530 - val_loss: 0.7948 - val_sparse_categorical_accuracy: 0.7947 Epoch 32/50 9/9 - 6s - loss: 0.9203 - sparse_categorical_accuracy: 0.7487 - val_loss: 0.8642 - val_sparse_categorical_accuracy: 0.7683 Epoch 33/50 9/9 - 7s - loss: 0.9915 - sparse_categorical_accuracy: 0.7313 - val_loss: 0.8313 - val_sparse_categorical_accuracy: 0.7823 Epoch 34/50 9/9 - 6s - loss: 1.0157 - sparse_categorical_accuracy: 0.7296 - val_loss: 0.8204 - val_sparse_categorical_accuracy: 0.7842 Epoch 35/50 9/9 - 7s - loss: 1.0600 - sparse_categorical_accuracy: 0.7188 - val_loss: 0.8281 - val_sparse_categorical_accuracy: 0.7866 Epoch 36/50 9/9 - 6s - loss: 0.9717 - sparse_categorical_accuracy: 0.7361 - val_loss: 0.8413 - val_sparse_categorical_accuracy: 0.7761 Epoch 37/50 9/9 - 7s - loss: 0.9517 - sparse_categorical_accuracy: 0.7326 - val_loss: 0.8223 - val_sparse_categorical_accuracy: 0.7839 Epoch 38/50 9/9 - 7s - loss: 0.8959 - sparse_categorical_accuracy: 0.7626 - val_loss: 0.7921 - val_sparse_categorical_accuracy: 0.7874 Epoch 39/50 9/9 - 7s - loss: 1.0345 - sparse_categorical_accuracy: 0.7196 - val_loss: 0.7694 - val_sparse_categorical_accuracy: 0.8036 Epoch 40/50 9/9 - 7s - loss: 0.9000 - sparse_categorical_accuracy: 0.7465 - val_loss: 0.7594 - val_sparse_categorical_accuracy: 0.8036 Epoch 41/50 9/9 - 7s - loss: 0.8503 - sparse_categorical_accuracy: 0.7626 - val_loss: 0.7567 - val_sparse_categorical_accuracy: 0.8012 Epoch 42/50 9/9 - 6s - loss: 0.8809 - sparse_categorical_accuracy: 0.7530 - val_loss: 0.7373 - val_sparse_categorical_accuracy: 0.8082 Epoch 43/50 9/9 - 6s - loss: 0.9019 - sparse_categorical_accuracy: 0.7539 - val_loss: 0.7463 - val_sparse_categorical_accuracy: 0.8106 Epoch 44/50 9/9 - 6s - loss: 0.8453 - sparse_categorical_accuracy: 0.7674 - val_loss: 0.7092 - val_sparse_categorical_accuracy: 0.8155 Epoch 45/50 9/9 - 7s - loss: 0.7987 - sparse_categorical_accuracy: 0.7687 - val_loss: 0.7131 - val_sparse_categorical_accuracy: 0.8152 Epoch 46/50 9/9 - 6s - loss: 0.8338 - sparse_categorical_accuracy: 0.7769 - val_loss: 0.7005 - val_sparse_categorical_accuracy: 0.8171 Epoch 47/50 9/9 - 7s - loss: 0.7969 - sparse_categorical_accuracy: 0.7799 - val_loss: 0.6921 - val_sparse_categorical_accuracy: 0.8230 Epoch 48/50 9/9 - 6s - loss: 0.7685 - sparse_categorical_accuracy: 0.7891 - val_loss: 0.6948 - val_sparse_categorical_accuracy: 0.8198 Epoch 49/50 9/9 - 7s - loss: 0.7290 - sparse_categorical_accuracy: 0.7986 - val_loss: 0.6804 - val_sparse_categorical_accuracy: 0.8254 Epoch 50/50 9/9 - 7s - loss: 0.7546 - sparse_categorical_accuracy: 0.7925 - val_loss: 0.6556 - val_sparse_categorical_accuracy: 0.8349 . . Training curve . import pickle with open(&#39;trainHistoryDict&#39;, &#39;wb&#39;) as fd: pickle.dump(history.history, fd) . import pickle history = pickle.load(open(&#39;trainHistoryDict&#39;, &quot;rb&quot;)) . def show_history(history): topics = [&#39;loss&#39;, &#39;accuracy&#39;] groups = [{k:v for (k,v) in history.items() if topic in k} for topic in topics] _,axs = plt.subplots(1,2,figsize=(15,6),facecolor=&#39;#F0F0F0&#39;) for topic,group,ax in zip(topics,groups,axs.flatten()): for (_,v) in group.items(): ax.plot(v) ax.set_facecolor(&#39;#F8F8F8&#39;) ax.set_title(f&#39;{topic} over epochs&#39;) ax.set_xlabel(&#39;epoch&#39;) ax.set_ylabel(topic) ax.legend([&#39;train&#39;, &#39;valid&#39;], loc=&#39;best&#39;) . show_history(history.history) . def show_confusion_matrix(cmat, score, precision, recall): _,ax = plt.subplots(1,1,figsize=(12,12),facecolor=&#39;#F0F0F0&#39;) ax.matshow(cmat, cmap=&#39;Blues&#39;) if len(CLASSES) &lt;= 10: ax.set_xticks(range(len(CLASSES)),) ax.set_xticklabels(CLASSES, fontdict={&#39;fontsize&#39;: 7}) plt.setp(ax.get_xticklabels(), rotation=45, ha=&quot;left&quot;, rotation_mode=&quot;anchor&quot;) ax.set_yticks(range(len(CLASSES))) ax.set_yticklabels(CLASSES, fontdict={&#39;fontsize&#39;: 7}) plt.setp(ax.get_yticklabels(), rotation=45, ha=&quot;right&quot;, rotation_mode=&quot;anchor&quot;) else: ax.axis(&#39;off&#39;) textstr = &quot;&quot; if precision: textstr += &#39;precision = {:.3f} &#39;.format(precision) if recall: textstr += &#39; nrecall = {:.3f} &#39;.format(recall) if score: textstr += &#39; nf1 = {:.3f} &#39;.format(score) if len(textstr) &gt; 0: props = dict(boxstyle=&#39;round&#39;, facecolor=&#39;wheat&#39;, alpha=0.2) ax.text(0.75, 0.95, textstr, transform=ax.transAxes, fontsize=14, verticalalignment=&#39;top&#39;, bbox=props) plt.show() . . model.load_weights(&quot;001_best_model.h5&quot;) . ordered_valid_ds = tf.data.TFRecordDataset(valid_filenames, num_parallel_reads=AUTOTUNE) ordered_valid_ds = (ordered_valid_ds .map(collate_labeled_tfrecord, num_parallel_calls=AUTOTUNE) .batch(BATCH_SIZE) .cache() .prefetch(AUTOTUNE)) x_valid_ds = ordered_valid_ds.map(lambda x,y : x, num_parallel_calls=AUTOTUNE) y_valid_ds = ordered_valid_ds.map(lambda x,y : y, num_parallel_calls=AUTOTUNE) . y_true = (y_valid_ds .unbatch() .batch(count_data_items(valid_filenames)) .as_numpy_iterator() .next()) y_probs = model.predict(x_valid_ds) y_preds = np.argmax(y_probs, axis=-1) . label_ids = range(len(CLASSES)) cmatrix = confusion_matrix(y_true, y_preds, labels=label_ids) cmatrix = (cmatrix.T / cmatrix.sum(axis=1)).T # normalize . You might be familiar with metrics like F1-score or precision and recall. This cell will compute these metrics and display them with a plot of the confusion matrix. (These metrics are defined in the Scikit-learn module sklearn.metrics; we&#39;ve imported them in the helper script for you.) . precision = precision_score(y_true, y_preds, labels=label_ids, average=&#39;macro&#39;) recall = recall_score(y_true, y_preds, labels=label_ids, average=&#39;macro&#39;) score = f1_score(y_true, y_preds, labels=label_ids,average=&#39;macro&#39;) . show_confusion_matrix(cmatrix, score, precision, recall) . Prediction . Once you&#39;re satisfied with everything, you&#39;re ready to make predictions on the test set. . test_ds = tf.data.TFRecordDataset(test_filenames, num_parallel_reads=AUTOTUNE) test_ds = (test_ds .map(process_unlabeled_tfrecord, num_parallel_calls=AUTOTUNE) .batch(BATCH_SIZE) .prefetch(AUTOTUNE)) x_test_ds = test_ds.map(lambda image,idnum: image) . y_probs = model.predict(x_test_ds) y_preds = np.argmax(y_probs, axis=-1) . Let&#39;s generate a file submission.csv. This file is what you&#39;ll submit to get your score on the leaderboard. . id_test_ds = test_ds.map(lambda image,idnum: idnum) . id_test_ds = (id_test_ds.unbatch() .batch(count_data_items(test_filenames)) .as_numpy_iterator() .next() .astype(&#39;U&#39;)) . np.savetxt(&#39;submission.csv&#39;, np.rec.fromarrays([id_test_ds, y_preds]), fmt=[&#39;%s&#39;, &#39;%d&#39;], delimiter=&#39;,&#39;, header=&#39;id,label&#39;, comments=&#39;&#39;) . !head submission.csv . id,label 252d840db,67 1c4736dea,28 c37a6f3e9,83 00e4f514e,103 59d1b6146,46 8d808a07b,53 aeb67eefb,52 53cfc6586,71 aaa580243,85 . 1% Better Everyday . reference . How to use my own data source? | TPU-speed data pipelines: tf.data.Dataset and TFRecords | . . todos . ~Finish the interpretation section~ | ~Comment out the 1/255.0 in the image preprocessing~ | ~Reorganize the notebook structure~ | .",
            "url": "https://austinyhc.github.io/blog/plant/classification/tpu/2020/12/30/petals-to-the-metal.html",
            "relUrl": "/plant/classification/tpu/2020/12/30/petals-to-the-metal.html",
            "date": " • Dec 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Jane Street Market Prediction",
            "content": "%%writefile conditional_cell_extension.py def run_if(line, cell=None): &#39;&#39;&#39;Execute current line/cell if line evaluates to True.&#39;&#39;&#39; if not eval(line): return get_ipython().ex(cell) def load_ipython_extension(shell): &#39;&#39;&#39;Registers the run_if magic when the extension loads.&#39;&#39;&#39; shell.register_magic_function(run_if, &#39;line_cell&#39;) def unload_ipython_extension(shell): &#39;&#39;&#39;Unregisters the run_if magic when the extension unloads.&#39;&#39;&#39; del shell.magics_manager.magics[&#39;cell&#39;][&#39;run_if&#39;] . Writing conditional_cell_extension.py . %reload_ext conditional_cell_extension . Dependencies . !pip install dabl &gt; /dev/null !pip install datatable &gt; /dev/null . import numpy as np import pandas as pd import seaborn as sns import albumentations as A import matplotlib.pyplot as plt import os, gc, cv2, random, warnings, math, sys, json, pprint, pdb import tensorflow as tf from tensorflow.keras import backend as K import tensorflow_hub as hub from sklearn.model_selection import train_test_split import dabl import datatable as dt warnings.simplefilter(&#39;ignore&#39;) print(f&quot;Using TensorFlow v{tf.__version__}&quot;) . Using TensorFlow v2.4.0 . #@title Accelerator { run: &quot;auto&quot; } DEVICE = &#39;GPU&#39; #@param [&quot;None&quot;, &quot;&#39;GPU&#39;&quot;, &quot;&#39;TPU&#39;&quot;] {type:&quot;raw&quot;, allow-input: true} if DEVICE == &quot;TPU&quot;: print(&quot;connecting to TPU...&quot;) try: tpu = tf.distribute.cluster_resolver.TPUClusterResolver() print(&#39;Running on TPU &#39;, tpu.master()) except ValueError: print(&quot;Could not connect to TPU&quot;) tpu = None if tpu: try: print(&quot;initializing TPU ...&quot;) tf.config.experimental_connect_to_cluster(tpu) tf.tpu.experimental.initialize_tpu_system(tpu) strategy = tf.distribute.experimental.TPUStrategy(tpu) print(&quot;TPU initialized&quot;) except _: print(&quot;failed to initialize TPU&quot;) else: DEVICE = &quot;GPU&quot; if DEVICE != &quot;TPU&quot;: print(&quot;Using default strategy for CPU and single GPU&quot;) strategy = tf.distribute.get_strategy() if DEVICE == &quot;GPU&quot;: print(&quot;Num GPUs Available: &quot;, len(tf.config.experimental.list_physical_devices(&#39;GPU&#39;))) AUTOTUNE = tf.data.experimental.AUTOTUNE REPLICAS = strategy.num_replicas_in_sync print(f&#39;REPLICAS: {REPLICAS}&#39;) . Using default strategy for CPU and single GPU Num GPUs Available: 1 REPLICAS: 1 . from google.colab import files uploaded = files.upload() for fn in uploaded.keys(): print(&#39;User uploaded file &quot;{name}&quot; with length {length} bytes&#39;.format( name=fn, length=len(uploaded[fn]))) # Then move kaggle.json into the folder where the API expects to find it. !mkdir -p ~/.kaggle/ &amp;&amp; mv kaggle.json ~/.kaggle/ &amp;&amp; chmod 600 ~/.kaggle/kaggle.json . def seed_everything(seed=0): random.seed(seed) np.random.seed(seed) tf.random.set_seed(seed) os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) os.environ[&#39;TF_DETERMINISTIC_OPS&#39;] = &#39;1&#39; GOOGLE = &#39;google.colab&#39; in str(get_ipython()) KAGGLE = not GOOGLE print(&quot;Running on {}!&quot;.format( &quot;Google Colab&quot; if GOOGLE else &quot;Kaggle Kernel&quot; )) . Running on Google Colab! . %%run_if {GOOGLE} from google.colab import drive drive.mount(&#39;/content/gdrive&#39;, force_remount=True) . Mounted at /content/gdrive . project_name = &#39;jane-street-market-prediction&#39; root_path = &#39;/content/gdrive/MyDrive/&#39; if GOOGLE else &#39;/&#39; input_path = f&#39;{root_path}kaggle/input/{project_name}/&#39; working_path = f&#39;{input_path}working/&#39; if GOOGLE else &#39;/kaggle/working/&#39; os.makedirs(working_path, exist_ok=True) os.chdir(working_path) os.listdir(input_path) . [&#39;train.csv&#39;, &#39;working&#39;, &#39;__init__.py&#39;, &#39;competition.cpython-37m-x86_64-linux-gnu.so&#39;, &#39;train.csv.zip&#39;, &#39;example_sample_submission.csv&#39;, &#39;features.csv&#39;, &#39;example_test.csv.zip&#39;] . !kaggle competitions download -c jane-street-market-prediction . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.10 / client 1.5.4) Downloading __init__.py to /content/gdrive/MyDrive/kaggle/input/jane-street-market-prediction/working 0% 0.00/59.0 [00:00&lt;?, ?B/s] 100% 59.0/59.0 [00:00&lt;00:00, 8.29kB/s] Downloading competition.cpython-37m-x86_64-linux-gnu.so to /content/gdrive/My Drive/kaggle/input/jane-street-market-prediction/working 0% 0.00/441k [00:00&lt;?, ?B/s] 100% 441k/441k [00:00&lt;00:00, 28.4MB/s] Downloading train.csv.zip to /content/gdrive/My Drive/kaggle/input/jane-street-market-prediction/working 100% 2.60G/2.61G [00:26&lt;00:00, 49.7MB/s] 100% 2.61G/2.61G [00:26&lt;00:00, 106MB/s] Downloading example_sample_submission.csv to /content/gdrive/My Drive/kaggle/input/jane-street-market-prediction/working 0% 0.00/108k [00:00&lt;?, ?B/s] 100% 108k/108k [00:00&lt;00:00, 15.2MB/s] Downloading features.csv to /content/gdrive/My Drive/kaggle/input/jane-street-market-prediction/working 0% 0.00/23.3k [00:00&lt;?, ?B/s] 100% 23.3k/23.3k [00:00&lt;00:00, 3.31MB/s] Downloading example_test.csv.zip to /content/gdrive/My Drive/kaggle/input/jane-street-market-prediction/working 55% 9.00M/16.4M [00:00&lt;00:00, 40.0MB/s] 100% 16.4M/16.4M [00:00&lt;00:00, 54.6MB/s] . We can observe that the train.csv is large: 6GB and it has 2390492 rows in the file. . !wc -l {input_path}train.csv . 2390492 /content/gdrive/MyDrive/kaggle/input/jane-street-market-prediction/train.csv . To speed things up here, let&#39;s use datatable to read the data, and then convert to a pandas dataframe. . %%time train_dt = dt.fread(f&quot;{input_path}train.csv&quot;) . CPU times: user 28.6 s, sys: 5.02 s, total: 33.6 s Wall time: 4min 41s . %%time train_df = train_dt.to_pandas() . CPU times: user 5.37 s, sys: 3.98 s, total: 9.35 s Wall time: 7.07 s . Exploration . resp . fig,ax = plt.subplots(figsize=(15,5), facecolor=&quot;#F0F0F0&quot;) balance = pd.Series(train_df[&#39;resp&#39;].cumsum()) ax.set_xlabel(&quot;Trade&quot;, fontsize=18) ax.set_ylabel(&quot;Cumulative resp&quot;, fontsize=18) balance.plot(lw=3) del balance gc.collect() . 2487 . as well as four time horizons . The longer the Time Horizon, the more aggressive, or riskier portfolio, ans investor can build. The shorter the Time Horizon, the more conservative, or less risky, the investor may want to adopt. . fig,ax = plt.subplots(figsize=(15,5), facecolor=&quot;#F0F0F0&quot;) resps = [] for colName, colData in train_df.iteritems(): if (&#39;resp&#39; in colName): resps.append(colData.cumsum()) ax.set_xlabel(&quot;Trade&quot;, fontsize=18) ax.set_title(&quot;Cumulative resp and time horizons 1,2,3,4 (500 days)&quot;, fontsize=18) for r in resps: r.plot(lw=3) plt.legend(loc=&quot;upper left&quot;) del resps; gc.collect() . 3382 . We can see that resp (in purple) most closely follows time horizon 4 (resp_4 is the uppermost curve, in red). . In the notebook Jane Street: time horizons and volatilities by @pcarta, if I understand correctly, by using maximum likelihood estimation it is calculated that if the time horizon ($T_j$) for resp_1 (i.e. $T_1$ is 1, then . $T_j$(resp_2) $ approx 1.4T_1$ | $T_j$(resp_3) $ approx 3.9T_1$ | $T_j$(resp_4) $ approx 11.1T_1$ where $T_1$ could correspond to 5 trading days. Let&#39;s now plot a histogram of all of the resp values (here only shown for values between -0.05 and 0.05) | . fig,ax = plt.subplots(figsize=(12,5), facecolor=&quot;#F0F0F0&quot;) sns.histplot( ax=ax, x=train_df[&#39;resp&#39;], bins=3000, kde_kws={&quot;clip&quot;:(-0.05,0.05)}, binrange=(-0.05,0.05), color=&#39;darkcyan&#39;, kde=False) values = np.array([rec.get_height() for rec in ax.patches]) norm = plt.Normalize(values.min(), values.max()) colors = plt.cm.jet(norm(values)) for rec, col in zip(ax.patches, colors): rec.set_color(col) plt.xlabel(&quot;Histogram of the resp values&quot;, size=14) plt.show(); del values gc.collect(); . This distribution has very long tails . print(&#39;The minimum value for resp is : %.5f&#39; % train_df[&#39;resp&#39;].min()) print(&#39;The minimum value for resp is : %.5f&#39; % train_df[&#39;resp&#39;].max()) . The minimum value for resp is : -0.54938 The minimum value for resp is : 0.44846 . Also calculate its&#39; skew and kurtosis of this distribution . print(&quot;Skew of resp is: %.2f&quot; % train_df[&#39;resp&#39;].skew() ) print(&quot;Kurtosis of resp is: %.2f&quot; % train_df[&#39;resp&#39;].kurtosis() ) . Skew of resp is: 0.10 Kurtosis of resp is: 17.36 . weight . Each trade has an associated weight and resp, which together represents a return on the trade. Trades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation. . percent_zeros = (100/train_df.shape[0])*((train_df.weight.values == 0).sum()) print(&#39;Percentage of zero weights is: %i&#39; % percent_zeros +&quot;%&quot;) . Percentage of zero weights is: 0% . Let us see if there are any negative weights. A negative weight would be meaningless, but you never know. . min_weight = train_df[&#39;weight&#39;].min() print(&#39;The minimum weight is: %.2f&#39; % min_weight) . The minimum weight is: 0.01 . An now to find the maximum weight used . max_weight = train_df[&#39;weight&#39;].max() print(&#39;The maximum weight was: %.2f&#39; % max_weight) . The maximum weight was: 167.29 . which occured on day 446 . train_df[train_df[&#39;weight&#39;]==train_df[&#39;weight&#39;].max()] . date weight resp_1 resp_2 resp_3 resp_4 resp feature_0 feature_1 feature_2 feature_3 feature_4 feature_5 feature_6 feature_7 feature_8 feature_9 feature_10 feature_11 feature_12 feature_13 feature_14 feature_15 feature_16 feature_17 feature_18 feature_19 feature_20 feature_21 feature_22 feature_23 feature_24 feature_25 feature_26 feature_27 feature_28 feature_29 feature_30 feature_31 feature_32 ... feature_92 feature_93 feature_94 feature_95 feature_96 feature_97 feature_98 feature_99 feature_100 feature_101 feature_102 feature_103 feature_104 feature_105 feature_106 feature_107 feature_108 feature_109 feature_110 feature_111 feature_112 feature_113 feature_114 feature_115 feature_116 feature_117 feature_118 feature_119 feature_120 feature_121 feature_122 feature_123 feature_124 feature_125 feature_126 feature_127 feature_128 feature_129 ts_id action . 1322733 446 | 167.293716 | 0.000281 | 0.001213 | 0.00138 | -0.000427 | -0.001215 | -1 | -0.735754 | -0.048433 | -0.175366 | -0.20698 | 0.150967 | 0.23681 | 0.035755 | 0.026819 | -0.981118 | -0.188376 | 0.069351 | 0.060235 | -0.361906 | 0.09651 | -0.789638 | -0.582599 | 0.110738 | 0.114945 | -0.455587 | -0.844032 | 0.167681 | 0.177815 | -0.3927 | -0.804331 | -0.527014 | -0.93592 | 0.12584 | 0.160809 | 0.560677 | 1.263861 | 0.207275 | 0.252047 | ... | 0.314832 | 1.342298 | 1.391578 | 0.900093 | 0.401429 | -1.338859 | 0.420475 | -0.509267 | -1.194013 | -0.420336 | 0.244751 | -0.94019 | 0.320808 | -0.237071 | -1.781693 | 0.616443 | 0.392753 | -0.484772 | 0.417847 | -0.253349 | -0.843852 | -0.468821 | 0.262442 | 0.935519 | 0.288209 | 0.828683 | 0.438545 | 0.57467 | -0.92586 | 0.948026 | -1.094062 | 0.326287 | -0.715126 | 1.490866 | -1.111595 | 1.083793 | -0.979801 | 0.913979 | 2097681 | 0 | . 1 rows × 139 columns . Let us take a look at a histogram of the non-zero weights . fig,ax = plt.subplots(figsize = (12,5), facecolor=&quot;#F0F0F0&quot;) sns.histplot( ax=ax, x=train_df[&#39;weight&#39;], bins=1400, kde_kws={&quot;clip&quot;:(0.001,1.4)}, binrange=(0.001,1.4), color=&#39;darkcyan&#39;, kde=False); values = np.array([rec.get_height() for rec in ax.patches]) norm = plt.Normalize(values.min(), values.max()) colors = plt.cm.jet(norm(values)) for rec, col in zip(ax.patches, colors): rec.set_color(col) plt.xlabel(&quot;Histogram of non-zero weights&quot;, size=14) plt.show(); del values gc.collect(); . https://www.kaggle.com/carlmcbrideellis/jane-street-eda-of-day-0-and-feature-importance . HyperParameters . Data . Loading training data . train_df = train_df.query(&#39;date &gt; 85&#39;).reset_index(drop=True) . train_df = train_df.astype({c: np.float32 for c in train_df.select_dtypes(include=&#39;float64&#39;).columns}) . train_df.fillna(train_df.mean(), inplace=True) . train_df = train_df.query(&#39;weight &gt; 0&#39;).reset_index(drop = True) . train_df[&#39;action&#39;] = ((train_df[&#39;resp_1&#39;] &gt; 0.00001) &amp; (train_df[&#39;resp_2&#39;] &gt; 0.00001) &amp; (train_df[&#39;resp_3&#39;] &gt; 0.00001 ) &amp; (train_df[&#39;resp_4&#39;] &gt; 0.00001 ) &amp; (train_df[&#39;resp&#39;] &gt; 0.00001 )).astype(&#39;int&#39;) . features = [c for c in train_df.columns if &#39;feature&#39; in c] resp_cols = [&#39;resp&#39;, &#39;resp_1&#39;, &#39;resp_2&#39;, &#39;resp_3&#39;, &#39;resp_4&#39;] . x_train = train_df[features].values y_train = np.stack([(train_df[col] &gt; 0.000001).astype(&#39;int&#39;) for col in resp_cols]).T # Multitarget . f_mean = np.mean(train_df[features[1:]].values, axis=0) . Model . The idea of using an encoder is the denoise the data. After many attempts at using a unsupervised autoencoder, the choice landed on a bottleneck encoder as this will preserve the intra-feature relations. . from sklearn.model_selection import KFold from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples from sklearn.utils.validation import _deprecate_positional_args . # modified code for group gaps; source # https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243 class PurgedGroupTimeSeriesSplit(_BaseKFold): &quot;&quot;&quot;Time Series cross-validator variant with non-overlapping groups. Allows for a gap in groups to avoid potentially leaking info from train into test if the model has windowed or lag features. Provides train/test indices to split time series data samples that are observed at fixed time intervals according to a third-party provided group. In each split, test indices must be higher than before, and thus shuffling in cross validator is inappropriate. This cross-validation object is a variation of :class:`KFold`. In the kth split, it returns first k folds as train set and the (k+1)th fold as test set. The same group will not appear in two different folds (the number of distinct groups has to be at least equal to the number of folds). Note that unlike standard cross-validation methods, successive training sets are supersets of those that come before them. Read more in the :ref:`User Guide &lt;cross_validation&gt;`. Parameters - n_splits : int, default=5 Number of splits. Must be at least 2. max_train_group_size : int, default=Inf Maximum group size for a single training set. group_gap : int, default=None Gap between train and test max_test_group_size : int, default=Inf We discard this number of groups from the end of each train split &quot;&quot;&quot; @_deprecate_positional_args def __init__(self, n_splits=5, *, max_train_group_size=np.inf, max_test_group_size=np.inf, group_gap=None, verbose=False ): super().__init__(n_splits, shuffle=False, random_state=None) self.max_train_group_size = max_train_group_size self.group_gap = group_gap self.max_test_group_size = max_test_group_size self.verbose = verbose def split(self, X, y=None, groups=None): &quot;&quot;&quot;Generate indices to split data into training and test set. Parameters - X : array-like of shape (n_samples, n_features) Training data, where n_samples is the number of samples and n_features is the number of features. y : array-like of shape (n_samples,) Always ignored, exists for compatibility. groups : array-like of shape (n_samples,) Group labels for the samples used while splitting the dataset into train/test set. Yields train : ndarray The training set indices for that split. test : ndarray The testing set indices for that split. &quot;&quot;&quot; if groups is None: raise ValueError( &quot;The &#39;groups&#39; parameter should not be None&quot;) X, y, groups = indexable(X, y, groups) n_samples = _num_samples(X) n_splits = self.n_splits group_gap = self.group_gap max_test_group_size = self.max_test_group_size max_train_group_size = self.max_train_group_size n_folds = n_splits + 1 group_dict = {} u, ind = np.unique(groups, return_index=True) unique_groups = u[np.argsort(ind)] n_samples = _num_samples(X) n_groups = _num_samples(unique_groups) for idx in np.arange(n_samples): if (groups[idx] in group_dict): group_dict[groups[idx]].append(idx) else: group_dict[groups[idx]] = [idx] if n_folds &gt; n_groups: raise ValueError( (&quot;Cannot have number of folds={0} greater than&quot; &quot; the number of groups={1}&quot;).format(n_folds, n_groups)) group_test_size = min(n_groups // n_folds, max_test_group_size) group_test_starts = range(n_groups - n_splits * group_test_size, n_groups, group_test_size) for group_test_start in group_test_starts: train_array = [] test_array = [] group_st = max(0, group_test_start - group_gap - max_train_group_size) for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]: train_array_tmp = group_dict[train_group_idx] train_array = np.sort(np.unique( np.concatenate((train_array, train_array_tmp)), axis=None), axis=None) train_end = train_array.size for test_group_idx in unique_groups[group_test_start: group_test_start + group_test_size]: test_array_tmp = group_dict[test_group_idx] test_array = np.sort(np.unique( np.concatenate((test_array, test_array_tmp)), axis=None), axis=None) test_array = test_array[group_gap:] if self.verbose &gt; 0: pass yield [int(i) for i in train_array], [int(i) for i in test_array] . class CVTuner(kt.engine.tuner.Tuner): def run_trial(self, trial, X, y, splits, batch_size=32, epochs=1,callbacks=None): val_losses = [] for train_indices, test_indices in splits: X_train, X_test = [x[train_indices] for x in X], [x[test_indices] for x in X] y_train, y_test = [a[train_indices] for a in y], [a[test_indices] for a in y] if len(X_train) &lt; 2: X_train = X_train[0] X_test = X_test[0] if len(y_train) &lt; 2: y_train = y_train[0] y_test = y_test[0] model = self.hypermodel.build(trial.hyperparameters) hist = model.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=epochs, batch_size=batch_size, callbacks=callbacks) val_losses.append([hist.history[k][-1] for k in hist.history]) val_losses = np.asarray(val_losses) self.oracle.update_trial(trial.trial_id, {k:np.mean(val_losses[:,i]) for i,k in enumerate(hist.history.keys())}) self.save_model(trial.trial_id, model) . Building the autoencoder . The autoencoder should aid in denoising the data based on this paper. . def build_autoencoder(input_dim, output_dim, noise=.05): inputs = tf.keras.layers.Input(input_dim) encoded = tf.keras.layers.BatchNormalization()(inputs) encoded = tf.keras.layers.GaussianNoise(noise)(encoded) encoded = tf.keras.layers.Dense(640, activation=&#39;relu&#39;)(encoded) decoded = tf.keras.layers.Dropout(0.2)(encoded) decoded = tf.keras.layers.Dense(input_dim, name=&#39;decoded&#39;)(decoded) x = tf.keras.layers.Dense(320, activation=&#39;relu&#39;)(decoded) x = tf.keras.layers.BatchNormalization()(x) x = tf.keras.layers.Dropout(0.2)(x) x = tf.keras.layers.Dense(output_dim, activation=&#39;sigmoid&#39;, name=&#39;label_output&#39;)(x) encoder = tf.keras.models.Model(inputs=inputs, outputs=encoded) autoencoder = tf.keras.models.Model(inputs=inputs, outputs=[decoded,x]) autoencoder.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss={&#39;decoded&#39;:&#39;mse&#39;, &#39;label_output&#39;:&#39;binary_crossentropy&#39;}) return autoencoder, encoder . Building the MLP . def build_model(hp, input_dim, output_dim, encoder): inputs = tf.keras.layers.Input(input_dim) x = encoder(inputs) x = tf.keras.layers.Concatenate()([x,inputs]) #use both raw and encoded features x = tf.keras.layers.BatchNormalization()(x) x = tf.keras.layers.Dropout(hp.Float(&#39;init_dropout&#39;,0.0,0.5))(x) for i in range(hp.Int(&#39;num_layers&#39;,1,5)): x = tf.keras.layers.Dense(hp.Int(&#39;num_units_{i}&#39;,128,256))(x) x = tf.keras.layers.BatchNormalization()(x) x = tf.keras.layers.Lambda(tf.keras.activations.swish)(x) x = tf.keras.layers.Dropout(hp.Float(f&#39;dropout_{i}&#39;,0.0,0.5))(x) x = tf.keras.layers.Dense(output_dim,activation=&#39;sigmoid&#39;)(x) model = tf.keras.models.Model(inputs=inputs, outputs=x) model.compile(optimizer = tf.keras.optimizers.Adam( hp.Float(&#39;lr&#39;,0.00001,0.1,default=0.001)), loss = tf.keras.losses.BinaryCrossentropy( label_smoothing = hp.Float(&#39;label_smoothing&#39;,0.0,0.1)), metrics = [tf.keras.metrics.AUC(name = &#39;auc&#39;)]) return model . Building a model . We add gaussian noise with mean and std from training datea. After training we lock the layersfin the encoder from further training. . Optimizer . Callbacks . callbacks = [ tf.keras.callbacks.EarlyStopping( monitor=&#39;val_loss&#39;, patience=10, restore_best_weights=True, verbose=1) ] . Training . autoencoder,encoder = build_autoencoder( x_train.shape[-1], y_train.shape[-1], noise=.1) . autoencoder.summary() . Model: &#34;model_5&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_3 (InputLayer) [(None, 130)] 0 _________________________________________________________________ batch_normalization_4 (Batch (None, 130) 520 _________________________________________________________________ gaussian_noise_2 (GaussianNo (None, 130) 0 _________________________________________________________________ dense_4 (Dense) (None, 640) 83840 _________________________________________________________________ dropout_4 (Dropout) (None, 640) 0 _________________________________________________________________ decoded (Dense) (None, 130) 83330 _________________________________________________________________ dense_5 (Dense) (None, 320) 41920 _________________________________________________________________ batch_normalization_5 (Batch (None, 320) 1280 _________________________________________________________________ dropout_5 (Dropout) (None, 320) 0 _________________________________________________________________ label_output (Dense) (None, 5) 1605 ================================================================= Total params: 212,495 Trainable params: 211,595 Non-trainable params: 900 _________________________________________________________________ . history = autoencoder.fit( x_train, (x_train, y_train), epochs=1002, batch_size=16384, validation_split=0.1, callbacks=callbacks ) . Epoch 1/1002 87/87 [==============================] - 6s 37ms/step - loss: 3.2765 - decoded_loss: 2.5013 - label_output_loss: 0.7752 - val_loss: 1.1174 - val_decoded_loss: 0.4225 - val_label_output_loss: 0.6949 Epoch 2/1002 87/87 [==============================] - 3s 30ms/step - loss: 1.1679 - decoded_loss: 0.4596 - label_output_loss: 0.7083 - val_loss: 0.8400 - val_decoded_loss: 0.1500 - val_label_output_loss: 0.6899 Epoch 3/1002 87/87 [==============================] - 3s 29ms/step - loss: 1.0435 - decoded_loss: 0.3474 - label_output_loss: 0.6961 - val_loss: 0.7875 - val_decoded_loss: 0.0986 - val_label_output_loss: 0.6889 Epoch 4/1002 87/87 [==============================] - 2s 29ms/step - loss: 0.9867 - decoded_loss: 0.2949 - label_output_loss: 0.6918 - val_loss: 0.7706 - val_decoded_loss: 0.0816 - val_label_output_loss: 0.6890 Epoch 5/1002 87/87 [==============================] - 3s 29ms/step - loss: 1.0088 - decoded_loss: 0.3185 - label_output_loss: 0.6903 - val_loss: 0.7628 - val_decoded_loss: 0.0742 - val_label_output_loss: 0.6886 Epoch 6/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.9435 - decoded_loss: 0.2538 - label_output_loss: 0.6898 - val_loss: 0.7582 - val_decoded_loss: 0.0695 - val_label_output_loss: 0.6887 Epoch 7/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9328 - decoded_loss: 0.2433 - label_output_loss: 0.6895 - val_loss: 0.7504 - val_decoded_loss: 0.0622 - val_label_output_loss: 0.6882 Epoch 8/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9254 - decoded_loss: 0.2361 - label_output_loss: 0.6893 - val_loss: 0.7489 - val_decoded_loss: 0.0604 - val_label_output_loss: 0.6885 Epoch 9/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9405 - decoded_loss: 0.2513 - label_output_loss: 0.6892 - val_loss: 0.7476 - val_decoded_loss: 0.0591 - val_label_output_loss: 0.6884 Epoch 10/1002 87/87 [==============================] - 2s 29ms/step - loss: 0.9278 - decoded_loss: 0.2386 - label_output_loss: 0.6892 - val_loss: 0.7449 - val_decoded_loss: 0.0567 - val_label_output_loss: 0.6882 Epoch 11/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9307 - decoded_loss: 0.2416 - label_output_loss: 0.6891 - val_loss: 0.7427 - val_decoded_loss: 0.0545 - val_label_output_loss: 0.6883 Epoch 12/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9113 - decoded_loss: 0.2225 - label_output_loss: 0.6888 - val_loss: 0.7406 - val_decoded_loss: 0.0520 - val_label_output_loss: 0.6885 Epoch 13/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9149 - decoded_loss: 0.2260 - label_output_loss: 0.6890 - val_loss: 0.7387 - val_decoded_loss: 0.0505 - val_label_output_loss: 0.6882 Epoch 14/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9261 - decoded_loss: 0.2374 - label_output_loss: 0.6887 - val_loss: 0.7384 - val_decoded_loss: 0.0503 - val_label_output_loss: 0.6881 Epoch 15/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9222 - decoded_loss: 0.2335 - label_output_loss: 0.6886 - val_loss: 0.7387 - val_decoded_loss: 0.0503 - val_label_output_loss: 0.6885 Epoch 16/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9061 - decoded_loss: 0.2176 - label_output_loss: 0.6885 - val_loss: 0.7372 - val_decoded_loss: 0.0490 - val_label_output_loss: 0.6882 Epoch 17/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.9178 - decoded_loss: 0.2292 - label_output_loss: 0.6886 - val_loss: 0.7405 - val_decoded_loss: 0.0526 - val_label_output_loss: 0.6879 Epoch 18/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8941 - decoded_loss: 0.2057 - label_output_loss: 0.6884 - val_loss: 0.7379 - val_decoded_loss: 0.0499 - val_label_output_loss: 0.6880 Epoch 19/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9067 - decoded_loss: 0.2183 - label_output_loss: 0.6884 - val_loss: 0.7360 - val_decoded_loss: 0.0482 - val_label_output_loss: 0.6878 Epoch 20/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.9197 - decoded_loss: 0.2313 - label_output_loss: 0.6884 - val_loss: 0.7385 - val_decoded_loss: 0.0505 - val_label_output_loss: 0.6880 Epoch 21/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.9045 - decoded_loss: 0.2163 - label_output_loss: 0.6882 - val_loss: 0.7391 - val_decoded_loss: 0.0511 - val_label_output_loss: 0.6880 Epoch 22/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8970 - decoded_loss: 0.2088 - label_output_loss: 0.6882 - val_loss: 0.7384 - val_decoded_loss: 0.0504 - val_label_output_loss: 0.6880 Epoch 23/1002 87/87 [==============================] - 2s 29ms/step - loss: 0.9360 - decoded_loss: 0.2479 - label_output_loss: 0.6881 - val_loss: 0.7339 - val_decoded_loss: 0.0461 - val_label_output_loss: 0.6878 Epoch 24/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8909 - decoded_loss: 0.2028 - label_output_loss: 0.6881 - val_loss: 0.7393 - val_decoded_loss: 0.0510 - val_label_output_loss: 0.6882 Epoch 25/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8966 - decoded_loss: 0.2086 - label_output_loss: 0.6880 - val_loss: 0.7351 - val_decoded_loss: 0.0471 - val_label_output_loss: 0.6880 Epoch 26/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9049 - decoded_loss: 0.2171 - label_output_loss: 0.6879 - val_loss: 0.7315 - val_decoded_loss: 0.0435 - val_label_output_loss: 0.6880 Epoch 27/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8959 - decoded_loss: 0.2081 - label_output_loss: 0.6878 - val_loss: 0.7348 - val_decoded_loss: 0.0468 - val_label_output_loss: 0.6880 Epoch 28/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8936 - decoded_loss: 0.2058 - label_output_loss: 0.6878 - val_loss: 0.7347 - val_decoded_loss: 0.0468 - val_label_output_loss: 0.6879 Epoch 29/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8993 - decoded_loss: 0.2116 - label_output_loss: 0.6877 - val_loss: 0.7346 - val_decoded_loss: 0.0467 - val_label_output_loss: 0.6878 Epoch 30/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.9043 - decoded_loss: 0.2166 - label_output_loss: 0.6876 - val_loss: 0.7349 - val_decoded_loss: 0.0470 - val_label_output_loss: 0.6878 Epoch 31/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8943 - decoded_loss: 0.2067 - label_output_loss: 0.6877 - val_loss: 0.7352 - val_decoded_loss: 0.0471 - val_label_output_loss: 0.6880 Epoch 32/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.9031 - decoded_loss: 0.2156 - label_output_loss: 0.6875 - val_loss: 0.7362 - val_decoded_loss: 0.0482 - val_label_output_loss: 0.6880 Epoch 33/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.9025 - decoded_loss: 0.2151 - label_output_loss: 0.6874 - val_loss: 0.7322 - val_decoded_loss: 0.0438 - val_label_output_loss: 0.6884 Epoch 34/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8991 - decoded_loss: 0.2117 - label_output_loss: 0.6874 - val_loss: 0.7318 - val_decoded_loss: 0.0439 - val_label_output_loss: 0.6879 Epoch 35/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8955 - decoded_loss: 0.2081 - label_output_loss: 0.6873 - val_loss: 0.7316 - val_decoded_loss: 0.0437 - val_label_output_loss: 0.6879 Epoch 36/1002 87/87 [==============================] - 3s 31ms/step - loss: 0.9228 - decoded_loss: 0.2355 - label_output_loss: 0.6874 - val_loss: 0.7311 - val_decoded_loss: 0.0429 - val_label_output_loss: 0.6883 Epoch 37/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8863 - decoded_loss: 0.1990 - label_output_loss: 0.6873 - val_loss: 0.7334 - val_decoded_loss: 0.0454 - val_label_output_loss: 0.6880 Epoch 38/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8938 - decoded_loss: 0.2067 - label_output_loss: 0.6871 - val_loss: 0.7317 - val_decoded_loss: 0.0436 - val_label_output_loss: 0.6881 Epoch 39/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8955 - decoded_loss: 0.2082 - label_output_loss: 0.6873 - val_loss: 0.7321 - val_decoded_loss: 0.0441 - val_label_output_loss: 0.6880 Epoch 40/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8989 - decoded_loss: 0.2117 - label_output_loss: 0.6872 - val_loss: 0.7318 - val_decoded_loss: 0.0436 - val_label_output_loss: 0.6883 Epoch 41/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8904 - decoded_loss: 0.2033 - label_output_loss: 0.6872 - val_loss: 0.7285 - val_decoded_loss: 0.0404 - val_label_output_loss: 0.6881 Epoch 42/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8889 - decoded_loss: 0.2018 - label_output_loss: 0.6870 - val_loss: 0.7344 - val_decoded_loss: 0.0460 - val_label_output_loss: 0.6884 Epoch 43/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8896 - decoded_loss: 0.2025 - label_output_loss: 0.6871 - val_loss: 0.7299 - val_decoded_loss: 0.0418 - val_label_output_loss: 0.6881 Epoch 44/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.9085 - decoded_loss: 0.2213 - label_output_loss: 0.6871 - val_loss: 0.7323 - val_decoded_loss: 0.0441 - val_label_output_loss: 0.6882 Epoch 45/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8972 - decoded_loss: 0.2104 - label_output_loss: 0.6867 - val_loss: 0.7300 - val_decoded_loss: 0.0421 - val_label_output_loss: 0.6879 Epoch 46/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.9084 - decoded_loss: 0.2216 - label_output_loss: 0.6868 - val_loss: 0.7313 - val_decoded_loss: 0.0433 - val_label_output_loss: 0.6880 Epoch 47/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8876 - decoded_loss: 0.2007 - label_output_loss: 0.6869 - val_loss: 0.7336 - val_decoded_loss: 0.0452 - val_label_output_loss: 0.6885 Epoch 48/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8860 - decoded_loss: 0.1993 - label_output_loss: 0.6867 - val_loss: 0.7272 - val_decoded_loss: 0.0390 - val_label_output_loss: 0.6881 Epoch 49/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8836 - decoded_loss: 0.1968 - label_output_loss: 0.6868 - val_loss: 0.7306 - val_decoded_loss: 0.0423 - val_label_output_loss: 0.6883 Epoch 50/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8836 - decoded_loss: 0.1970 - label_output_loss: 0.6866 - val_loss: 0.7289 - val_decoded_loss: 0.0407 - val_label_output_loss: 0.6882 Epoch 51/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8905 - decoded_loss: 0.2038 - label_output_loss: 0.6866 - val_loss: 0.7317 - val_decoded_loss: 0.0433 - val_label_output_loss: 0.6884 Epoch 52/1002 87/87 [==============================] - 3s 29ms/step - loss: 0.8814 - decoded_loss: 0.1949 - label_output_loss: 0.6865 - val_loss: 0.7290 - val_decoded_loss: 0.0409 - val_label_output_loss: 0.6880 Epoch 53/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8897 - decoded_loss: 0.2029 - label_output_loss: 0.6868 - val_loss: 0.7281 - val_decoded_loss: 0.0399 - val_label_output_loss: 0.6883 Epoch 54/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8829 - decoded_loss: 0.1964 - label_output_loss: 0.6865 - val_loss: 0.7303 - val_decoded_loss: 0.0419 - val_label_output_loss: 0.6883 Epoch 55/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8815 - decoded_loss: 0.1951 - label_output_loss: 0.6864 - val_loss: 0.7294 - val_decoded_loss: 0.0413 - val_label_output_loss: 0.6881 Epoch 56/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8809 - decoded_loss: 0.1945 - label_output_loss: 0.6864 - val_loss: 0.7286 - val_decoded_loss: 0.0406 - val_label_output_loss: 0.6880 Epoch 57/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.8834 - decoded_loss: 0.1970 - label_output_loss: 0.6864 - val_loss: 0.7286 - val_decoded_loss: 0.0405 - val_label_output_loss: 0.6880 Epoch 58/1002 87/87 [==============================] - 3s 30ms/step - loss: 0.9081 - decoded_loss: 0.2218 - label_output_loss: 0.6864 - val_loss: 0.7275 - val_decoded_loss: 0.0396 - val_label_output_loss: 0.6879 Restoring model weights from the end of the best epoch. Epoch 00058: early stopping . encoder.save_weights(&#39;encoder.hdf5&#39;) . encoder.trainable = False . Running CV . Following this notebook which use 5 PurgedGroupTimeSeriesSplit split on the dates in the training data. . We add the locked encoder as the first layer of the MLP. This seems to help in speeding up the submission rather than first predicting using the encoder then using the MLP. . We use a Baysian Optimizer to find the optimal HPs for out model. 20 trials take about 2 hours on GPU. . #https://medium.com/@micwurm/using-tensorflow-lite-to-speed-up-predictions-a3954886eb98 class LiteModel: @classmethod def from_file(cls, model_path): return LiteModel(tf.lite.Interpreter(model_path=model_path)) @classmethod def from_keras_model(cls, kmodel): converter = tf.lite.TFLiteConverter.from_keras_model(kmodel) tflite_model = converter.convert() return LiteModel(tf.lite.Interpreter(model_content=tflite_model)) def __init__(self, interpreter): self.interpreter = interpreter self.interpreter.allocate_tensors() input_det = self.interpreter.get_input_details()[0] output_det = self.interpreter.get_output_details()[0] self.input_index = input_det[&quot;index&quot;] self.output_index = output_det[&quot;index&quot;] self.input_shape = input_det[&quot;shape&quot;] self.output_shape = output_det[&quot;shape&quot;] self.input_dtype = input_det[&quot;dtype&quot;] self.output_dtype = output_det[&quot;dtype&quot;] def predict(self, inp): inp = inp.astype(self.input_dtype) count = inp.shape[0] out = np.zeros((count, self.output_shape[1]), dtype=self.output_dtype) for i in range(count): self.interpreter.set_tensor(self.input_index, inp[i:i+1]) self.interpreter.invoke() out[i] = self.interpreter.get_tensor(self.output_index)[0] return out def predict_single(self, inp): &quot;&quot;&quot; Like predict(), but only for a single record. The input data can be a Python list. &quot;&quot;&quot; inp = np.array([inp], dtype=self.input_dtype) self.interpreter.set_tensor(self.input_index, inp) self.interpreter.invoke() out = self.interpreter.get_tensor(self.output_index) return out[0] . model_fn = lambda hp: build_model( hp, x_train.shape[-1], y_train.shape[-1], encoder) .",
            "url": "https://austinyhc.github.io/blog/time%20series/stock/2020/12/30/jane_street_market_prediction.html",
            "relUrl": "/time%20series/stock/2020/12/30/jane_street_market_prediction.html",
            "date": " • Dec 30, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Continuous Self Motivation",
            "content": "The Four Big Ideas . Habits are the compound interest of self-improvement. | If you want better results, then forget about setting goals. Focus on your system instead. | The most effective way to change your habits is to focus not on what you want to achieve, but on who you wish to become. | The Four Laws of Behavior Change are a simple set of rules we can use to build better habits. They are make it obvious | make it attractive | make it easy | make it satisfying. | . | . I. Customize 1cycle . This learning rate scheduler allows us to easily train a network using Leslie Smith&#39;s 1cycle policy. To learn more about the 1cycle technique for training neural networks check out Leslie Smith&#39;s paper and for more graphical and intuitive explanation checkout out Sylvain Gugger&#39;s post. . To use 1cycle policy we will need an optimum learning rate. We can find this learning rate by using a learning finder which can be called by using lr_finder as fastai does. It will do a mock training by going over a large range of learning rates, then plot them against the losses. We will then pick a value a bit before the minimum, where the loss still improves. Our graph would something like this: . . There is somthing to add, if we are transfer learning, we do not want to start off with too large a learning rate, or we will erase the intelligence of the model already contained in its weights. Instead, we begin with a very small learning rate and increase it gradually before lowering it again to fine-tune the weights. . . Important: After digging into the rabbit hole, I found there are two different learning rate schedule utility in tensorflow, the naming is very confusing, keras.optimizers.schedules.LearningRateSchedule and keras.callbacks.LearningRateScheduler. Although the naming is very similar, they are different in some senses. - The former is subclassing from tf.keras.optimizers while the latter is from tf.keras.Callback . The former schedule the learning rate per iteration while the former is per epoch. | . II. EfficientNet . (Read the EfficientNet paper and summarize in one of the section of this notebook) . EfficientNet, first introduced in Tan and Le, 2019 is among the most efficient models (i.e. requiring least FLOPS for inference) that reaches state-of-the-art accracy on both imagenet and common image classification transfer learning tasks. . The smallest base model is similar to MnasNet, which reached near-SOTA with a significantly smaller model. By introducing a heuristic way to scale the model, EfficientNet provides a family of models (B0 to B7) that represents a good combination of efficiency and accuracy on a variety of scales. Such a scaling heuristics (compound-scaling, details see Tan and Le, 2019) allows the efficiency-oriented base model (B0) to surpass models at every scale, while avoiding extensive grid-search of hyperparameters. . A summary of the latest updates on the model is available at here, where various augmentation schemes and semi-supervised learning approaches are applied to further improve the imagenet performance of the models. These extensions of the model can be used by updating weights without changing model topology . B0 to B7 variats of EfficientNet . Keras implementation of EfficientNet . An implementation of EfficientNet B0 to B7 has been shipped with tf.keras since TF2.3. To use EfficientNetB0 for classifying 1000 classes of images from imagenet, run: . from tensorflow.keras.applications import EfficientNetB0 model = EfficientNetB0(weights=&#39;imagenet&#39;) . The B0 model takes input images of shape (224,224,3), and the input data should range [0,255]. Normailzation is included as part of the model. . Because training EfficientNet on imagenet takes a tremendous amount of resources and several techniques that are not a part of the model architecture itself. Hence the Keras implementations by default loads pre-trained weights obtained via training with AutoAugment. . From B0 to B7 base model, the input shapes are different. Here is a list of input shpae expected for each model: . Base model resolution . EfficientNetB0 | 224 | . EfficientNetB1 | 240 | . EfficientNetB2 | 260 | . EfficientNetB3 | 300 | . EfficientNetB4 | 380 | . EfficientNetB5 | 456 | . EfficientNetB6 | 528 | . EfficientNetB7 | 600 | . When the model is intended for transfer learning, the Keras implementation provides a option to remove the top layers: . model = EfficientNetB0(include_top=False, weights=&#39;imagenet&#39;) . This option excludes the final Dense layer that turns 1280 features on the penultimate layer into prediction of the 1000 ImageNet classes. Replacing the top layer with custom layers allows using EfficientNet as a feature extractor in a transfer learning workflow. . Another argument in the model constructor worth noticing is drop_connect_rate which controls the dropout rate responsible for stochastic depth. This parameter serves as a toggle for extra regularization in finetuning, but does not affect loaded weights. For example, when stronger regularization is desired, try: . model = EfficientNetB0(weights=&#39;imagenet&#39;, drop_connect_rate=0.4) . The default value for drop_connect_rate is 0. . Clarification . AutoAugment . In this article, in section Keras implementation of EfficientNet, it says . Because training EfficientNet on ImageNet takes a tremendous amount of resources and several techniques that are not a part of the model architecture itself. Hence the Keras implementation by default loads pre-trained weights obtained via training with AutoAugment . It means the weights of keras EfficientNet are trained on the pre-trained from AutoAugment. My follow-up question is what dataset does the AutoAugment trained on? .",
            "url": "https://austinyhc.github.io/blog/personal%20development/motivation/habit/2020/12/30/continuous-self-motivation.html",
            "relUrl": "/personal%20development/motivation/habit/2020/12/30/continuous-self-motivation.html",
            "date": " • Dec 30, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Cassava Leaf Disease Classification",
            "content": "%%writefile conditional_cell_extension.py def run_if(line, cell=None): &#39;&#39;&#39;Execute current line/cell if line evaluates to True.&#39;&#39;&#39; if not eval(line): return get_ipython().ex(cell) def load_ipython_extension(shell): &#39;&#39;&#39;Registers the run_if magic when the extension loads.&#39;&#39;&#39; shell.register_magic_function(run_if, &#39;line_cell&#39;) def unload_ipython_extension(shell): &#39;&#39;&#39;Unregisters the run_if magic when the extension unloads.&#39;&#39;&#39; del shell.magics_manager.magics[&#39;cell&#39;][&#39;run_if&#39;] . Writing conditional_cell_extension.py . %reload_ext conditional_cell_extension . Preliminaries . This notebook is a simple training pipeline in TensorFlow for the Cassava Leaf Competition where we are given 21,397 labeled images of cassava leaves classified as 5 different groups (4 diseases and a healthy group) and asked to predict on unseen images of cassava leaves. As with most image classification problems, we can use and experiment with many different forms of augmentation and we can explore transfer learning. . . Note: I am using Dimitre&#8217;s TFRecords that can be found here. He also has 128x128, 256x256, and 384x384 sized images that I added for experimental purposes. Please give his datasets an upvote (and his work in general, it is excellent). . Dependencies . import numpy as np import pandas as pd import seaborn as sns import albumentations as A import matplotlib.pyplot as plt import os, gc, cv2, random, warnings, math, sys, json, pprint, pdb import tensorflow as tf from tensorflow.keras import backend as K import tensorflow_hub as hub from sklearn.model_selection import train_test_split warnings.simplefilter(&#39;ignore&#39;) print(f&quot;Using TensorFlow v{tf.__version__}&quot;) . Using TensorFlow v2.4.0 . Setup . DEVICE = &#39;GPU&#39; #@param [&quot;None&quot;, &quot;&#39;GPU&#39;&quot;, &quot;&#39;TPU&#39;&quot;] {type:&quot;raw&quot;, allow-input: true} if DEVICE == &quot;TPU&quot;: print(&quot;connecting to TPU...&quot;) try: tpu = tf.distribute.cluster_resolver.TPUClusterResolver() print(&#39;Running on TPU &#39;, tpu.master()) except ValueError: print(&quot;Could not connect to TPU&quot;) tpu = None if tpu: try: print(&quot;initializing TPU ...&quot;) tf.config.experimental_connect_to_cluster(tpu) tf.tpu.experimental.initialize_tpu_system(tpu) strategy = tf.distribute.experimental.TPUStrategy(tpu) print(&quot;TPU initialized&quot;) except _: print(&quot;failed to initialize TPU&quot;) else: DEVICE = &quot;GPU&quot; if DEVICE != &quot;TPU&quot;: print(&quot;Using default strategy for CPU and single GPU&quot;) strategy = tf.distribute.get_strategy() if DEVICE == &quot;GPU&quot;: print(&quot;Num GPUs Available: &quot;, len(tf.config.experimental.list_physical_devices(&#39;GPU&#39;))) AUTOTUNE = tf.data.experimental.AUTOTUNE REPLICAS = strategy.num_replicas_in_sync print(f&#39;REPLICAS: {REPLICAS}&#39;) . Using default strategy for CPU and single GPU Num GPUs Available: 0 REPLICAS: 1 . def seed_everything(seed=0): random.seed(seed) np.random.seed(seed) tf.random.set_seed(seed) os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) os.environ[&#39;TF_DETERMINISTIC_OPS&#39;] = &#39;1&#39; GOOGLE = &#39;google.colab&#39; in str(get_ipython()) KAGGLE = not GOOGLE print(&quot;Running on {}!&quot;.format( &quot;Google Colab&quot; if GOOGLE else &quot;Kaggle Kernel&quot; )) . Running on Google Colab! . . Tip: Adding seed helps reproduce results. Setting debug parameter wil run the model on smaller number of epochs to validate the architecture. . #@title ML Liftcycle { run: &quot;auto&quot;, display-mode:&quot;form&quot; } SEED = 16 DEBUG = False #@param {type:&quot;boolean&quot;} TRAIN = True #@param {type:&quot;boolean&quot;} INFERENCE = True #@param {type:&quot;boolean&quot;} seed_everything(SEED) . %%run_if {GOOGLE} from google.colab import drive drive.mount(&#39;/content/gdrive&#39;, force_remount=True) . Mounted at /content/gdrive . project_name = &#39;cassava-leaf-disease-classification&#39; root_path = &#39;/content/gdrive/MyDrive/&#39; if GOOGLE else &#39;/&#39; input_path = f&#39;{root_path}kaggle/input/{project_name}/&#39; working_path = f&#39;{input_path}working/&#39; if GOOGLE else &#39;/kaggle/working/&#39; os.makedirs(working_path, exist_ok=True) os.chdir(working_path) os.listdir(input_path) . [&#39;label_num_to_disease_map.json&#39;, &#39;sample_submission.csv&#39;, &#39;train.csv&#39;, &#39;cassava-leaf-disease-classification.zip&#39;, &#39;test_images&#39;, &#39;test_tfrecords&#39;, &#39;train_images&#39;, &#39;train_tfrecords&#39;, &#39;dump.tfcache.data-00000-of-00001&#39;, &#39;dump.tfcache.index&#39;, &#39;working&#39;] . EDA . df = pd.read_csv(f&#39;{input_path}train.csv&#39;) . df.head() . image_id label . 0 1000015157.jpg | 0 | . 1 1000201771.jpg | 3 | . 2 100042118.jpg | 1 | . 3 1000723321.jpg | 1 | . 4 1000812911.jpg | 3 | . Check how many images are available in the training dataset and also check if each item in the training set are unique . print(f&quot;There are {len(df)} train images&quot;) len(df.image_id) == len(df.image_id.unique()) . There are 21397 train images . True . (df.label.value_counts(normalize=True) * 100).plot.barh(figsize = (8, 5)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f264034a080&gt; . df[&#39;filename&#39;] = df[&#39;image_id&#39;].map(lambda x : f&#39;{input_path}train_images/{x}&#39;) df = df.drop(columns = [&#39;image_id&#39;]) df = df.sample(frac=1).reset_index(drop=True) . df.head() . label filename . 0 3 | /content/gdrive/MyDrive/kaggle/input/cassava-l... | . 1 3 | /content/gdrive/MyDrive/kaggle/input/cassava-l... | . 2 3 | /content/gdrive/MyDrive/kaggle/input/cassava-l... | . 3 3 | /content/gdrive/MyDrive/kaggle/input/cassava-l... | . 4 3 | /content/gdrive/MyDrive/kaggle/input/cassava-l... | . if DEBUG: _, df = train_test_split( df, test_size = 0.1, random_state=SEED, shuffle=True, stratify=df[&#39;label&#39;]) . with open(f&#39;{input_path}label_num_to_disease_map.json&#39;) as file: id2label = json.loads(file.read()) id2label . {&#39;0&#39;: &#39;Cassava Bacterial Blight (CBB)&#39;, &#39;1&#39;: &#39;Cassava Brown Streak Disease (CBSD)&#39;, &#39;2&#39;: &#39;Cassava Green Mottle (CGM)&#39;, &#39;3&#39;: &#39;Cassava Mosaic Disease (CMD)&#39;, &#39;4&#39;: &#39;Healthy&#39;} . In this case, we have 5 labels (4 diseases and healthy): . Cassava Bacterial Blight (CBB) | Cassava Brown Streak Disease (CBSD) | Cassava Green Mottle (CGM) | Cassava Mosaic Disease (CMD) | Healthy | In this case label 3, Cassava Mosaic Disease (CMD) is the most common label. This imbalance may have to be addressed with a weighted loss function or oversampling. I might try this in a future iteration of this kernel or in a new kernel. . Let&#39;s check an example image to see what it looks like . from PIL import Image img = Image.open(df[df.label==3][&#39;filename&#39;].iloc[0]) . width, height = img.size print(f&quot;Width: {width}, Height: {height}&quot;) . Width: 800, Height: 600 . img . EfficientNet . Configuration . BASE_MODEL, IMG_SIZE = (&#39;efficientnet_b3&#39;, 300) #@param [&quot;(&#39;efficientnet_b3&#39;, 300)&quot;, &quot;(&#39;efficientnet_b4&#39;, 380)&quot;, &quot;(&#39;efficientnet_b2&#39;, 260)&quot;] {type:&quot;raw&quot;, allow-input: true} BATCH_SIZE = 32 #@param {type:&quot;integer&quot;} IMG_SIZE = (IMG_SIZE, IMG_SIZE) #@param [&quot;(IMG_SIZE, IMG_SIZE)&quot;, &quot;(512,512)&quot;] {type:&quot;raw&quot;} print(&quot;Using {} with input size {}&quot;.format(BASE_MODEL, IMG_SIZE)) . Using efficientnet_b3 with input size (300, 300) . Loading data . After my quick and rough EDA, let&#39;s load the PIL Image to a Numpy array, so we can move on to data augmentation. . In fastai, they have item_tfms and batch_tfms defined for their data loader API. The item transforms performs a fairly large crop to 224 and also apply other standard augmentations (in aug_tranforms) at the batch level on the GPU. The batch size is set to 32 here. . Splitting . train_df, valid_df = train_test_split( df ,test_size = 0.2 ,random_state = SEED ,shuffle = True ,stratify = df[&#39;label&#39;]) . Constructing Dataset . train_ds = tf.data.Dataset.from_tensor_slices( (train_df.filename.values,train_df.label.values)) valid_ds = tf.data.Dataset.from_tensor_slices( (valid_df.filename.values, valid_df.label.values)) adapt_ds = tf.data.Dataset.from_tensor_slices( train_df.filename.values) . for x,y in valid_ds.take(3): print(x, y) . tf.Tensor(b&#39;/content/gdrive/MyDrive/kaggle/input/cassava-leaf-disease-classification/train_images/2484271873.jpg&#39;, shape=(), dtype=string) tf.Tensor(4, shape=(), dtype=int64) tf.Tensor(b&#39;/content/gdrive/MyDrive/kaggle/input/cassava-leaf-disease-classification/train_images/3704210007.jpg&#39;, shape=(), dtype=string) tf.Tensor(4, shape=(), dtype=int64) tf.Tensor(b&#39;/content/gdrive/MyDrive/kaggle/input/cassava-leaf-disease-classification/train_images/1655615998.jpg&#39;, shape=(), dtype=string) tf.Tensor(2, shape=(), dtype=int64) . . Important: At this point, you may have noticed that I have not used any kind of normalization or rescaling. I recently discovered that there is Normalization layer included in Keras&#8217; pretrained EfficientNet, as mentioned here. . Item transformation . Basically item transformations mainly make sure the input data is of the same size so that it can be collated in batches. . def decode_image(filename): img = tf.io.read_file(filename) img = tf.image.decode_jpeg(img, channels=3) return img def collate_train(filename, label): img = decode_image(filename) img = tf.image.random_brightness(img, 0.3) img = tf.image.random_flip_left_right(img, seed=None) img = tf.image.random_crop(img, size=[*IMG_SIZE, 3]) return img, label def process_adapt(filename): img = decode_image(filename) img = tf.keras.layers.experimental.preprocessing.Rescaling(1.0 / 255)(img) return img def collate_valid(filename, label): img = decode_image(filename) img = tf.image.resize(img, [*IMG_SIZE]) return img, label . train_ds = train_ds.map(collate_train, num_parallel_calls=AUTOTUNE) valid_ds = valid_ds.map(collate_valid, num_parallel_calls=AUTOTUNE) adapt_ds = adapt_ds.map(process_adapt, num_parallel_calls=AUTOTUNE) . def show_images(ds): _,axs = plt.subplots(4,6,figsize=(24,16)) for ((x, y), ax) in zip(ds.take(24), axs.flatten()): ax.imshow(x.numpy().astype(np.uint8)) ax.set_title(np.argmax(y)) ax.axis(&#39;off&#39;) . show_images(train_ds) . show_images(valid_ds) . Batching Dataset . . Note: I was shuffing the validation set which is a bug . train_ds_batch = (train_ds .cache(&#39;dump.tfcache&#39;) .shuffle(buffer_size=1000) .batch(BATCH_SIZE) .prefetch(buffer_size=AUTOTUNE)) valid_ds_batch = (valid_ds #.shuffle(buffer_size=1000) .batch(BATCH_SIZE*2) .prefetch(buffer_size=AUTOTUNE)) adapt_ds_batch = (adapt_ds .shuffle(buffer_size=1000) .batch(BATCH_SIZE) .prefetch(buffer_size=AUTOTUNE)) . Batch augmentation . data_augmentation = tf.keras.Sequential( [ tf.keras.layers.experimental.preprocessing.RandomCrop(*IMG_SIZE), tf.keras.layers.experimental.preprocessing.RandomFlip(&quot;horizontal_and_vertical&quot;), tf.keras.layers.experimental.preprocessing.RandomRotation(0.25), tf.keras.layers.experimental.preprocessing.RandomZoom((-0.2, 0)), tf.keras.layers.experimental.preprocessing.RandomContrast((0.2,0.2)) ] ) . func = lambda x,y: (data_augmentation(x), y) x = (train_ds .batch(BATCH_SIZE) .take(1) .map(func, num_parallel_calls=AUTOTUNE)) . show_images(x.unbatch()) . Building a model . I am using an EfficientNetB3 on top of which I add some output layers to predict our 5 disease classes. I decided to load the imagenet pretrained weights locally to keep the internet off (part of the requirements to submit a kernal to this competition). . from tensorflow.keras.applications import EfficientNetB3 . efficientnet = EfficientNetB3( weights = &#39;imagenet&#39; if TRAIN else None, include_top = False, input_shape = (*IMG_SIZE, 3), pooling=&#39;avg&#39;) . def build_model(base_model, num_class): inputs = tf.keras.layers.Input(shape=(*IMG_SIZE, 3)) x = data_augmentation(inputs) x = base_model(x) x = tf.keras.layers.Dropout(0.4)(x) outputs = tf.keras.layers.Dense(num_class, activation=&quot;softmax&quot;, name=&quot;pred&quot;)(x) model = tf.keras.models.Model(inputs=inputs, outputs=outputs) return model . model = build_model(base_model=efficientnet, num_class=len(id2label)) . model.summary() . Model: &#34;model_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_5 (InputLayer) [(None, 300, 300, 3)] 0 _________________________________________________________________ sequential_2 (Sequential) (None, 300, 300, 3) 0 _________________________________________________________________ efficientnetb3 (Functional) (None, 1536) 10783535 _________________________________________________________________ dropout_1 (Dropout) (None, 1536) 0 _________________________________________________________________ pred (Dense) (None, 5) 7685 ================================================================= Total params: 10,791,220 Trainable params: 10,703,917 Non-trainable params: 87,303 _________________________________________________________________ . Fine tune . The 3rd layer of the Efficient is the Normalization layer, which can be tuned to our new dataset instead of imagenet. Be patient on this one, it does take a bit of time as we&#39;re going through the entire training set. . %%time if TRAIN: if not os.path.exists(f&quot;{working_path}000_normalization.h5&quot;): model.get_layer(&#39;efficientnetb3&#39;).get_layer(&#39;normalization&#39;).adapt(adapt_ds_batch) model.save_weights(&quot;000_normalization.h5&quot;) else: model.load_weights(&quot;000_normalization.h5&quot;) . CPU times: user 4 µs, sys: 0 ns, total: 4 µs Wall time: 6.68 µs . Optimizer . CosineDecay . . Important: I always wanted to try the new CosineDecayRestarts function implemented in tf.keras as it seemed promising and I struggled to find the right settings (if there were any) for the ReduceLROnPlateau . EPOCHS = 8 STEPS = int(round(len(train_df)/BATCH_SIZE)) * EPOCHS schedule = tf.keras.experimental.CosineDecayRestarts( initial_learning_rate=1e-4, first_decay_steps=300 ) . schedule.get_config() . {&#39;alpha&#39;: 0.0, &#39;first_decay_steps&#39;: 300, &#39;initial_learning_rate&#39;: 0.0001, &#39;m_mul&#39;: 1.0, &#39;name&#39;: None, &#39;t_mul&#39;: 2.0} . x = [i for i in range(STEPS)] y = [schedule(s) for s in range(STEPS)] plt.plot(x, y) . [&lt;matplotlib.lines.Line2D at 0x7f264156fc18&gt;] . . Warning: There is a gap between what I had expected and the acutal LearningRateScheduler that tensorflow gives us. The LearningRateScheduler update the lr on_epoch_begin while it makes more sense to do it on_batch_end or on_batch_begin. . Callbacks . callbacks = [ tf.keras.callbacks.ModelCheckpoint( filepath=&#39;001_best_model.h5&#39;, monitor=&#39;val_loss&#39;, save_best_only=True), ] model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(schedule), metrics=[&quot;accuracy&quot;]) . Training . if TRAIN: history = model.fit(train_ds_batch, epochs = EPOCHS, validation_data=valid_ds_batch, callbacks=callbacks) . Epoch 1/8 12/54 [=====&gt;........................] - ETA: 21:15 - loss: 0.4106 - accuracy: 0.8684 . Evaluating . def plot_hist(hist): plt.plot(history.history[&#39;loss&#39;]) plt.plot(history.history[&#39;val_loss&#39;]) plt.title(&#39;Loss over epochs&#39;) plt.ylabel(&#39;loss&#39;) plt.xlabel(&#39;epoch&#39;) plt.legend([&#39;train&#39;, &#39;valid&#39;], loc=&#39;best&#39;) plt.show() . if TRAIN: plot_hist(history) . We load the best weight that were kept from the training phase. Just to check how our model is performing, we will attempt predictions over the validation set. This can help to highlight any classes that will be consistently miscategorised. . model.load_weights(&#39;001_best_model.h5&#39;) . Prediction . x = train_df.sample(1).filename.values[0] img = decode_image(x) . %%time imgs = [tf.image.random_crop(img, size=[*IMG_SIZE, 3]) for _ in range(4)] _,axs = plt.subplots(1,4,figsize=(16,4)) for (x, ax) in zip(imgs, axs.flatten()): ax.imshow(x.numpy().astype(np.uint8)) ax.axis(&#39;off&#39;) . CPU times: user 57.3 ms, sys: 870 µs, total: 58.2 ms Wall time: 62.1 ms . I apply some very basic test time augmentation to every local image extracted from the original 600-by-800 images. We know we can do some fancy augmentation with albumentations but I wanted to do that exclusively with Keras preprocessing layers to keep the cleanest pipeline possible. . tta = tf.keras.Sequential( [ tf.keras.layers.experimental.preprocessing.RandomCrop((*IMG_SIZE)), tf.keras.layers.experimental.preprocessing.RandomFlip(&quot;horizontal_and_vertical&quot;), tf.keras.layers.experimental.preprocessing.RandomZoom((-0.2, 0.2)), tf.keras.layers.experimental.preprocessing.RandomContrast((0.2,0.2)) ] ) . def predict_tta(filename, num_tta=4): img = decode_image(filename) img = tf.expand_dims(img, 0) imgs = tf.concat([tta(img) for _ in range(num_tta)], 0) preds = model.predict(imgs) return preds.sum(0).argmax() . pred = predict_tta(df.sample(1).filename.values[0]) print(pred) . 3 . if INFERENCE: from tqdm import tqdm preds = [] with tqdm(total=len(valid_df)) as pbar: for filename in valid_df.filename: pbar.update() preds.append(predict_tta(filename, num_tta=4)) . 100%|██████████| 4280/4280 [25:34&lt;00:00, 2.79it/s] . if INFERENCE: cm = tf.math.confusion_matrix(valid_df.label.values, np.array(preds)) plt.figure(figsize=(10, 8)) sns.heatmap(cm, xticklabels=id2label.values(), yticklabels=id2label.values(), annot=True, fmt=&#39;g&#39;, cmap=&quot;Blues&quot;) plt.xlabel(&#39;Prediction&#39;) plt.ylabel(&#39;Label&#39;) plt.show() . test_folder = input_path + &#39;/test_images/&#39; submission_df = pd.DataFrame(columns={&quot;image_id&quot;,&quot;label&quot;}) submission_df[&quot;image_id&quot;] = os.listdir(test_folder) submission_df[&quot;label&quot;] = 0 . submission_df[&#39;label&#39;] = (submission_df[&#39;image_id&#39;] .map(lambda x : predict_tta(test_folder+x))) . submission_df . image_id label . 0 2216849948.jpg | 4 | . submission_df.to_csv(&quot;submission.csv&quot;, index=False) . 1% Better Everyday . reference . https://www.kaggle.com/c/cassava-leaf-disease-classification | https://www.kaggle.com/dimitreoliveira/cassava-leaf-disease-training-with-tpu-v2-pods/notebook#Training-data-samples-(with-augmentation) | https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/#keras-implementation-of-efficientnet | https://www.tensorflow.org/guide/gpu_performance_analysis | https://www.tensorflow.org/guide/data_performance#prefetching | https://www.tensorflow.org/guide/data_performance_analysis | . . todos . See if I can integrate the Cutmix/Mixup augmentations in the appendix into our existing notebook. This is an excellent example | Still want to figure out some intuition of item aug and batch aug. I don&#39;t know, maybe there is some limitation or how to do so to help to speed up. | Learn more about the adapt function that being used to retrain the normalization layer of the EfficientNetB3. | . . done . Predict in batch to speed up | Add a cell for checkbox parameter to select between kaggle and colab, default is Kaggle. | Try out the data_generator and the data_frame_iterator | Removing normalizaiton step in generator since in EfficientNet, normalization is done within the model itself and the model expects input in the range of [0,255] | Find out the intuition and the difference between item_tfm and batch_tfm . In fastai, item_tfm defines the transforms that are done on the CPU and batch_tfm defines those done on the GPU. . | Customize my own data generator as fastai creates their Dataloader . No need, things are much easier than what I was originally expecting. Please refer to the Loading data section in this notebook. . | The 3rd layer of the Efficientnet is the Normalization layer, which can be tuned to our new dataset instead of imagenet. Be patient on this one, it does take a bit of time we&#39;re going through the entire training set. . | Add seed_everything function | . Appendix . The albumentation is primarily used for resizing and normalization. . def albu_transforms_train(data_resize): return A.Compose([ A.ToFloat(), A.Resize(data_resize, data_resize), ], p=1.) # For Validation def albu_transforms_valid(data_resize): return A.Compose([ A.ToFloat(), A.Resize(data_resize, data_resize), ], p=1.) . def CutMix(image, label, DIM, PROBABILITY = 1.0): # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3] # output - a batch of images with cutmix applied CLASSES = 5 imgs = []; labs = [] for j in range(len(image)): # DO CUTMIX WITH PROBABILITY DEFINED ABOVE P = tf.cast( tf.random.uniform([],0,1)&lt;=PROBABILITY, tf.int32) # CHOOSE RANDOM IMAGE TO CUTMIX WITH k = tf.cast( tf.random.uniform([],0,len(image)),tf.int32) # CHOOSE RANDOM LOCATION x = tf.cast( tf.random.uniform([],0,DIM),tf.int32) y = tf.cast( tf.random.uniform([],0,DIM),tf.int32) b = tf.random.uniform([],0,1) # this is beta dist with alpha=1.0 WIDTH = tf.cast( DIM * tf.math.sqrt(1-b),tf.int32) * P ya = tf.math.maximum(0,y-WIDTH//2) yb = tf.math.minimum(DIM,y+WIDTH//2) xa = tf.math.maximum(0,x-WIDTH//2) xb = tf.math.minimum(DIM,x+WIDTH//2) # MAKE CUTMIX IMAGE one = image[j,ya:yb,0:xa,:] two = image[k,ya:yb,xa:xb,:] three = image[j,ya:yb,xb:DIM,:] middle = tf.concat([one,two,three],axis=1) img = tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM,:,:]],axis=0) imgs.append(img) # MAKE CUTMIX LABEL a = tf.cast(WIDTH*WIDTH/DIM/DIM,tf.float32) labs.append((1-a)*label[j] + a*label[k]) # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?) image2 = tf.reshape(tf.stack(imgs),(len(image),DIM,DIM,3)) label2 = tf.reshape(tf.stack(labs),(len(image),CLASSES)) return image2,label2 . def MixUp(image, label, DIM, PROBABILITY = 1.0): # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3] # output - a batch of images with mixup applied CLASSES = 5 imgs = []; labs = [] for j in range(len(image)): # DO MIXUP WITH PROBABILITY DEFINED ABOVE P = tf.cast( tf.random.uniform([],0,1)&lt;=PROBABILITY, tf.float32) # CHOOSE RANDOM k = tf.cast( tf.random.uniform([],0,len(image)),tf.int32) a = tf.random.uniform([],0,1)*P # this is beta dist with alpha=1.0 # MAKE MIXUP IMAGE img1 = image[j,] img2 = image[k,] imgs.append((1-a)*img1 + a*img2) # MAKE CUTMIX LABEL labs.append((1-a)*label[j] + a*label[k]) # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?) image2 = tf.reshape(tf.stack(imgs),(len(image),DIM,DIM,3)) label2 = tf.reshape(tf.stack(labs),(len(image),CLASSES)) return image2,label2 . .",
            "url": "https://austinyhc.github.io/blog/plant/disease/classification/efficientnet/2020/12/30/cassava-leaf-disease-classification.html",
            "relUrl": "/plant/disease/classification/efficientnet/2020/12/30/cassava-leaf-disease-classification.html",
            "date": " • Dec 30, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am Austin; An Embedded System Engineer by profession and passion with extensive experience in the areas of Deep Learning, Diagnostic Modeling, Predictive Modeling, Digital Signal/Image Processing by using Python and C++. . As an enthusiastic learner and practioner that involves making things intelligently work, I pursue my interest in real-life application and cutting-edge researches. Hope to meet a ton of you in the sphere of machine learning and to contribute as best as I can to your community. .",
          "url": "https://austinyhc.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://austinyhc.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}