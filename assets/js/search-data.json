{
  
    
        "post0": {
            "title": "Cassava Leaf Disease Classification - EDA",
            "content": "In this competition, we are trying to identify common diseases of cassava crops using data science and machine learning. Previous methods of disease detection require farmers to solicit the help of government-funded agricultural experts to visually inspect and diagnose the plants. This suffers from being labor-intensive, low-supply and costly. Instead, it would be preferred if an automated pipeline based on mobile-quality photos of the cassava leafs could be developed. . This competition provides a farmer-crowdsourced dataset, labeled by experts at the National Crops Resources Research Institute (NaCRRI). . In this kernel, I will present a quick EDA. . import numpy as np import pandas as pd import seaborn as sns import albumentations as A import matplotlib.pyplot as plt from pathlib import Path Path.ls = lambda x : list(x.iterdir()) import os, gc, cv2, random, warnings, math, sys, json, pprint, pdb import tensorflow as tf from tensorflow.keras import backend as K os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;3&#39; warnings.simplefilter(&#39;ignore&#39;) . # helper function to plot sample def plot_imgs(dataset_show, iters): rcParams[&#39;figure.figsize&#39;] = 20,10 for i in range(iters): f, ax = plt.subplots(1,5) for p in range(5): idx = np.random.randint(0, len(dataset_show)) img, label = dataset_show[idx] ax[p].grid(False) ax[p].imshow(img[0]) ax[p].set_title(idx) plt.show() def visualize(path, n_images, is_random=True, figsize=(16, 16)): plt.figure(figsize=figsize) w = int(n_images ** .5) h = math.ceil(n_images / w) image_names = path.ls() for i in range(n_images): image_name = random.choice(image_names) if is_random else image_name[i]; img = cv2.imread(str(image_name)) plt.subplot(h, w, i + 1) plt.imshow(img) plt.xticks([]) plt.yticks([]) plt.show() . from google.colab import drive drive.mount(&#39;/content/gdrive&#39;, force_remount=True) . Mounted at /content/gdrive . dataset_path = Path(&#39;/content/gdrive/MyDrive/1_AUSTIN CHEN/Data Scientist/Datasets/cassava-leaf-disease-classification&#39;) dataset_path.ls() . [PosixPath(&#39;/content/gdrive/MyDrive/1_AUSTIN CHEN/Data Scientist/Datasets/cassava-leaf-disease-classification/label_num_to_disease_map.json&#39;), PosixPath(&#39;/content/gdrive/MyDrive/1_AUSTIN CHEN/Data Scientist/Datasets/cassava-leaf-disease-classification/sample_submission.csv&#39;), PosixPath(&#39;/content/gdrive/MyDrive/1_AUSTIN CHEN/Data Scientist/Datasets/cassava-leaf-disease-classification/train.csv&#39;), PosixPath(&#39;/content/gdrive/MyDrive/1_AUSTIN CHEN/Data Scientist/Datasets/cassava-leaf-disease-classification/cassava-leaf-disease-classification.zip&#39;), PosixPath(&#39;/content/gdrive/MyDrive/1_AUSTIN CHEN/Data Scientist/Datasets/cassava-leaf-disease-classification/test_images&#39;), PosixPath(&#39;/content/gdrive/MyDrive/1_AUSTIN CHEN/Data Scientist/Datasets/cassava-leaf-disease-classification/test_tfrecords&#39;), PosixPath(&#39;/content/gdrive/MyDrive/1_AUSTIN CHEN/Data Scientist/Datasets/cassava-leaf-disease-classification/train_images&#39;), PosixPath(&#39;/content/gdrive/MyDrive/1_AUSTIN CHEN/Data Scientist/Datasets/cassava-leaf-disease-classification/train_tfrecords&#39;)] . train_df = pd.read_csv(dataset_path/&#39;train.csv&#39;) . train_df.head() . image_id label . 0 1000015157.jpg | 0 | . 1 1000201771.jpg | 3 | . 2 100042118.jpg | 1 | . 3 1000723321.jpg | 1 | . 4 1000812911.jpg | 3 | . train_df[&#39;path&#39;] = train_df[&#39;image_id&#39;].map(lambda x : dataset_path/&#39;train_images&#39;/x) train_df = train_df.drop(columns = [&#39;image_id&#39;]) train_df = train_df.sample(frac=1).reset_index(drop=True) . train_df.head() . label path . 0 3 | /content/gdrive/MyDrive/1_AUSTIN CHEN/Data Sci... | . 1 3 | /content/gdrive/MyDrive/1_AUSTIN CHEN/Data Sci... | . 2 0 | /content/gdrive/MyDrive/1_AUSTIN CHEN/Data Sci... | . 3 2 | /content/gdrive/MyDrive/1_AUSTIN CHEN/Data Sci... | . 4 3 | /content/gdrive/MyDrive/1_AUSTIN CHEN/Data Sci... | . Check how many images are available in the training dataset and also check if each item in the training set are unique . print(f&quot;There are {len(train_df)} train images&quot;) len(train_df.image_id) == len(train_df.image_id.unique()) . There are 21397 train images . True . train_df[&#39;label&#39;].hist(figsize = (10, 5)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f6867bfc9e8&gt; . with open(dataset_path/&#39;label_num_to_disease_map.json&#39;) as file: id2label = json.loads(file.read()) id2label . {&#39;0&#39;: &#39;Cassava Bacterial Blight (CBB)&#39;, &#39;1&#39;: &#39;Cassava Brown Streak Disease (CBSD)&#39;, &#39;2&#39;: &#39;Cassava Green Mottle (CGM)&#39;, &#39;3&#39;: &#39;Cassava Mosaic Disease (CMD)&#39;, &#39;4&#39;: &#39;Healthy&#39;} . In this case, we have 5 labels (4 diseases and healthy): . Cassava Bacterial Blight (CBB) | Cassava Brown Streak Disease (CBSD) | Cassava Green Mottle (CGM) | Cassava Mosaic Disease (CMD) | Healthy | In this case label 3, Cassava Mosaic Disease (CMD) is the most common label. This imbalance may have to be addressed with a weighted loss function or oversampling. I might try this in a future iteration of this kernel or in a new kernel. . Let&#39;s check an example image to see what it looks like . from PIL import Image im = Image.open(train_df[train_df.label==3][&#39;path&#39;][0]) . width, height = im.size print(width, height) . 800 600 . im . Data loading . After my quick and rough EDA, let&#39;s load the PIL Image to a Numpy array, so we can move on to data augmentation. . In fastai, they have item_tfms and batch_tfms defined for their data loader API. The item transforms performs a fairly large crop to 224 and also apply other standard augmentations (in aug_tranforms) at the batch level on the GPU. The batch size is set to 32 here. . #batch_tfms = [*aug_transforms(size=224, max_warp=0), Normalize.from_stats(*imagenet_stats)] . IMG_SIZE = 224 . from tensorflow.keras import layers data_augmentation = tf.keras.Sequential([ layers.experimental.preprocessing.RandomFlip(&quot;horizontal_and_vertical&quot;), layers.experimental.preprocessing.RandomRotation(0.2) ]) resize_and_rescale = tf.keras.Sequential([ layers.experimental.preprocessing.Resizing(IMG_SIZE, IMG_SIZE), layers.experimental.preprocessing.Rescaling(1./255) ]) . im_array = tf.keras.preprocessing.image.img_to_array(im) im_array = resize_and_rescale(im_array) . im_array = tf.expand_dims(im_array, 0) . plt.figure(figsize=(10,10)) for i in range(9): augmented = data_augmentation(im_array) ax = plt.subplot(3, 3, i + 1) plt.imshow(augmented[0]) plt.axis(&quot;off&quot;) . Next 1% Improvement . Find out the intuition and the difference between item_tfm and batch_tfm | Customize my own data generator as fastai creates their Dataloader | Fix the TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment error and figure why there is a difference between Kaggle and Colab. | from skimage.io import imread from skimage.transform import resize import numpy as np import math # Here, `x_set` is list of path to the images # and `y_set` are the associated classes. class CIFAR10Sequence(Sequence): def __init__(self, x_set, y_set, batch_size): self.x, self.y = x_set, y_set self.batch_size = batch_size def __len__(self): return math.ceil(len(self.x) / self.batch_size) def __getitem__(self, idx): batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size] batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size] return np.array([ resize(imread(file_name), (200, 200)) for file_name in batch_x]), np.array(batch_y) . class CassavaGenerator(tf.keras.utils.Sequence): def __init__(self, path, data, bs, dim, shuffle=True, tfm=None, use_mixup=False, use_cutmix=False, use_fmix=False): self.path = path self.data = data self.bs = bs self.dim = dim self.shuffle = shuffle self.augment = tfm self.use_mixup = use_mixup self.use_cutmix = use_cutmix self.use_fmix = use_fmix self.list_ids = self.data.index.values self.label = pd.get_dummies(self.data[&#39;label&#39;], columns=[&#39;label&#39;]) self.on_epoch_end() def __len__(self): return int(np.ceil(float(len(self.data)) / float(self.bs))) def __getitem__(self, index): start = index * self.bs end = (index+1) * self.bs batch_ids = self.indices[start:end] xs = np.empty((self.bs, *self.dim)) ys = np.empty((self.bs, 5), dtype=np.float32) for i,k in enumerate(batch_ids): pdb.set_trace() imamge = tf.keras.preprocessing.image.load_img( train_df[train_df.label==3][&#39;path&#39;][0], grayscale = False, color_mode = &#39;rgb&#39;, target_size = None, interpolation = &quot;nearest&quot; ) augmented = self.augment(image=image) image = augmented[&#39;image&#39;] xs[i, :, :, :] = image ys[i, :] = self.label.loc[k, :].values if self.use_cutmix: xs, ys = CutMix(xs, ys, self.dim[0]) if self.use_mixup: xs, ys = MixUp(xs, ys, self.dim[0]) if self.use_fmix: xs, ys = FMix(xs, ys, self.dim[0]) return xs, ys def on_epoch_end(self): self.indices = np.arange(len(self.list_ids)) if self.shuffle: np.random.shuffle(self.indices) . dg = CassavaGenerator(BaseConfig.TRAIN_IMG_PATH, df, 20, (128,128,3), shuffle=True, use_mixup=False, use_cutmix=True, use_fmix=False, tfm=albu_transforms_train(128)) . visualize(BaseConfig.TRAIN_IMG_PATH, 16, is_random=True) . Augmentation . The albumentation is primarily used for resizing and normalization. . def albu_transforms_train(data_resize): return A.Compose([ A.ToFloat(), A.Resize(data_resize, data_resize), ], p=1.) # For Validation def albu_transforms_valid(data_resize): return A.Compose([ A.ToFloat(), A.Resize(data_resize, data_resize), ], p=1.) . def CutMix(image, label, DIM, PROBABILITY = 1.0): # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3] # output - a batch of images with cutmix applied CLASSES = 5 imgs = []; labs = [] for j in range(len(image)): # DO CUTMIX WITH PROBABILITY DEFINED ABOVE P = tf.cast( tf.random.uniform([],0,1)&lt;=PROBABILITY, tf.int32) # CHOOSE RANDOM IMAGE TO CUTMIX WITH k = tf.cast( tf.random.uniform([],0,len(image)),tf.int32) # CHOOSE RANDOM LOCATION x = tf.cast( tf.random.uniform([],0,DIM),tf.int32) y = tf.cast( tf.random.uniform([],0,DIM),tf.int32) b = tf.random.uniform([],0,1) # this is beta dist with alpha=1.0 WIDTH = tf.cast( DIM * tf.math.sqrt(1-b),tf.int32) * P ya = tf.math.maximum(0,y-WIDTH//2) yb = tf.math.minimum(DIM,y+WIDTH//2) xa = tf.math.maximum(0,x-WIDTH//2) xb = tf.math.minimum(DIM,x+WIDTH//2) # MAKE CUTMIX IMAGE one = image[j,ya:yb,0:xa,:] two = image[k,ya:yb,xa:xb,:] three = image[j,ya:yb,xb:DIM,:] middle = tf.concat([one,two,three],axis=1) img = tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM,:,:]],axis=0) imgs.append(img) # MAKE CUTMIX LABEL a = tf.cast(WIDTH*WIDTH/DIM/DIM,tf.float32) labs.append((1-a)*label[j] + a*label[k]) # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?) image2 = tf.reshape(tf.stack(imgs),(len(image),DIM,DIM,3)) label2 = tf.reshape(tf.stack(labs),(len(image),CLASSES)) return image2,label2 . def MixUp(image, label, DIM, PROBABILITY = 1.0): # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3] # output - a batch of images with mixup applied CLASSES = 5 imgs = []; labs = [] for j in range(len(image)): # DO MIXUP WITH PROBABILITY DEFINED ABOVE P = tf.cast( tf.random.uniform([],0,1)&lt;=PROBABILITY, tf.float32) # CHOOSE RANDOM k = tf.cast( tf.random.uniform([],0,len(image)),tf.int32) a = tf.random.uniform([],0,1)*P # this is beta dist with alpha=1.0 # MAKE MIXUP IMAGE img1 = image[j,] img2 = image[k,] imgs.append((1-a)*img1 + a*img2) # MAKE CUTMIX LABEL labs.append((1-a)*label[j] + a*label[k]) # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?) image2 = tf.reshape(tf.stack(imgs),(len(image),DIM,DIM,3)) label2 = tf.reshape(tf.stack(labs),(len(image),CLASSES)) return image2,label2 . Custom Data Generator . def plot_imgs(dataset_show, iters): rcParams[&#39;figure.figsize&#39;] = 20,10 for i in range(iters): f, ax = plt.subplots(1,5) for p in range(5): idx = np.random.randint(0, len(dataset_show)) img, label = dataset_show[idx] ax[p].grid(False) ax[p].imshow(img[0]) ax[p].set_title(idx) plt.show() . dg = CassavaGenerator(BaseConfig.TRAIN_IMG_PATH, df, 20, (128,128,3), shuffle=True, use_mixup=False, use_cutmix=True, use_fmix=False, tfm=albu_transforms_train(128)) . dg[0] . &gt; &lt;ipython-input-105-ccce2c0ab35a&gt;(30)__getitem__() -&gt; image = cv2.imread(f&#34;{self.path}/{self.data[&#39;image_id&#39;][k]}&#34;) (Pdb) n &gt; &lt;ipython-input-105-ccce2c0ab35a&gt;(31)__getitem__() -&gt; image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) (Pdb) n &gt; &lt;ipython-input-105-ccce2c0ab35a&gt;(33)__getitem__() -&gt; augmented = self.augment(image=image) (Pdb) n &gt; &lt;ipython-input-105-ccce2c0ab35a&gt;(34)__getitem__() -&gt; image = augmented[&#39;image&#39;] (Pdb) xs array([[[[ 5.01746356e-001, 5.53707123e-001, 5.50765991e-001], [ 5.09558856e-001, 5.87990165e-001, 6.24509871e-001], [ 5.41038632e-001, 5.84175885e-001, 6.06571734e-001], ..., [ 2.22824752e-001, 2.99019605e-001, 3.82812507e-002], [ 2.46476710e-001, 3.19148272e-001, 1.17953438e-002], [ 2.37362146e-001, 2.87117034e-001, 1.76011026e-002]], [[ 5.01225531e-001, 5.45511723e-001, 5.32123208e-001], [ 5.21721840e-001, 5.91789246e-001, 6.02328479e-001], [ 5.21093786e-001, 5.99019647e-001, 6.12882972e-001], ..., [ 3.32306981e-001, 4.34742659e-001, 5.05361520e-002], [ 2.57337630e-001, 3.57460201e-001, 0.00000000e+000], [ 2.60523915e-001, 3.40104163e-001, 5.66482879e-002]], [[ 4.38802123e-001, 4.81081516e-001, 4.52006757e-001], [ 4.51470554e-001, 5.14675319e-001, 4.83563125e-001], [ 4.99800891e-001, 6.11320436e-001, 6.15150154e-001], ..., [ 4.78967547e-001, 5.61197937e-001, 1.57628674e-002], [ 3.66069257e-001, 4.40487146e-001, 1.89950988e-002], [ 4.30361509e-001, 4.82077241e-001, 2.90211380e-001]], ..., [[ 1.53186277e-001, 1.97135419e-001, 7.19362795e-002], [ 1.26455277e-001, 2.35324770e-001, 5.87162971e-002], [ 1.36136651e-001, 2.98743874e-001, 7.80484080e-002], ..., [ 1.52588859e-001, 2.52251834e-001, 0.00000000e+000], [ 1.23912379e-001, 2.34083951e-001, 4.90196107e-004], [ 6.15962073e-002, 1.70251235e-001, 0.00000000e+000]], [[ 6.45220652e-002, 6.45373762e-002, 1.63143389e-002], [ 1.34344369e-001, 1.90058216e-001, 3.44362743e-002], [ 1.54105410e-001, 2.64414817e-001, 7.87530690e-002], ..., [ 1.06510416e-001, 1.92356005e-001, 1.65594369e-002], [ 7.28860348e-002, 1.80867046e-001, 1.64368879e-002], [ 7.45251253e-002, 1.63403809e-001, 2.22273301e-002]], [[ 1.40670955e-001, 8.90778229e-002, 5.42585813e-002], [ 1.68949142e-001, 1.69194251e-001, 5.01991436e-002], [ 1.37101725e-001, 2.18673408e-001, 5.71844392e-002], ..., [ 6.57781884e-002, 1.57444850e-001, 0.00000000e+000], [ 4.24479172e-002, 1.52481616e-001, 0.00000000e+000], [ 4.51439992e-002, 1.37392774e-001, 9.95710841e-004]]], [[[ 5.26220659e-292, -2.15660153e+164, -6.20775819e+178], [-2.61772572e+046, 3.74343403e+012, -3.41299277e+124], [ 1.59995689e-128, 6.48085657e-019, -1.47646713e-248], ..., [-1.35323366e-143, 6.40515392e-116, 2.63365254e-077], [ 3.83535745e-129, -2.73621173e+007, 1.33863755e-149], [ 1.91939534e+211, -8.72390899e-051, -8.12082566e-027]], [[ 5.46877619e+195, -2.70620781e-161, -1.95489085e+203], [-3.14615517e+140, 1.72747434e-168, 3.46326610e-236], [ 7.71731948e+006, -5.40750208e-264, 4.66853486e-083], ..., [ 1.30249669e+060, 1.86327307e-155, -1.74035470e+041], [-2.60430103e+186, -7.32410625e+268, 6.73462288e-018], [-4.07790764e-066, 7.10038466e+010, -2.78674177e-280]], [[ 3.25491950e+275, 1.02933137e+043, 2.48897018e-017], [ 1.67307855e-221, -3.18189097e-199, 2.18215853e-179], [ 3.73749679e-084, 2.16514868e-197, -1.01216889e+221], ..., [ 1.29491232e-022, -1.02287237e+116, 1.36024124e-093], [-1.45729763e+218, 9.41383507e+069, 5.02917467e-261], [ 3.04792497e+114, -1.78579684e-040, -4.27432214e+104]], ..., [[ 1.67399259e+305, 1.81882267e+230, 7.89040386e+247], [ 1.23784377e-129, 1.60162530e+099, 3.82220974e+241], [-2.40136228e-142, 1.59212342e-246, 4.10305863e+097], ..., [-2.44611872e-196, 1.70753244e+245, 8.37321744e-203], [ 3.05113499e-126, -2.31438184e-225, 4.02963237e+025], [ 1.57317472e-299, -1.16259890e-253, -3.93822837e-052]], [[ 5.95877800e-124, 1.47789331e-074, 2.78249670e+301], [ 1.34504248e-167, -8.24564641e+196, -3.68887403e-179], [-5.97609606e-010, 1.72293309e+099, -3.52443620e-239], ..., [-1.31664806e-103, -2.83918399e-023, 3.41812325e-016], [-1.09716490e+158, 3.05118223e+295, -2.87082694e+105], [-1.13748872e+173, -9.97753070e+263, 2.06834714e-055]], [[-4.53254370e-166, 2.33524863e-153, -7.17106249e-066], [-8.67688036e+033, -8.94964547e-132, 8.06097301e+140], [ 1.33859771e-106, 4.48117220e-258, 6.99679719e-168], ..., [ 7.81739354e-219, -6.98151595e+002, 4.63289924e+067], [ 5.09214970e-197, 4.85214024e+061, 6.90358156e-261], [-1.31297300e-047, -1.21151835e-207, -2.86941821e+110]]], [[[ 5.01746356e-001, 5.53707123e-001, 5.50765991e-001], [ 5.09558856e-001, 5.87990165e-001, 6.24509871e-001], [ 5.41038632e-001, 5.84175885e-001, 6.06571734e-001], ..., [ 2.22824752e-001, 2.99019605e-001, 3.82812507e-002], [ 2.46476710e-001, 3.19148272e-001, 1.17953438e-002], [ 2.37362146e-001, 2.87117034e-001, 1.76011026e-002]], [[ 5.01225531e-001, 5.45511723e-001, 5.32123208e-001], [ 5.21721840e-001, 5.91789246e-001, 6.02328479e-001], [ 5.21093786e-001, 5.99019647e-001, 6.12882972e-001], ..., [ 3.32306981e-001, 4.34742659e-001, 5.05361520e-002], [ 2.57337630e-001, 3.57460201e-001, 0.00000000e+000], [ 2.60523915e-001, 3.40104163e-001, 5.66482879e-002]], [[ 4.38802123e-001, 4.81081516e-001, 4.52006757e-001], [ 4.51470554e-001, 5.14675319e-001, 4.83563125e-001], [ 4.99800891e-001, 6.11320436e-001, 6.15150154e-001], ..., [ 4.78967547e-001, 5.61197937e-001, 1.57628674e-002], [ 3.66069257e-001, 4.40487146e-001, 1.89950988e-002], [ 4.30361509e-001, 4.82077241e-001, 2.90211380e-001]], ..., [[ 1.53186277e-001, 1.97135419e-001, 7.19362795e-002], [ 1.26455277e-001, 2.35324770e-001, 5.87162971e-002], [ 1.36136651e-001, 2.98743874e-001, 7.80484080e-002], ..., [ 1.52588859e-001, 2.52251834e-001, 0.00000000e+000], [ 1.23912379e-001, 2.34083951e-001, 4.90196107e-004], [ 6.15962073e-002, 1.70251235e-001, 0.00000000e+000]], [[ 6.45220652e-002, 6.45373762e-002, 1.63143389e-002], [ 1.34344369e-001, 1.90058216e-001, 3.44362743e-002], [ 1.54105410e-001, 2.64414817e-001, 7.87530690e-002], ..., [ 1.06510416e-001, 1.92356005e-001, 1.65594369e-002], [ 7.28860348e-002, 1.80867046e-001, 1.64368879e-002], [ 7.45251253e-002, 1.63403809e-001, 2.22273301e-002]], [[ 1.40670955e-001, 8.90778229e-002, 5.42585813e-002], [ 1.68949142e-001, 1.69194251e-001, 5.01991436e-002], [ 1.37101725e-001, 2.18673408e-001, 5.71844392e-002], ..., [ 6.57781884e-002, 1.57444850e-001, 0.00000000e+000], [ 4.24479172e-002, 1.52481616e-001, 0.00000000e+000], [ 4.51439992e-002, 1.37392774e-001, 9.95710841e-004]]], ..., [[[ 2.84527587e-037, 3.20243726e+031, 3.77524626e+112], [ 3.08724968e+083, 5.53715740e+121, 1.87021530e+252], [ 6.00794778e+015, 1.96920025e+276, 6.64437457e+251], ..., [ 1.38597611e+233, 8.27121318e-071, 6.68403617e+281], [ 1.56848297e+194, 1.34493334e+036, 2.17171801e+011], [ 4.98035118e+203, 9.62954151e+175, 1.99847759e+045]], [[ 3.93555876e-071, 9.02360627e+064, 8.99259642e+218], [ 4.14640954e+079, 3.17335288e+103, 7.25420569e+039], [ 2.03696837e+103, 5.86655423e+069, 1.71762707e+021], ..., [ 8.59907819e-081, 1.31685463e+060, 4.74102439e+237], [ 1.12303225e+113, 2.65684507e+126, 6.63239365e+092], [ 1.11419747e+165, 1.00376039e+123, 2.02855286e-066]], [[ 9.28275940e+199, 4.35171732e+016, 1.32144261e+272], [ 4.09144096e-037, 8.74400301e+256, 4.94198202e-056], [ 4.70844314e-061, 1.89001072e+122, 9.47512048e+188], ..., [ 1.82228238e+185, 4.41125436e+074, 4.04399190e+021], [ 6.15369737e-081, 4.91178061e-100, 6.68391283e+281], [ 2.29393537e+049, 2.54657731e+238, 3.38465875e+107]], ..., [[ 1.76761951e+054, 5.98396168e+021, 1.31908695e-047], [ 3.09158908e+045, 9.48894592e+097, 9.12975844e+232], [ 2.55433381e-052, 1.81162911e-042, 3.87519461e+055], ..., [ 4.34703828e+242, 5.65954127e+122, 1.63553892e+069], [ 1.22820705e+252, 1.47776166e+276, 2.67026624e-081], [ 9.74285320e+020, 2.30396518e+122, 4.60251543e+209]], [[ 5.94201188e+107, 8.65297552e+020, 1.27626879e-080], [ 5.76533416e+227, 2.37380143e+103, 2.02102891e-037], [ 1.45724291e-047, 4.09574431e+184, 6.26230883e+054], ..., [ 1.08202535e+123, 6.15425463e+170, 2.55861356e+247], [ 1.75866136e+036, 2.12547565e+165, 6.66886181e+194], [ 3.69941215e+127, 7.35395041e+030, 1.29350272e+031]], [[ 2.20678747e+175, 2.69322086e+165, 3.91883293e+247], [ 2.65756041e+059, 1.34387591e+277, 2.09795978e+011], [ 3.82615529e+011, 8.84439345e+179, 1.40202844e+218], ..., [ 1.32040311e+060, 1.09430293e+248, 2.07637057e+031], [ 7.12947517e+074, 1.37758627e+219, 6.37727781e+275], [ 3.27540521e-032, 1.32776415e+007, 3.22574396e+223]]], [[[ 2.27370008e-075, 3.01118960e+209, 2.98449376e+261], [ 1.69664095e-051, 4.32233349e+020, 3.81768942e+098], [ 4.82887338e-071, 4.14795272e-061, 2.11434289e-046], ..., [ 8.27893573e+040, 1.25849661e+200, 1.51211826e+233], [ 3.39435215e+267, 1.46626323e+266, 2.35878049e+223], [ 1.08736971e+103, 3.00011778e-080, 1.59658590e+257]], [[ 1.35550957e+209, 1.53055872e+083, 1.07582330e+006], [ 2.06914956e+219, 1.90328615e+185, 2.22133833e+112], [ 4.43013876e+092, 5.65645156e-081, 3.24861026e+035], ..., [ 8.99593264e+034, 2.65045598e+247, 3.92934633e+280], [ 2.15550833e-080, 1.12952078e-042, 3.22465780e+227], [ 7.69481588e+218, 4.36170448e+011, 8.81293680e+281]], [[ 2.72938569e-056, 4.09167592e+247, 6.22370078e+045], [ 9.95549555e+116, 3.56470484e+209, 9.89309191e+111], [ 1.27744585e+118, 1.53791235e+007, 3.92305056e+213], ..., [ 1.16657075e-032, 1.52595111e+059, 3.51758576e+233], [ 1.05141192e+281, 3.66648719e+204, 1.15932270e+112], [ 1.67566879e+026, 6.33717119e+040, 4.72743827e+050]], ..., [[-2.57770249e+163, -5.71713766e-259, 1.04107113e+016], [ 7.74574490e-022, -4.75468414e-148, 1.02437340e+180], [ 1.35154165e+177, -2.73320479e+258, 1.75752642e+185], ..., [ 3.14286873e+137, -2.28058726e+186, 2.80658682e-305], [ 4.40263920e+238, 3.83318773e+289, 3.36776177e-211], [ 3.63112719e-216, 6.40683087e-145, -3.02366339e-244]], [[-9.65245424e+023, -5.31791996e+016, 3.26261243e-181], [ 3.54189435e+239, -8.57289824e-198, 7.92063938e-187], [ 9.01889916e+123, -5.53113615e-274, 1.29794304e+258], ..., [-1.68462601e-074, -9.56971804e-080, -2.93494394e-218], [-2.49047453e+042, -1.14405500e-161, 4.03416385e+297], [-9.75326728e+027, -3.82810978e-007, -1.69668053e-261]], [[-1.48272620e+018, -1.00222232e-195, -1.43786192e-223], [-5.02716836e+143, -1.70403995e-118, 1.16896961e+080], [ 2.63548122e+238, 2.13567729e-053, 4.46512835e-278], ..., [-1.18727851e+173, -3.66520678e+172, -2.77246392e-189], [-2.77611524e+095, 4.96487781e+300, 5.43716235e-042], [ 1.17779397e+267, -3.44593455e-205, 3.06979283e+263]]], [[[-2.37137426e+003, -1.08684475e-045, -1.49183531e-203], [-2.97767322e-002, -7.02385247e-220, 3.59899146e+273], [-1.73861221e-074, -3.94260694e+012, 1.57270658e-069], ..., [-1.35424284e-140, -6.96410438e+142, 1.74379530e-307], [ 8.19951581e-158, -1.21087213e-245, 7.86851223e+233], [-2.42371265e+260, -5.42264051e-269, 2.00289065e-038]], [[ 2.96713673e-090, 1.87101593e+063, 5.99434912e+102], [-1.11093475e+077, -3.03983389e+287, 8.71643729e-100], [-6.50337996e-291, -7.49271325e-158, 9.80152071e-033], ..., [-4.17740423e-083, -3.92128491e+295, 2.24957211e+110], [-2.02828083e+076, -4.42514750e+045, 3.96581732e-162], [-3.48449007e-015, -1.47283751e+214, 2.71565130e+292]], [[-4.54037537e+115, -1.95900024e-139, 1.11560837e+129], [-4.19257665e+192, -8.81883891e+276, 1.94477583e+264], [-6.55360285e+110, -3.07059180e-159, 1.89972419e-011], ..., [-1.15670414e+076, -1.56773308e-176, -6.77134831e-282], [-7.96058466e-186, -1.60327840e-205, 2.27637258e+114], [-6.78608627e-302, -6.95323090e-278, 3.60515896e-220]], ..., [[-7.11428508e+032, -1.68911093e+027, 2.40720755e+143], [ 9.11751012e+243, -1.07429512e-113, 2.04769000e+276], [ 4.76474330e+099, -9.99790847e-003, -3.71849745e-113], ..., [ 9.75452101e+266, 5.80212462e-029, 3.52166523e-157], [-2.63689688e-084, -7.97632681e-191, 5.27141767e-272], [ 1.10939084e+180, -2.43541235e+006, 1.32453040e-073]], [[ 1.77499326e-052, 6.57771736e+266, 4.38837171e-240], [ 4.41566863e+007, 5.69056411e+202, 1.55984439e-288], [ 1.72241880e-070, -9.01820205e-299, 2.09916633e-288], ..., [-2.70435531e+298, -4.43516554e+157, -3.18572348e-058], [-5.81425147e+066, -1.21041925e-104, 2.69809534e+240], [-3.87580169e-011, -6.52346496e-221, -1.00409662e-001]], [[-1.50185200e+308, -8.51407166e+176, -7.28359978e-295], [-3.85779943e+264, -9.26348427e+002, 7.20193014e-046], [-6.42393653e+085, -2.91628850e+070, -1.14996975e-169], ..., [ 2.84273305e+194, 4.46448099e+092, 4.92634538e-138], [ 2.05748524e+112, 1.71280162e-038, 2.30824709e-196], [ 2.75160962e+083, 1.69238762e-067, 4.68803921e-143]]]]) (Pdb) quit . BdbQuit Traceback (most recent call last) &lt;ipython-input-110-89a6c6208e75&gt; in &lt;module&gt;() -&gt; 1 dg[0] &lt;ipython-input-105-ccce2c0ab35a&gt; in __getitem__(self, index) 32 33 augmented = self.augment(image=image) &gt; 34 image = augmented[&#39;image&#39;] 35 36 xs[i, :, :, :] = image &lt;ipython-input-105-ccce2c0ab35a&gt; in __getitem__(self, index) 32 33 augmented = self.augment(image=image) &gt; 34 image = augmented[&#39;image&#39;] 35 36 xs[i, :, :, :] = image /usr/lib/python3.6/bdb.py in trace_dispatch(self, frame, event, arg) 49 return # None 50 if event == &#39;line&#39;: &gt; 51 return self.dispatch_line(frame) 52 if event == &#39;call&#39;: 53 return self.dispatch_call(frame, arg) /usr/lib/python3.6/bdb.py in dispatch_line(self, frame) 68 if self.stop_here(frame) or self.break_here(frame): 69 self.user_line(frame) &gt; 70 if self.quitting: raise BdbQuit 71 return self.trace_dispatch 72 BdbQuit: . Moving to the next 1% improvement . Fix the TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment error and figure why there is a difference between Kaggle and Colab. |",
            "url": "https://austinyhc.github.io/blog/plant/disease/classification/eda/2020/12/10/Cassava_Leaf_Disease_Classification_EDA.html",
            "relUrl": "/plant/disease/classification/eda/2020/12/10/Cassava_Leaf_Disease_Classification_EDA.html",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Cassava Leaf Disease Classification",
            "content": "This is an introductory notebook contains creating a baseline model using Tensor Processing Units (TPUs) and begins making submissions to the Cassava Leaf Disease Classification competition. . TPUs with TensorFlow . We&#39;ll be using TensorFlow and Keras to build our computer vision model, and using TPUs to both train our model and make predictions. . References . This notebook was built using the following amazing resources created by Kagglers: . Learn With Me: Getting Started with Tensor Processing Units (TPUs) | Martin Gorner: Getting Started With 100 Flowers on TPU | Amy Jang: TensorFlow + Transfer Learning: Melanoma | Phil Culliton: [A Simple TF 2.1 Notebook](https://www.kaggle.com/philculliton/a-simple-tf-2-1-notebook | . Dependencies . import math, os, re, warnings, random import numpy as np import pandas as pd import seaborn as sns from functools import partial from matplotlib import pyplot as plt from sklearn.utils import class_weight from sklearn.model_selection import KFold, train_test_split from sklearn.metrics import classification_report, confusion_matrix, accuracy_score from tensorflow import keras import tensorflow as tf #import efficientnet.tfkeras as efn def seed_everything(seed=0): random.seed(seed) np.random.seed(seed) tf.random.set_seed(seed) os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) os.environ[&#39;TF_DETERMINISTIC_OPS&#39;] = &#39;1&#39; seed = 0 seed_everything(seed) warnings.filterwarnings(&#39;ignore&#39;) . Hardware configuration . Download data from Kaggle . Set up variables . AUTOTUNE = tf.data.experimental.AUTOTUNE GCS_PATH = &#39;gs://kds-041bbb000630fa3aaebc67ffddc6dd4b536981c56338d1734b8511fe&#39; BATCH_SIZE = 16 * strategy.num_replicas_in_sync IMAGE_SIZE = [512, 512] CLASSES = [&#39;0&#39;, &#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;] EPOCHS = 25 . Load data . The data we&#39;re working with have been formatted into TFRecords, which are a format for storing a sequence of binary records. TFRecords work really well with TPUs, and allow us to send a small number of large files across the TPU for processing. . If you&#39;d like to learn more about TFRecords and maybe even try creating them yourself, check out this TFRecords Basics notebook and corresponding video from Kaggle Data Scientist Ryan Holbrook. . Because our data consists of training and test images only, we&#39;re going to split our training data into training and validation data using the train_test_split() function. . Decode the data . In the code chunk below we&#39;ll set up a series of functions that allow us to convert our images into tensors so that we can utilize them in our model. We&#39;ll also normalize our data. Our images are using a &quot;Red, Blue, Green(RGB)&quot; scale that has a range of [0,255], and by normalizing it we&#39;ll set each pixel&#39;s value to a number in the range of [0, 1]. . def decode_image(image): image = tf.image.decode_jpeg(image, channels=3) image = tf.cast(image, tf.float32) / 255.0 image = tf.reshape(image, [*IMAGE_SIZE, 3]) return image . def read_tfrecord(example, labeled): tfrecord_format = { &quot;image&quot;: tf.io.FixedLenFeature([], tf.string), &quot;target&quot;: tf.io.FixedLenFeature([], tf.int64) } if labeled else { &quot;image&quot;: tf.io.FixedLenFeature([], tf.string), &quot;image_name&quot;: tf.io.FixedLenFeature([], tf.string) } example = tf.io.parse_single_example(example, tfrecord_format) image = decode_image(example[&#39;image&#39;]) if labeled: label = tf.cast(example[&#39;target&#39;], tf.int32) return image, label idnum = example[&#39;image_name&#39;] return image, idnum . We&#39;ll use the following function to load our dataset. One of the advantage of a TPU is that we can run multiple files across the TPU at once, and this accounts for the speed advantages of using a TPU. To capitalize on that, we want to make sure that we&#39;re using data as soon as it streams in, rather than creating a data streaming bottleneck. . def load_dataset(filenames, labeled=True, ordered=False): ignore_order = tf.data.Options() if not ordered: ignore_order.experimental_deterministic = False # disable order, increase speed dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) # automatically interleaves reads from multiple files dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE) return dataset . A note of using train_test_split() . While i used train_test_split() to create both a training and validation dataset, consider exploring cross validation instead. . TRAINING_FILENAMES, VALID_FILENAMES = train_test_split( tf.io.gfile.glob(GCS_PATH + &#39;/train_tfrecords/ld_train*.tfrec&#39;), test_size=0.35, random_state=5 ) TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + &#39;/test_tfrecords/ld_test*.tfrec&#39;) . Adding in augmentations . def data_augment(image, label): # Thanks to the dataset.prefetch(AUTO) statement in the following function this happens essentially for free on TPU. # Data pipeline code is executed on the &quot;CPU&quot; part of the TPU while the TPU itself is computing gradients. image = tf.image.random_flip_left_right(image) return image, label . Define data loading methods . The following functions will be used to load our training, validation, and test datasets, as well as print out the number of images in each dataset. . def get_training_dataset(): dataset = load_dataset(TRAINING_FILENAMES, labeled=True) dataset = dataset.map(data_augment, num_parallel_calls=AUTOTUNE) dataset = dataset.repeat() dataset = dataset.shuffle(2048) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.prefetch(AUTOTUNE) return dataset . def get_validation_dataset(ordered=False): dataset = load_dataset(VALID_FILENAMES, labeled=True, ordered=ordered) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.cache() dataset = dataset.prefetch(AUTOTUNE) return dataset . def get_test_dataset(ordered=False): dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.prefetch(AUTOTUNE) return dataset . def count_data_items(filenames): n = [int(re.compile(r&quot;-([0-9]*) .&quot;).search(filename).group(1)) for filename in filenames] return np.sum(n) . NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES) NUM_VALIDATION_IMAGES = count_data_items(VALID_FILENAMES) NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES) print(&#39;Dataset: {} training images, {} validation images, {} (unlabeled) test images&#39;.format( NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES)) . Dataset: 13380 training images, 8017 validation images, 1 (unlabeled) test images . Brief EDA . print(&quot;Training data shapes:&quot;) for image, label in get_training_dataset().take(3): print(image.numpy().shape, label.numpy().shape) print(&quot;Training data label examples:&quot;, label.numpy()) print(&quot;Validation data shapes:&quot;) for image, label in get_validation_dataset().take(3): print(image.numpy().shape, label.numpy().shape) print(&quot;Validation data label examples:&quot;, label.numpy()) print(&quot;Test data shapes:&quot;) for image, idnum in get_test_dataset().take(3): print(image.numpy().shape, idnum.numpy().shape) print(&quot;Test data IDs:&quot;, idnum.numpy().astype(&#39;U&#39;)) # U=unicode string . Training data shapes: (128, 512, 512, 3) (128,) (128, 512, 512, 3) (128,) (128, 512, 512, 3) (128,) Training data label examples: [3 3 3 0 1 1 3 3 0 3 2 1 3 1 4 4 3 3 3 3 3 3 3 3 3 3 3 3 4 3 3 3 1 2 3 3 3 2 3 3 3 3 3 1 3 4 3 3 3 3 3 0 2 2 3 3 0 0 3 3 3 3 4 3 3 3 3 1 0 3 3 1 3 4 2 3 3 4 3 1 2 3 3 4 3 2 3 1 3 3 3 2 1 2 3 3 4 2 3 3 3 3 3 3 2 3 1 2 4 1 3 3 3 4 3 2 3 2 2 1 3 3 3 1 3 3 3 3] Validation data shapes: (128, 512, 512, 3) (128,) (128, 512, 512, 3) (128,) (128, 512, 512, 3) (128,) Validation data label examples: [3 3 3 1 3 3 3 3 4 0 4 3 0 2 4 3 3 3 3 1 3 3 2 3 3 3 1 2 3 3 1 3 0 3 3 1 3 4 3 3 3 3 4 4 2 3 2 2 3 3 2 3 3 1 1 3 4 3 4 3 4 3 3 3 3 3 2 1 0 4 3 3 3 3 3 0 0 2 3 3 3 2 3 3 1 3 3 3 3 3 4 3 0 3 3 2 1 3 3 3 4 4 4 3 4 3 3 3 2 4 1 3 4 3 4 3 2 0 1 3 3 2 2 3 3 2 3 0] Test data shapes: (1, 512, 512, 3) (1,) Test data IDs: [&#39;2216849948.jpg&#39;] . np.set_printoptions(threshold=15, linewidth=80) def batch_to_numpy_images_and_labels(data): images, labels = data numpy_images = images.numpy() numpy_labels = labels.numpy() if numpy_labels.dtype == object: # binary string in this case, these are image ID strings numpy_labels = [None for _ in enumerate(numpy_images)] # If no labels, only image IDs, return None for labels (this is the case for test data) return numpy_images, numpy_labels def title_from_label_and_target(label, correct_label): if correct_label is None: return CLASSES[label], True correct = (label == correct_label) return &quot;{} [{}{}{}]&quot;.format(CLASSES[label], &#39;OK&#39; if correct else &#39;NO&#39;, u&quot; u2192&quot; if not correct else &#39;&#39;, CLASSES[correct_label] if not correct else &#39;&#39;), correct def display_one_plant(image, title, subplot, red=False, titlesize=16): plt.subplot(*subplot) plt.axis(&#39;off&#39;) plt.imshow(image) if len(title) &gt; 0: plt.title(title, fontsize=int(titlesize) if not red else int(titlesize/1.2), color=&#39;red&#39; if red else &#39;black&#39;, fontdict={&#39;verticalalignment&#39;:&#39;center&#39;}, pad=int(titlesize/1.5)) return (subplot[0], subplot[1], subplot[2]+1) def display_batch_of_images(databatch, predictions=None): &quot;&quot;&quot;This will work with: display_batch_of_images(images) display_batch_of_images(images, predictions) display_batch_of_images((images, labels)) display_batch_of_images((images, labels), predictions) &quot;&quot;&quot; # data images, labels = batch_to_numpy_images_and_labels(databatch) if labels is None: labels = [None for _ in enumerate(images)] # auto-squaring: this will drop data that does not fit into square or square-ish rectangle rows = int(math.sqrt(len(images))) cols = len(images)//rows # size and spacing FIGSIZE = 13.0 SPACING = 0.1 subplot=(rows,cols,1) if rows &lt; cols: plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows)) else: plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE)) # display for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])): title = &#39;&#39; if label is None else CLASSES[label] correct = True if predictions is not None: title, correct = title_from_label_and_target(predictions[i], label) dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images subplot = display_one_plant(image, title, subplot, not correct, titlesize=dynamic_titlesize) #layout plt.tight_layout() if label is None and predictions is None: plt.subplots_adjust(wspace=0, hspace=0) else: plt.subplots_adjust(wspace=SPACING, hspace=SPACING) plt.show() . training_dataset = get_training_dataset() training_dataset = training_dataset.unbatch().batch(20) train_batch = iter(training_dataset) . display_batch_of_images(next(train_batch)) . validation_dataset = get_validation_dataset() validation_dataset = validation_dataset.unbatch().batch(20) valid_batch = iter(validation_dataset) . display_batch_of_images(next(valid_batch)) . testing_dataset = get_test_dataset() testing_dataset = testing_dataset.unbatch().batch(20) test_batch = iter(testing_dataset) . display_batch_of_images(next(test_batch)) . Building the model . Learning rate schedule . We learned about learning rates in the Intro to Deep Learning: Stochastic Gradient Descent lesson, and here I&#39;ve created a learning rate schedule mostly using the defaults in the Keras Exponential Decay Learning Rate Scheduler documentation (I did change the initial_learning_rate. You can adjust the learning rate scheduler below, and read more about the other types of schedulers available to you in the Keras learning rate schedules API. . print(&quot;Tensorflow version &quot; + tf.__version__) . Tensorflow version 2.3.0 . lr_scheduler = keras.optimizers.schedules.ExponentialDecay( initial_learning_rate=1e-5, decay_steps=10000, decay_rate=0.9) . Building our model . In order to ensure that our model is trained on the TPU, we build it using with strategy.scope(). . This model was built using transfer learning, meaning that we have a pre-trained model (ResNet50) as our base model and then the customizable model built using tf.keras.Sequential. If you&#39;re new to transfer learning I recommend setting base_model.trainable to False, but do encourage you to change which base model you&#39;re using (more options are available in the tf.keras.applications Module documentation) as well iterate on the custom model. . Note that we&#39;re using sparse_categorical_crossentropy as our loss function, because we did not one-hot encode our labels. . with strategy.scope(): img_adjust_layer = tf.keras.layers.Lambda(tf.keras.applications.resnet50.preprocess_input, input_shape=[*IMAGE_SIZE, 3]) base_model = tf.keras.applications.ResNet50(weights=&#39;imagenet&#39;, include_top=False) base_model.trainable = False model = tf.keras.Sequential([ tf.keras.layers.BatchNormalization(renorm=True), img_adjust_layer, base_model, tf.keras.layers.GlobalAveragePooling2D(), tf.keras.layers.Dense(8, activation=&#39;relu&#39;), #tf.keras.layers.BatchNormalization(renorm=True), tf.keras.layers.Dense(len(CLASSES), activation=&#39;softmax&#39;) ]) model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler, epsilon=0.001), loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;sparse_categorical_accuracy&#39;]) . Train the model . As our model is training you&#39;ll see a printout for each epoch, and can also monitor TPU usage by clicking on the TPU metrics in the toolbar at the top right of your notebook. . train_dataset = get_training_dataset() valid_dataset = get_validation_dataset() . STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE VALID_STEPS = NUM_VALIDATION_IMAGES // BATCH_SIZE history = model.fit(train_dataset, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS, validation_data=valid_dataset, validation_steps=VALID_STEPS) . With model.summary() we&#39;ll see a printout of each of our layers, their corresponding shape, as well as the associated number of parameters. Notice that at the bottom of the printout we&#39;ll see information on the total parameters, trainable parameters, and non-trainable parameters. Because we&#39;re using a pre-trained model, we expect there to be a large number of non-trainable parameters (because the weights have already been assigned in the pre-trained model). . model.summary() . Evaluating our model . The first chunk of code is provided to show you where the variables in the second chunk of code came from. As you can see, there&#39;s a lot of room for improvement in this model, but because we&#39;re using TPUs and have a relatively short training time, we&#39;re able to iterate on our model fairly rapidly. . print(history.history.keys()) . history_frame = pd.DataFrame(history.history) history_frame.loc[:, [&#39;loss&#39;, &#39;val_loss&#39;]].plot() history_frame.loc[:, [&#39;sparse_categorical_accuracy&#39;, &#39;val_sparse_categorical_accuracy&#39;]].plot(); . Making predictions . Now that we&#39;ve trained our model we can use it to make predictions. . def to_float32(image, label): return tf.cast(image, tf.float32), label . test_ds = get_test_dataset(ordered=True) test_ds = test_ds.map(to_float32) print(&#39;Computing predictions...&#39;) test_images_ds = testing_dataset test_images_ds = test_ds.map(lambda image, idnum: image) probabilities = model.predict(test_images_ds) predictions = np.argmax(probabilities, axis=-1) print(predictions) . Creating a submission file . Now that we&#39;ve trained a model and made predictions we&#39;re ready to submit to the competition! You can run the following code below to get your submission file. . print(&#39;Generating submission.csv file...&#39;) test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch() test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype(&#39;U&#39;) # all in one batch np.savetxt(&#39;submission.csv&#39;, np.rec.fromarrays([test_ids, predictions]), fmt=[&#39;%s&#39;, &#39;%d&#39;], delimiter=&#39;,&#39;, header=&#39;id,label&#39;, comments=&#39;&#39;) !head submission.csv . Be aware that because this is a code competition with a hidden test set, internet and TPUs cannot be enabled on your submission notebook. Therefore TPUs will only be available for training models. For a walk-through on how to train on TPUs and run inference/submit on GPUs, see our TPU Docs. .",
            "url": "https://austinyhc.github.io/blog/jupyter/2020/12/10/Cassava_Leaf_Disease_Classification.html",
            "relUrl": "/jupyter/2020/12/10/Cassava_Leaf_Disease_Classification.html",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://austinyhc.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://austinyhc.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am Austin; after years plunging into Smartphone Industry and IC Design, I have found data science is crucial in every aspect possible. Thanks to Jeremy Howard, the founder of Fast.ai, he re-enlightens my passion for Machine Learning, especially Deep Learning and Transfer Learning. I have freelanced with different organizations and continuously strive to apply DL to any project, team, or goal. Hope to meet a ton of you in the sphere of AI and to contribute as best as I can to your community. .",
          "url": "https://austinyhc.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://austinyhc.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}