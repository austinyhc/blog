{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jane_street_market_prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ-F1VMOcltK"
      },
      "source": [
        "# Jane Street Market Prediction \n",
        "> Buy low, sell high. It sounds so easyâ€¦.\n",
        "\n",
        "- toc: true\n",
        "- badges: true\n",
        "- author: Austin Chen\n",
        "- categories: [time series,stock,kaggle]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmRjB3nsetpw"
      },
      "source": [
        "# 1% Better Everyday\r\n",
        "\r\n",
        "- I see that autoencoders are widely being used in this competition. However, I am having some doubts on wether the autoencoded features really help the model. As shown in the notebook, the compressed features concatenated back into the original data as extra featues. For me personally, I've tried 2~3 different random seed and the variation on the results are large. It makes me wonder that the autoencoder is only useful on certain features, so dropping these from the autoencoder might be a good idea.\r\n",
        "\r\n",
        "- Maybe we can feed the autoencoded features to non-nn models e.g. Xgboost, CatBoost, CBM? [refence](https://www.kaggle.com/gregorycalvez/de-anonymization-time-aggregation-tags)\r\n",
        "\r\n",
        "- Maybe we need to remove correlated featues before generating autoencoded features.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMpYkoWjTtOz"
      },
      "source": [
        "# Overview\r\n",
        "\r\n",
        "The efficient market hypothesis posits that markets cannot be beaten because asset prices will always reflect the fundamental value of the assets. In a perfectly efficient market, buyers and sellers would have all the agency and information needed to make rational trading decisions.\r\n",
        "\r\n",
        "In reality, financial markets are not efficient. The purpose of this trading model is to identify arbitrage opportunities to \"buy low and seell high\". In other words, we exploit market inefficiencies to identify and decide whether to execute profitable trades.\r\n",
        "\r\n",
        "The dataset, provided by Jane Street, contains an anonymized set of 129 features representing real stock market data. Each row in the dataset represents a trading opportunity, for which I predict an action value: 1 to amke the trade and 0 to pass on it. Due to the high demensionality of the dataset, I use Principal Components Analysis (PCA) to identify features to be used for supervised learning. The intuition is to compress the dataset and use it more efficiently. I then use XGBoost (extreme gradient boosting) - a hugely popular ML library due to its superior execution speed and model performance - to predict profitable trades. I also use Optuna (an automatic hyperparameter optimization software framework) to tune the hyperparameters of the classification model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkShraJ1gmU4"
      },
      "source": [
        "# Prework"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjtrZ9nvkRg-",
        "outputId": "f15852bc-c694-44fd-e6f9-cb48cad2890f"
      },
      "source": [
        "#hide\n",
        "\n",
        "%%writefile conditional_cell_extension.py\n",
        "def run_if(line, cell=None):\n",
        "    '''Execute current line/cell if line evaluates to True.'''\n",
        "    if not eval(line):\n",
        "        return\n",
        "    get_ipython().ex(cell)\n",
        "\n",
        "def load_ipython_extension(shell):\n",
        "    '''Registers the run_if magic when the extension loads.'''\n",
        "    shell.register_magic_function(run_if, 'line_cell')\n",
        "\n",
        "def unload_ipython_extension(shell):\n",
        "    '''Unregisters the run_if magic when the extension unloads.'''\n",
        "    del shell.magics_manager.magics['cell']['run_if']"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting conditional_cell_extension.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z54FNfMkWuZ"
      },
      "source": [
        "#hide\n",
        "%reload_ext conditional_cell_extension"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at15lXY1gwJT"
      },
      "source": [
        "#hide\n",
        "!pip install dabl > /dev/null\n",
        "!pip install datatable > /dev/null\n",
        "!pip install keras-tuner > /dev/null"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTR47QX7d2OS",
        "outputId": "9c259311-2a8d-481b-c348-559bbbe83562"
      },
      "source": [
        "#collapse-hide\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "import os, gc, cv2, random, warnings\n",
        "import re, math, sys, json, pprint, pdb\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import dabl\n",
        "import datatable as dt\n",
        "import kerastuner as kt\n",
        "\n",
        "warnings.simplefilter('ignore')\n",
        "print(f\"Using TensorFlow v{tf.__version__}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow v2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGiTrV67XLtQ",
        "outputId": "157e9380-e856-4bf6-cbaa-05f5b75473ee"
      },
      "source": [
        "#hide\n",
        "#@title Accelerator { run: \"auto\" }\n",
        "DEVICE = 'GPU' #@param [\"None\", \"'GPU'\", \"'TPU'\"] {type:\"raw\", allow-input: true}\n",
        "\n",
        "if DEVICE == \"TPU\":\n",
        "    print(\"connecting to TPU...\")\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        print('Running on TPU ', tpu.master())\n",
        "    except ValueError:\n",
        "        print(\"Could not connect to TPU\")\n",
        "        tpu = None\n",
        "\n",
        "    if tpu:\n",
        "        try:\n",
        "            print(\"initializing  TPU ...\")\n",
        "            tf.config.experimental_connect_to_cluster(tpu)\n",
        "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "            print(\"TPU initialized\")\n",
        "        except _:\n",
        "            print(\"failed to initialize TPU\")\n",
        "    else:\n",
        "        DEVICE = \"GPU\"\n",
        "\n",
        "if DEVICE != \"TPU\":\n",
        "    print(\"Using default strategy for CPU and single GPU\")\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "if DEVICE == \"GPU\":\n",
        "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "    \n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = strategy.num_replicas_in_sync\n",
        "print(f'REPLICAS: {REPLICAS}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using default strategy for CPU and single GPU\n",
            "Num GPUs Available:  1\n",
            "REPLICAS: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YSfQITMgtZr",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "46b5ac65-4a62-4521-a070-19c2c1a8a18a"
      },
      "source": [
        "#hide\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "                name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-65332308-6590-4ccf-bb06-d1f836619495\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-65332308-6590-4ccf-bb06-d1f836619495\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "User uploaded file \"kaggle.json\" with length 65 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3lNUbBYxJSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a0926bf-00cc-44af-b6e3-8359d7973da2"
      },
      "source": [
        "#collapse-hide\r\n",
        "#@title Notebook type { run: \"auto\", display-mode:\"form\" }\r\n",
        "SEED = 10120919\r\n",
        "DEBUG = False #@param {type:\"boolean\"}\r\n",
        "TRAIN = True #@param {type:\"boolean\"}\r\n",
        "\r\n",
        "def seed_everything(seed=0):\r\n",
        "    random.seed(seed)\r\n",
        "    np.random.seed(seed)\r\n",
        "    tf.random.set_seed(seed)\r\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\r\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\r\n",
        "\r\n",
        "GOOGLE = 'google.colab' in str(get_ipython())\r\n",
        "KAGGLE = not GOOGLE\r\n",
        "\r\n",
        "seed_everything(SEED)\r\n",
        "\r\n",
        "print(\"Running on {}!\".format(\r\n",
        "   \"Google Colab\" if GOOGLE else \"Kaggle Kernel\"\r\n",
        "))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on Google Colab!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF5aqyTEgFd-",
        "outputId": "9735d54e-9b59-4bc1-c858-09f84dc1b403"
      },
      "source": [
        "#hide\n",
        "%%run_if {GOOGLE}\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjiJYRTSkhyF",
        "outputId": "5e865d36-a2b3-4928-cb64-c542f3f00ec5"
      },
      "source": [
        "#hide\n",
        "project_name = 'jane-street-market-prediction'\n",
        "root_path  = '/content/gdrive/MyDrive/' if GOOGLE else '/'\n",
        "input_path = f'{root_path}kaggle/input/{project_name}/'\n",
        "working_path = f'{input_path}working/' if GOOGLE else '/kaggle/working/'\n",
        "os.makedirs(working_path, exist_ok=True)\n",
        "os.chdir(working_path)\n",
        "os.listdir(input_path)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train.csv',\n",
              " 'working',\n",
              " '__init__.py',\n",
              " 'competition.cpython-37m-x86_64-linux-gnu.so',\n",
              " 'train.csv.zip',\n",
              " 'example_sample_submission.csv',\n",
              " 'features.csv',\n",
              " 'example_test.csv.zip']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RukG4ZVwk3mb"
      },
      "source": [
        "#hide\n",
        "!kaggle competitions download -c jane-street-market-prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpJtt72E5sA4"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3s7Zsve5tNN"
      },
      "source": [
        "## Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKOzWvE7tEYo"
      },
      "source": [
        "Let's try [`datatable`](https://github.com/h2oai/datatable) to load the data, and then convert to a pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PX-44ijqlxEp",
        "outputId": "c20d2133-d66e-4236-971e-a59b5afcf4b1"
      },
      "source": [
        "%%run_if {}\n",
        "train_dt = dt.fread(f\"{input_path}train.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 30.9 s, sys: 6.73 s, total: 37.6 s\n",
            "Wall time: 3min 20s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPA-cPVlpf1t",
        "outputId": "65ce45a8-5635-42ce-e40f-4e932146868b"
      },
      "source": [
        "train_df = train_dt.to_pandas()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5.35 s, sys: 4.15 s, total: 9.5 s\n",
            "Wall time: 7.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjSxH15-ZmdD"
      },
      "source": [
        "The dataframe is `2390490` rows by `138` columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmZg91SiZZrN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75965087-681f-4cbf-e8d6-dce44eb4d01d"
      },
      "source": [
        "#hide_input\r\n",
        "train_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2390491 entries, 0 to 2390490\n",
            "Columns: 138 entries, date to ts_id\n",
            "dtypes: float64(135), int32(3)\n",
            "memory usage: 2.4 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_RVsA0KtYJX"
      },
      "source": [
        "## Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcnYFcC5EARU"
      },
      "source": [
        "This dataset coantians an anonymized set of features, featue_{0..129}, representing real stock market data. Each row in the dataset represents a trading opportunity, for which which you will be predicting an action value: 1 to make the trade and 0 to pass on it. Each trade has an associated weight and resp, which together represents a return on the trade. the data column is an integer which represents the day of the trade, while `ts_id` represents a time ordering. In addition to anonymized feature values, you are provided iwth metatdata about the features in `features.csv`{% fn 1%}\r\n",
        "\r\n",
        "In the training set, `train.csv`, you are provided a `resp` value, as well as several other `resp_{1,2,3,4}` values that represent returns over different time horizons. These variables are not included in the test set. \r\n",
        "\r\n",
        "{{ 'Kaggle Notebook: [EDA / A Quant's Prespective](https://www.kaggle.com/hamzashabbirbhatti/eda-a-quant-s-prespective) @hamzashabbirbhatti` | fndetail: 1 }}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukw0LhB4FE-G"
      },
      "source": [
        "feature_df = pd.read_csv(f\"{input_path}features.csv\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "jbQW_QvTFMak",
        "outputId": "3c1d8b62-d7ea-4eb5-abac-1970f000cc90"
      },
      "source": [
        "feature_df.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>tag_0</th>\n",
              "      <th>tag_1</th>\n",
              "      <th>tag_2</th>\n",
              "      <th>tag_3</th>\n",
              "      <th>tag_4</th>\n",
              "      <th>tag_5</th>\n",
              "      <th>tag_6</th>\n",
              "      <th>tag_7</th>\n",
              "      <th>tag_8</th>\n",
              "      <th>tag_9</th>\n",
              "      <th>tag_10</th>\n",
              "      <th>tag_11</th>\n",
              "      <th>tag_12</th>\n",
              "      <th>tag_13</th>\n",
              "      <th>tag_14</th>\n",
              "      <th>tag_15</th>\n",
              "      <th>tag_16</th>\n",
              "      <th>tag_17</th>\n",
              "      <th>tag_18</th>\n",
              "      <th>tag_19</th>\n",
              "      <th>tag_20</th>\n",
              "      <th>tag_21</th>\n",
              "      <th>tag_22</th>\n",
              "      <th>tag_23</th>\n",
              "      <th>tag_24</th>\n",
              "      <th>tag_25</th>\n",
              "      <th>tag_26</th>\n",
              "      <th>tag_27</th>\n",
              "      <th>tag_28</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>feature_0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>feature_1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>feature_2</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>feature_3</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>feature_4</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     feature  tag_0  tag_1  tag_2  ...  tag_25  tag_26  tag_27  tag_28\n",
              "0  feature_0  False  False  False  ...   False   False   False   False\n",
              "1  feature_1  False  False  False  ...   False   False   False   False\n",
              "2  feature_2  False  False  False  ...   False   False   False   False\n",
              "3  feature_3  False  False  False  ...   False   False   False   False\n",
              "4  feature_4  False  False  False  ...   False   False   False   False\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NLbgafYcwXN"
      },
      "source": [
        "## Preparing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B13mEW86HG6"
      },
      "source": [
        "train_df = train_df.query('date > 85').reset_index(drop=True)\r\n",
        "# limit memory usage\r\n",
        "train_df = train_df.astype({c: np.float32\r\n",
        "    for c in train_df.select_dtypes(include='float64').columns})\r\n",
        "train_df.fillna(train_df.mean(), inplace=True)\r\n",
        "train_df = train_df.query('weight > 0').reset_index(drop = True)\r\n",
        "\r\n",
        "train_df['action'] = ((train_df['resp_1'] > 0) &\r\n",
        "                      (train_df['resp_2'] > 0) &\r\n",
        "                      (train_df['resp_3'] > 0) &\r\n",
        "                      (train_df['resp_4'] > 0) &\r\n",
        "                      (train_df['resp'] > 0)).astype('int')\r\n",
        "\r\n",
        "features = [c for c in train_df.columns if 'feature' in c]\r\n",
        "resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\r\n",
        "\r\n",
        "x_train = train_df[features].values\r\n",
        "y_train = np.stack([(train_df[col] > 0).astype('int') \r\n",
        "                        for col in resp_cols]).T\r\n",
        "                        \r\n",
        "f_mean = np.mean(train_df[features[1:]].values, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1--OU7Pdl62"
      },
      "source": [
        "> Note: Modified code for [`class GroupTimeSeriesSplit(_BaseKFold)`](https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhMIklB5g5Cv"
      },
      "source": [
        "#collapse-show\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
        "from sklearn.utils.validation import _deprecate_positional_args\n",
        "\n",
        "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
        "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
        "    Allows for a gap in groups to avoid potentially leaking info from\n",
        "    train into test if the model has windowed or lag features.\n",
        "    Provides train/test indices to split time series data samples\n",
        "    that are observed at fixed time intervals according to a\n",
        "    third-party provided group.\n",
        "    In each split, test indices must be higher than before, and thus shuffling\n",
        "    in cross validator is inappropriate.\n",
        "    This cross-validation object is a variation of :class:`KFold`.\n",
        "    In the kth split, it returns first k folds as train set and the\n",
        "    (k+1)th fold as test set.\n",
        "    The same group will not appear in two different folds (the number of\n",
        "    distinct groups has to be at least equal to the number of folds).\n",
        "    Note that unlike standard cross-validation methods, successive\n",
        "    training sets are supersets of those that come before them.\n",
        "    Read more in the :ref:`User Guide <cross_validation>`.\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_splits : int, default=5\n",
        "        Number of splits. Must be at least 2.\n",
        "    max_train_group_size : int, default=Inf\n",
        "        Maximum group size for a single training set.\n",
        "    group_gap : int, default=None\n",
        "        Gap between train and test\n",
        "    max_test_group_size : int, default=Inf\n",
        "        We discard this number of groups from the end of each train split\n",
        "    \"\"\"\n",
        "\n",
        "    @_deprecate_positional_args\n",
        "    def __init__(self,\n",
        "                 n_splits=5,\n",
        "                 *,\n",
        "                 max_train_group_size=np.inf,\n",
        "                 max_test_group_size=np.inf,\n",
        "                 group_gap=None,\n",
        "                 verbose=False\n",
        "                 ):\n",
        "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
        "        self.max_train_group_size = max_train_group_size\n",
        "        self.group_gap = group_gap\n",
        "        self.max_test_group_size = max_test_group_size\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def split(self, X, y=None, groups=None):\n",
        "        \"\"\"Generate indices to split data into training and test set.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training data, where n_samples is the number of samples\n",
        "            and n_features is the number of features.\n",
        "        y : array-like of shape (n_samples,)\n",
        "            Always ignored, exists for compatibility.\n",
        "        groups : array-like of shape (n_samples,)\n",
        "            Group labels for the samples used while splitting the dataset into\n",
        "            train/test set.\n",
        "        Yields\n",
        "        ------\n",
        "        train : ndarray\n",
        "            The training set indices for that split.\n",
        "        test : ndarray\n",
        "            The testing set indices for that split.\n",
        "        \"\"\"\n",
        "        if groups is None:\n",
        "            raise ValueError(\n",
        "                \"The 'groups' parameter should not be None\")\n",
        "        X, y, groups = indexable(X, y, groups)\n",
        "        n_samples = _num_samples(X)\n",
        "        n_splits = self.n_splits\n",
        "        group_gap = self.group_gap\n",
        "        max_test_group_size = self.max_test_group_size\n",
        "        max_train_group_size = self.max_train_group_size\n",
        "        n_folds = n_splits + 1\n",
        "        group_dict = {}\n",
        "        u, ind = np.unique(groups, return_index=True)\n",
        "        unique_groups = u[np.argsort(ind)]\n",
        "        n_samples = _num_samples(X)\n",
        "        n_groups = _num_samples(unique_groups)\n",
        "        for idx in np.arange(n_samples):\n",
        "            if (groups[idx] in group_dict):\n",
        "                group_dict[groups[idx]].append(idx)\n",
        "            else:\n",
        "                group_dict[groups[idx]] = [idx]\n",
        "        if n_folds > n_groups:\n",
        "            raise ValueError(\n",
        "                (\"Cannot have number of folds={0} greater than\"\n",
        "                 \" the number of groups={1}\").format(n_folds,\n",
        "                                                     n_groups))\n",
        "\n",
        "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
        "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
        "                                  n_groups, group_test_size)\n",
        "        for group_test_start in group_test_starts:\n",
        "            train_array = []\n",
        "            test_array = []\n",
        "\n",
        "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
        "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
        "                train_array_tmp = group_dict[train_group_idx]\n",
        "                \n",
        "                train_array = np.sort(np.unique(\n",
        "                                      np.concatenate((train_array,\n",
        "                                                      train_array_tmp)),\n",
        "                                      axis=None), axis=None)\n",
        "\n",
        "            train_end = train_array.size\n",
        " \n",
        "            for test_group_idx in unique_groups[group_test_start:\n",
        "                                                group_test_start +\n",
        "                                                group_test_size]:\n",
        "                test_array_tmp = group_dict[test_group_idx]\n",
        "                test_array = np.sort(np.unique(\n",
        "                                              np.concatenate((test_array,\n",
        "                                                              test_array_tmp)),\n",
        "                                     axis=None), axis=None)\n",
        "\n",
        "            test_array  = test_array[group_gap:]\n",
        "            \n",
        "            \n",
        "            if self.verbose > 0:\n",
        "                    pass\n",
        "                    \n",
        "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M5D23bygyo8"
      },
      "source": [
        "#collapse-show\n",
        "class CVTuner(kt.engine.tuner.Tuner):\n",
        "    def run_trial(self, trial, X, y, splits, batch_size=32, epochs=1,callbacks=None):\n",
        "        val_losses = []\n",
        "        for train_indices, test_indices in splits:\n",
        "            X_train, X_test = [x[train_indices] for x in X], [x[test_indices] for x in X]\n",
        "            y_train, y_test = [a[train_indices] for a in y], [a[test_indices] for a in y]\n",
        "            if len(X_train) < 2:\n",
        "                X_train = X_train[0]\n",
        "                X_test = X_test[0]\n",
        "            if len(y_train) < 2:\n",
        "                y_train = y_train[0]\n",
        "                y_test = y_test[0]\n",
        "            \n",
        "            model = self.hypermodel.build(trial.hyperparameters)\n",
        "            hist = model.fit(X_train,y_train,\n",
        "                      validation_data=(X_test,y_test),\n",
        "                      epochs=epochs,\n",
        "                        batch_size=batch_size,\n",
        "                      callbacks=callbacks)\n",
        "            \n",
        "            val_losses.append([hist.history[k][-1] for k in hist.history])\n",
        "        val_losses = np.asarray(val_losses)\n",
        "        self.oracle.update_trial(trial.trial_id, {k:np.mean(val_losses[:,i]) for i,k in enumerate(hist.history.keys())})\n",
        "        self.save_model(trial.trial_id, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68AjcRXM-jQl"
      },
      "source": [
        "# Model\n",
        "\n",
        "As many people have mentioned, finance datasets are usually have low signal-to-noise ratio (SNR). So, here comes an intuitive question. Does Denoising Auto Encoder (DAE) help in reducing noise and cleaning data in finance datasets?\n",
        "\n",
        "The idea of using an encoder is the denoise the data. After many attempts at using a unsupervised autoencoder, the choice landed on a bottleneck encoder as this will preserve the intra-feature relations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtwblMShCDYh"
      },
      "source": [
        "## Building the autoencoder\n",
        "\n",
        "The autoencoder should aid in denoising the data based on [this](https://www.semanticscholar.org/paper/Deep-Bottleneck-Classifiers-in-Supervised-Dimension-Parviainen/fb86483f7573f6430fe4597432b0cd3e34b16e43) paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce7QkKta-riB"
      },
      "source": [
        "def build_autoencoder(input_dim, output_dim, noise=.05):\n",
        "    inputs = tf.keras.layers.Input(input_dim)\n",
        "    encoded = tf.keras.layers.BatchNormalization()(inputs)\n",
        "    encoded = tf.keras.layers.GaussianNoise(noise)(encoded)\n",
        "    encoded = tf.keras.layers.Dense(640, activation='relu')(encoded)\n",
        "    decoded = tf.keras.layers.Dropout(0.2)(encoded)\n",
        "    decoded = tf.keras.layers.Dense(input_dim, name='decoded')(decoded)\n",
        "    x = tf.keras.layers.Dense(320, activation='relu')(decoded)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Dense(output_dim, activation='sigmoid',\n",
        "                              name='label_output')(x)\n",
        "\n",
        "    encoder = tf.keras.models.Model(inputs=inputs, outputs=encoded)\n",
        "\n",
        "    autoencoder = tf.keras.models.Model(inputs=inputs, outputs=[decoded,x])\n",
        "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "                        loss={'decoded':'mse',\n",
        "                              'label_output':'binary_crossentropy'})\n",
        "    return autoencoder, encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpXA-UOkCH0l"
      },
      "source": [
        "## Building the MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6R0rSS2A3Uh"
      },
      "source": [
        "def build_model(hp, input_dim, output_dim, encoder):\n",
        "    inputs = tf.keras.layers.Input(input_dim)\n",
        "    \n",
        "    x = encoder(inputs)\n",
        "    \n",
        "    x = tf.keras.layers.Concatenate()([x,inputs]) #use both raw and encoded features\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(hp.Float('init_dropout',0.0,0.5))(x)\n",
        "    \n",
        "    for i in range(hp.Int('num_layers',1,5)):\n",
        "        x = tf.keras.layers.Dense(hp.Int('num_units_{i}',128,256))(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Lambda(tf.keras.activations.swish)(x)\n",
        "        x = tf.keras.layers.Dropout(hp.Float(f'dropout_{i}',0.0,0.5))(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(output_dim,activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "    model.compile(optimizer = tf.keras.optimizers.Adam(\n",
        "                    hp.Float('lr',0.00001,0.1,default=0.001)),\n",
        "                  loss = tf.keras.losses.BinaryCrossentropy(\n",
        "                    label_smoothing = hp.Float('label_smoothing',0.0,0.1)),\n",
        "                  metrics = [tf.keras.metrics.AUC(name = 'auc')])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN8xSjQKCoct"
      },
      "source": [
        "## Defining and training the autoencoder\n",
        "\n",
        "We add gaussian noise with mean and std from training datea. After training we lock the layersfin the encoder from further training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IawHYvSFvDPm"
      },
      "source": [
        "autoencoder, encoder = build_autoencoder(x_train.shape[-1], \r\n",
        "                                         y_train.shape[-1],\r\n",
        "                                         noise=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxsTJpM1v595",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79e0e43d-69b5-4b63-8fb6-86a390038c51"
      },
      "source": [
        "#collapse-output\r\n",
        "autoencoder.fit(x_train,(x_train, y_train),\r\n",
        "                epochs=1000,\r\n",
        "                batch_size=4096, \r\n",
        "                validation_split=0.1,\r\n",
        "                callbacks = [\r\n",
        "                    tf.keras.callbacks.EarlyStopping(\r\n",
        "                        'val_loss', patience=10,\r\n",
        "                        restore_best_weights=True)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "346/346 [==============================] - 7s 12ms/step - loss: 2.1184 - decoded_loss: 1.3832 - label_output_loss: 0.7352 - val_loss: 0.8046 - val_decoded_loss: 0.1152 - val_label_output_loss: 0.6894\n",
            "Epoch 2/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 1.0088 - decoded_loss: 0.3173 - label_output_loss: 0.6915 - val_loss: 0.7703 - val_decoded_loss: 0.0810 - val_label_output_loss: 0.6893\n",
            "Epoch 3/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9952 - decoded_loss: 0.3053 - label_output_loss: 0.6899 - val_loss: 0.7617 - val_decoded_loss: 0.0728 - val_label_output_loss: 0.6890\n",
            "Epoch 4/1000\n",
            "346/346 [==============================] - 3s 9ms/step - loss: 0.9690 - decoded_loss: 0.2794 - label_output_loss: 0.6895 - val_loss: 0.7576 - val_decoded_loss: 0.0686 - val_label_output_loss: 0.6890\n",
            "Epoch 5/1000\n",
            "346/346 [==============================] - 3s 9ms/step - loss: 0.9750 - decoded_loss: 0.2856 - label_output_loss: 0.6894 - val_loss: 0.7562 - val_decoded_loss: 0.0676 - val_label_output_loss: 0.6886\n",
            "Epoch 6/1000\n",
            "346/346 [==============================] - 3s 9ms/step - loss: 0.9759 - decoded_loss: 0.2867 - label_output_loss: 0.6892 - val_loss: 0.7537 - val_decoded_loss: 0.0645 - val_label_output_loss: 0.6892\n",
            "Epoch 7/1000\n",
            "346/346 [==============================] - 4s 10ms/step - loss: 0.9767 - decoded_loss: 0.2876 - label_output_loss: 0.6891 - val_loss: 0.7552 - val_decoded_loss: 0.0666 - val_label_output_loss: 0.6886\n",
            "Epoch 8/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9884 - decoded_loss: 0.2994 - label_output_loss: 0.6889 - val_loss: 0.7473 - val_decoded_loss: 0.0590 - val_label_output_loss: 0.6882\n",
            "Epoch 9/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9531 - decoded_loss: 0.2644 - label_output_loss: 0.6887 - val_loss: 0.7472 - val_decoded_loss: 0.0592 - val_label_output_loss: 0.6880\n",
            "Epoch 10/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 1.0210 - decoded_loss: 0.3324 - label_output_loss: 0.6886 - val_loss: 0.7472 - val_decoded_loss: 0.0586 - val_label_output_loss: 0.6887\n",
            "Epoch 11/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9625 - decoded_loss: 0.2740 - label_output_loss: 0.6885 - val_loss: 0.7477 - val_decoded_loss: 0.0595 - val_label_output_loss: 0.6883\n",
            "Epoch 12/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9719 - decoded_loss: 0.2835 - label_output_loss: 0.6884 - val_loss: 0.7530 - val_decoded_loss: 0.0650 - val_label_output_loss: 0.6879\n",
            "Epoch 13/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9867 - decoded_loss: 0.2983 - label_output_loss: 0.6884 - val_loss: 0.7494 - val_decoded_loss: 0.0611 - val_label_output_loss: 0.6882\n",
            "Epoch 14/1000\n",
            "346/346 [==============================] - 3s 9ms/step - loss: 0.9518 - decoded_loss: 0.2636 - label_output_loss: 0.6881 - val_loss: 0.7477 - val_decoded_loss: 0.0590 - val_label_output_loss: 0.6887\n",
            "Epoch 15/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9466 - decoded_loss: 0.2585 - label_output_loss: 0.6881 - val_loss: 0.7474 - val_decoded_loss: 0.0595 - val_label_output_loss: 0.6879\n",
            "Epoch 16/1000\n",
            "346/346 [==============================] - 3s 9ms/step - loss: 0.9588 - decoded_loss: 0.2708 - label_output_loss: 0.6880 - val_loss: 0.7467 - val_decoded_loss: 0.0585 - val_label_output_loss: 0.6882\n",
            "Epoch 17/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 1.0079 - decoded_loss: 0.3200 - label_output_loss: 0.6879 - val_loss: 0.7465 - val_decoded_loss: 0.0584 - val_label_output_loss: 0.6881\n",
            "Epoch 18/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9491 - decoded_loss: 0.2614 - label_output_loss: 0.6877 - val_loss: 0.7464 - val_decoded_loss: 0.0584 - val_label_output_loss: 0.6881\n",
            "Epoch 19/1000\n",
            "346/346 [==============================] - 3s 9ms/step - loss: 0.9493 - decoded_loss: 0.2616 - label_output_loss: 0.6877 - val_loss: 0.7551 - val_decoded_loss: 0.0671 - val_label_output_loss: 0.6881\n",
            "Epoch 20/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9348 - decoded_loss: 0.2472 - label_output_loss: 0.6876 - val_loss: 0.7574 - val_decoded_loss: 0.0693 - val_label_output_loss: 0.6881\n",
            "Epoch 21/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9351 - decoded_loss: 0.2474 - label_output_loss: 0.6877 - val_loss: 0.7526 - val_decoded_loss: 0.0644 - val_label_output_loss: 0.6882\n",
            "Epoch 22/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9658 - decoded_loss: 0.2783 - label_output_loss: 0.6874 - val_loss: 0.7521 - val_decoded_loss: 0.0640 - val_label_output_loss: 0.6881\n",
            "Epoch 23/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9430 - decoded_loss: 0.2556 - label_output_loss: 0.6874 - val_loss: 0.7504 - val_decoded_loss: 0.0620 - val_label_output_loss: 0.6884\n",
            "Epoch 24/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9473 - decoded_loss: 0.2602 - label_output_loss: 0.6871 - val_loss: 0.7534 - val_decoded_loss: 0.0653 - val_label_output_loss: 0.6881\n",
            "Epoch 25/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9362 - decoded_loss: 0.2489 - label_output_loss: 0.6873 - val_loss: 0.7517 - val_decoded_loss: 0.0635 - val_label_output_loss: 0.6882\n",
            "Epoch 26/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9463 - decoded_loss: 0.2592 - label_output_loss: 0.6872 - val_loss: 0.7538 - val_decoded_loss: 0.0654 - val_label_output_loss: 0.6883\n",
            "Epoch 27/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9584 - decoded_loss: 0.2712 - label_output_loss: 0.6872 - val_loss: 0.7582 - val_decoded_loss: 0.0699 - val_label_output_loss: 0.6883\n",
            "Epoch 28/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9484 - decoded_loss: 0.2614 - label_output_loss: 0.6870 - val_loss: 0.7580 - val_decoded_loss: 0.0699 - val_label_output_loss: 0.6881\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa49aa04fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gzgpbPJ2TOZ"
      },
      "source": [
        "encoder.save_weights('encoder.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZG-nTmAwUOO"
      },
      "source": [
        "encoder.load_weights('encoder.hdf5')\r\n",
        "encoder.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I58px8-rgDwp"
      },
      "source": [
        "## Running CV\r\n",
        "\r\n",
        "Following [this notebook](https://www.kaggle.com/gogo827jz/jane-street-ffill-xgboost-purgedtimeseriescv) which use 5 `PurgedGroupTimeSeriesSplit` split on the dates in the training data. \r\n",
        "\r\n",
        "We add the locked encoder as the first layer of the MLP. This seems to help in speeding up the submission rather than first predicting using the encoder then using the MLP. \r\n",
        "\r\n",
        "We use a Baysian Optimizer to find the optimal HPs for out model. 20 trials take about 2 hours on GPU.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gSLY6Q0ehFa"
      },
      "source": [
        "model_fn = lambda hp: build_model(\n",
        "    hp, x_train.shape[-1], y_train.shape[-1], encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj7L8EldhIRx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f22a64-5e62-4a61-bd25-198185bfa855"
      },
      "source": [
        "tuner = CVTuner(\r\n",
        "    hypermodel = model_fn,\r\n",
        "    oracle=kt.oracles.BayesianOptimization(\r\n",
        "        objective=kt.Objective('val_auc', direction='max'),\r\n",
        "        num_initial_points=4,\r\n",
        "        max_trials=20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project ./untitled_project/oracle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUeEFEv4iC4U"
      },
      "source": [
        "#collapse-output\r\n",
        "gkf = PurgedGroupTimeSeriesSplit(n_splits = 5, group_gap=20)\r\n",
        "splits = list(gkf.split(y_train, groups=train_df['date'].values))\r\n",
        "tuner.search((x_train,),(y_train,),\r\n",
        "             splits=splits,\r\n",
        "             batch_size=4096,\r\n",
        "             epochs=100,\r\n",
        "             callbacks=[\r\n",
        "                tf.keras.callbacks.EarlyStopping(\r\n",
        "                    'val_auc', mode='max', patience=3)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iImxHjyGyv-z"
      },
      "source": [
        "hp  = tuner.get_best_hyperparameters(1)[0]\r\n",
        "pd.to_pickle(hp,f'./best_hp_{SEED}.pkl')\r\n",
        "for fold, (train_indices, test_indices) in enumerate(splits):\r\n",
        "    model = model_fn(hp)\r\n",
        "    X_train, X_test = X[train_indices], X[test_indices]\r\n",
        "    y_train, y_test = y[train_indices], y[test_indices]\r\n",
        "    model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100,batch_size=4096,callbacks=[EarlyStopping('val_auc',mode='max',patience=10,restore_best_weights=True)])\r\n",
        "    model.save_weights(f'./model_{SEED}_{fold}.hdf5')\r\n",
        "    model.compile(Adam(hp.get('lr')/100),loss='binary_crossentropy')\r\n",
        "    model.fit(X_test,y_test,epochs=3,batch_size=4096)\r\n",
        "    model.save_weights(f'./model_{SEED}_{fold}_finetune.hdf5')\r\n",
        "tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}