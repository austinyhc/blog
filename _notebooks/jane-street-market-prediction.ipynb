{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jane_street_market_prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ-F1VMOcltK"
      },
      "source": [
        "# Jane Street Market Prediction \n",
        "> Buy low, sell high. It sounds so easyâ€¦.\n",
        "\n",
        "- toc: true\n",
        "- badges: true\n",
        "- author: Austin Chen\n",
        "- categories: [time series,stock,kaggle]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMpYkoWjTtOz"
      },
      "source": [
        "The efficient market hypothesis posits that markets cannot be beaten because asset prices will always reflect the fundamental value of the assets. In a perfectly efficient market, buyers and sellers would have all the agency and information needed to make rational trading decisions.\r\n",
        "\r\n",
        "In reality, financial markets are not efficient. The purpose of this trading model is to identify arbitrage opportunities to \"buy low and seell high\". In other words, we exploit market inefficiencies to identify and decide whether to execute profitable trades.\r\n",
        "\r\n",
        "The dataset, provided by Jane Street, contains an anonymized set of 129 features representing real stock market data. Each row in the dataset represents a trading opportunity, for which I predict an action value: 1 to amke the trade and 0 to pass on it. Due to the high demensionality of the dataset, I use Principal Components Analysis (PCA) to identify features to be used for supervised learning. The intuition is to compress the dataset and use it more efficiently. I then use XGBoost (extreme gradient boosting) - a hugely popular ML library due to its superior execution speed and model performance - to predict profitable trades. I also use Optuna (an automatic hyperparameter optimization software framework) to tune the hyperparameters of the classification model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjtrZ9nvkRg-",
        "outputId": "3343e17c-68c9-4435-a791-39d272b68e3b"
      },
      "source": [
        "#hide\n",
        "\n",
        "%%writefile conditional_cell_extension.py\n",
        "def run_if(line, cell=None):\n",
        "    '''Execute current line/cell if line evaluates to True.'''\n",
        "    if not eval(line):\n",
        "        return\n",
        "    get_ipython().ex(cell)\n",
        "\n",
        "def load_ipython_extension(shell):\n",
        "    '''Registers the run_if magic when the extension loads.'''\n",
        "    shell.register_magic_function(run_if, 'line_cell')\n",
        "\n",
        "def unload_ipython_extension(shell):\n",
        "    '''Unregisters the run_if magic when the extension unloads.'''\n",
        "    del shell.magics_manager.magics['cell']['run_if']"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing conditional_cell_extension.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z54FNfMkWuZ"
      },
      "source": [
        "#hide\n",
        "%reload_ext conditional_cell_extension"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkShraJ1gmU4"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at15lXY1gwJT"
      },
      "source": [
        "#hide\n",
        "!pip install dabl > /dev/null\n",
        "!pip install datatable > /dev/null\n",
        "!pip install keras-tuner > /dev/null"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTR47QX7d2OS",
        "outputId": "c2cc9dcb-7c72-4b4b-a493-cb6acb0d7444"
      },
      "source": [
        "#collapse-hide\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "import os, gc, cv2, random, warnings\n",
        "import re, math, sys, json, pprint, pdb\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import dabl\n",
        "import datatable as dt\n",
        "import kerastuner as kt\n",
        "\n",
        "warnings.simplefilter('ignore')\n",
        "print(f\"Using TensorFlow v{tf.__version__}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow v2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGiTrV67XLtQ",
        "outputId": "b80b8668-b070-4154-dc52-37584230d21a"
      },
      "source": [
        "#hide\n",
        "#@title Accelerator { run: \"auto\" }\n",
        "DEVICE = 'GPU' #@param [\"None\", \"'GPU'\", \"'TPU'\"] {type:\"raw\", allow-input: true}\n",
        "\n",
        "if DEVICE == \"TPU\":\n",
        "    print(\"connecting to TPU...\")\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        print('Running on TPU ', tpu.master())\n",
        "    except ValueError:\n",
        "        print(\"Could not connect to TPU\")\n",
        "        tpu = None\n",
        "\n",
        "    if tpu:\n",
        "        try:\n",
        "            print(\"initializing  TPU ...\")\n",
        "            tf.config.experimental_connect_to_cluster(tpu)\n",
        "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "            print(\"TPU initialized\")\n",
        "        except _:\n",
        "            print(\"failed to initialize TPU\")\n",
        "    else:\n",
        "        DEVICE = \"GPU\"\n",
        "\n",
        "if DEVICE != \"TPU\":\n",
        "    print(\"Using default strategy for CPU and single GPU\")\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "if DEVICE == \"GPU\":\n",
        "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "    \n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = strategy.num_replicas_in_sync\n",
        "print(f'REPLICAS: {REPLICAS}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using default strategy for CPU and single GPU\n",
            "Num GPUs Available:  1\n",
            "REPLICAS: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YSfQITMgtZr"
      },
      "source": [
        "#hide\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "                name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3lNUbBYxJSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "041295c3-a884-450a-e77c-1e56993b9e0f"
      },
      "source": [
        "#collapse-hide\r\n",
        "#@title Notebook type { run: \"auto\", display-mode:\"form\" }\r\n",
        "SEED = 10120919\r\n",
        "DEBUG = False #@param {type:\"boolean\"}\r\n",
        "TRAIN = True #@param {type:\"boolean\"}\r\n",
        "\r\n",
        "def seed_everything(seed=0):\r\n",
        "    random.seed(seed)\r\n",
        "    np.random.seed(seed)\r\n",
        "    tf.random.set_seed(seed)\r\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\r\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\r\n",
        "\r\n",
        "GOOGLE = 'google.colab' in str(get_ipython())\r\n",
        "KAGGLE = not GOOGLE\r\n",
        "\r\n",
        "seed_everything(SEED)\r\n",
        "\r\n",
        "print(\"Running on {}!\".format(\r\n",
        "   \"Google Colab\" if GOOGLE else \"Kaggle Kernel\"\r\n",
        "))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on Google Colab!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF5aqyTEgFd-",
        "outputId": "25c64b86-a674-4ae9-b1ad-bc90f87c2cb6"
      },
      "source": [
        "#hide\n",
        "%%run_if {GOOGLE}\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjiJYRTSkhyF",
        "outputId": "0eba8b8d-bd6d-420e-dc20-9f38ff8d6db7"
      },
      "source": [
        "#hide\n",
        "project_name = 'jane-street-market-prediction'\n",
        "root_path  = '/content/gdrive/MyDrive/' if GOOGLE else '/'\n",
        "input_path = f'{root_path}kaggle/input/{project_name}/'\n",
        "working_path = f'{input_path}working/' if GOOGLE else '/kaggle/working/'\n",
        "os.makedirs(working_path, exist_ok=True)\n",
        "os.chdir(working_path)\n",
        "os.listdir(input_path)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train.csv',\n",
              " 'working',\n",
              " '__init__.py',\n",
              " 'competition.cpython-37m-x86_64-linux-gnu.so',\n",
              " 'train.csv.zip',\n",
              " 'example_sample_submission.csv',\n",
              " 'features.csv',\n",
              " 'example_test.csv.zip']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RukG4ZVwk3mb"
      },
      "source": [
        "#hide\n",
        "!kaggle competitions download -c jane-street-market-prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjCJGF3so1rU"
      },
      "source": [
        "We can observe that the train.csv is large: `6GB` and it has `2390492` rows in the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2g7LDGZk-rX",
        "outputId": "b182df3d-603a-4343-b138-7795be8fbdd2"
      },
      "source": [
        "#hide_input\n",
        "!wc -l {input_path}train.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2390492 /content/gdrive/MyDrive/kaggle/input/jane-street-market-prediction/train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKOzWvE7tEYo"
      },
      "source": [
        "To speed things up here, let's use `datatable` to read the data, and then convert to a pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PX-44ijqlxEp",
        "outputId": "09cd184c-bfa2-4aca-ee12-295ec54043b6"
      },
      "source": [
        "%%time\n",
        "train_dt = dt.fread(f\"{input_path}train.csv\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 25.6 s, sys: 5.94 s, total: 31.5 s\n",
            "Wall time: 3min 1s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPA-cPVlpf1t",
        "outputId": "0152c3d7-d2b2-4f7e-8982-c3ee0c3ec29b"
      },
      "source": [
        "%%time\n",
        "train_df = train_dt.to_pandas()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.82 s, sys: 3.88 s, total: 8.7 s\n",
            "Wall time: 6.72 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_RVsA0KtYJX"
      },
      "source": [
        "# Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzPKoUfh53I4"
      },
      "source": [
        "# HyperParameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpJtt72E5sA4"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3s7Zsve5tNN"
      },
      "source": [
        "## Loading training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B13mEW86HG6"
      },
      "source": [
        "train_df = train_df.query('date > 85').reset_index(drop=True)\r\n",
        "# limit memory usage\r\n",
        "train_df = train_df.astype({c: np.float32\r\n",
        "    for c in train_df.select_dtypes(include='float64').columns})\r\n",
        "train_df.fillna(train_df.mean(), inplace=True)\r\n",
        "train_df = train_df.query('weight > 0').reset_index(drop = True)\r\n",
        "\r\n",
        "train_df['action'] = ((train_df['resp_1'] > 0) &\r\n",
        "                      (train_df['resp_2'] > 0) &\r\n",
        "                      (train_df['resp_3'] > 0) &\r\n",
        "                      (train_df['resp_4'] > 0) &\r\n",
        "                      (train_df['resp'] > 0)).astype('int')\r\n",
        "\r\n",
        "features = [c for c in train_df.columns if 'feature' in c]\r\n",
        "resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\r\n",
        "\r\n",
        "x_train = train_df[features].values\r\n",
        "y_train = np.stack([(train_df[col] > 0).astype('int') \r\n",
        "                        for col in resp_cols]).T\r\n",
        "                        \r\n",
        "f_mean = np.mean(train_df[features[1:]].values, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1--OU7Pdl62"
      },
      "source": [
        "> Note: Modified code for [`class GroupTimeSeriesSplit(_BaseKFold)`](https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhMIklB5g5Cv"
      },
      "source": [
        "#collapse-show\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
        "from sklearn.utils.validation import _deprecate_positional_args\n",
        "\n",
        "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
        "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
        "    Allows for a gap in groups to avoid potentially leaking info from\n",
        "    train into test if the model has windowed or lag features.\n",
        "    Provides train/test indices to split time series data samples\n",
        "    that are observed at fixed time intervals according to a\n",
        "    third-party provided group.\n",
        "    In each split, test indices must be higher than before, and thus shuffling\n",
        "    in cross validator is inappropriate.\n",
        "    This cross-validation object is a variation of :class:`KFold`.\n",
        "    In the kth split, it returns first k folds as train set and the\n",
        "    (k+1)th fold as test set.\n",
        "    The same group will not appear in two different folds (the number of\n",
        "    distinct groups has to be at least equal to the number of folds).\n",
        "    Note that unlike standard cross-validation methods, successive\n",
        "    training sets are supersets of those that come before them.\n",
        "    Read more in the :ref:`User Guide <cross_validation>`.\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_splits : int, default=5\n",
        "        Number of splits. Must be at least 2.\n",
        "    max_train_group_size : int, default=Inf\n",
        "        Maximum group size for a single training set.\n",
        "    group_gap : int, default=None\n",
        "        Gap between train and test\n",
        "    max_test_group_size : int, default=Inf\n",
        "        We discard this number of groups from the end of each train split\n",
        "    \"\"\"\n",
        "\n",
        "    @_deprecate_positional_args\n",
        "    def __init__(self,\n",
        "                 n_splits=5,\n",
        "                 *,\n",
        "                 max_train_group_size=np.inf,\n",
        "                 max_test_group_size=np.inf,\n",
        "                 group_gap=None,\n",
        "                 verbose=False\n",
        "                 ):\n",
        "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
        "        self.max_train_group_size = max_train_group_size\n",
        "        self.group_gap = group_gap\n",
        "        self.max_test_group_size = max_test_group_size\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def split(self, X, y=None, groups=None):\n",
        "        \"\"\"Generate indices to split data into training and test set.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training data, where n_samples is the number of samples\n",
        "            and n_features is the number of features.\n",
        "        y : array-like of shape (n_samples,)\n",
        "            Always ignored, exists for compatibility.\n",
        "        groups : array-like of shape (n_samples,)\n",
        "            Group labels for the samples used while splitting the dataset into\n",
        "            train/test set.\n",
        "        Yields\n",
        "        ------\n",
        "        train : ndarray\n",
        "            The training set indices for that split.\n",
        "        test : ndarray\n",
        "            The testing set indices for that split.\n",
        "        \"\"\"\n",
        "        if groups is None:\n",
        "            raise ValueError(\n",
        "                \"The 'groups' parameter should not be None\")\n",
        "        X, y, groups = indexable(X, y, groups)\n",
        "        n_samples = _num_samples(X)\n",
        "        n_splits = self.n_splits\n",
        "        group_gap = self.group_gap\n",
        "        max_test_group_size = self.max_test_group_size\n",
        "        max_train_group_size = self.max_train_group_size\n",
        "        n_folds = n_splits + 1\n",
        "        group_dict = {}\n",
        "        u, ind = np.unique(groups, return_index=True)\n",
        "        unique_groups = u[np.argsort(ind)]\n",
        "        n_samples = _num_samples(X)\n",
        "        n_groups = _num_samples(unique_groups)\n",
        "        for idx in np.arange(n_samples):\n",
        "            if (groups[idx] in group_dict):\n",
        "                group_dict[groups[idx]].append(idx)\n",
        "            else:\n",
        "                group_dict[groups[idx]] = [idx]\n",
        "        if n_folds > n_groups:\n",
        "            raise ValueError(\n",
        "                (\"Cannot have number of folds={0} greater than\"\n",
        "                 \" the number of groups={1}\").format(n_folds,\n",
        "                                                     n_groups))\n",
        "\n",
        "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
        "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
        "                                  n_groups, group_test_size)\n",
        "        for group_test_start in group_test_starts:\n",
        "            train_array = []\n",
        "            test_array = []\n",
        "\n",
        "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
        "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
        "                train_array_tmp = group_dict[train_group_idx]\n",
        "                \n",
        "                train_array = np.sort(np.unique(\n",
        "                                      np.concatenate((train_array,\n",
        "                                                      train_array_tmp)),\n",
        "                                      axis=None), axis=None)\n",
        "\n",
        "            train_end = train_array.size\n",
        " \n",
        "            for test_group_idx in unique_groups[group_test_start:\n",
        "                                                group_test_start +\n",
        "                                                group_test_size]:\n",
        "                test_array_tmp = group_dict[test_group_idx]\n",
        "                test_array = np.sort(np.unique(\n",
        "                                              np.concatenate((test_array,\n",
        "                                                              test_array_tmp)),\n",
        "                                     axis=None), axis=None)\n",
        "\n",
        "            test_array  = test_array[group_gap:]\n",
        "            \n",
        "            \n",
        "            if self.verbose > 0:\n",
        "                    pass\n",
        "                    \n",
        "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M5D23bygyo8"
      },
      "source": [
        "#collapse-show\n",
        "class CVTuner(kt.engine.tuner.Tuner):\n",
        "    def run_trial(self, trial, X, y, splits, batch_size=32, epochs=1,callbacks=None):\n",
        "        val_losses = []\n",
        "        for train_indices, test_indices in splits:\n",
        "            X_train, X_test = [x[train_indices] for x in X], [x[test_indices] for x in X]\n",
        "            y_train, y_test = [a[train_indices] for a in y], [a[test_indices] for a in y]\n",
        "            if len(X_train) < 2:\n",
        "                X_train = X_train[0]\n",
        "                X_test = X_test[0]\n",
        "            if len(y_train) < 2:\n",
        "                y_train = y_train[0]\n",
        "                y_test = y_test[0]\n",
        "            \n",
        "            model = self.hypermodel.build(trial.hyperparameters)\n",
        "            hist = model.fit(X_train,y_train,\n",
        "                      validation_data=(X_test,y_test),\n",
        "                      epochs=epochs,\n",
        "                        batch_size=batch_size,\n",
        "                      callbacks=callbacks)\n",
        "            \n",
        "            val_losses.append([hist.history[k][-1] for k in hist.history])\n",
        "        val_losses = np.asarray(val_losses)\n",
        "        self.oracle.update_trial(trial.trial_id, {k:np.mean(val_losses[:,i]) for i,k in enumerate(hist.history.keys())})\n",
        "        self.save_model(trial.trial_id, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68AjcRXM-jQl"
      },
      "source": [
        "# Model\n",
        "\n",
        "The idea of using an encoder is the denoise the data. After many attempts at using a unsupervised autoencoder, the choice landed on a bottleneck encoder as this will preserve the intra-feature relations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtwblMShCDYh"
      },
      "source": [
        "## Building the autoencoder\n",
        "\n",
        "The autoencoder should aid in denoising the data based on [this](https://www.semanticscholar.org/paper/Deep-Bottleneck-Classifiers-in-Supervised-Dimension-Parviainen/fb86483f7573f6430fe4597432b0cd3e34b16e43) paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce7QkKta-riB"
      },
      "source": [
        "def build_autoencoder(input_dim, output_dim, noise=.05):\n",
        "    inputs = tf.keras.layers.Input(input_dim)\n",
        "    encoded = tf.keras.layers.BatchNormalization()(inputs)\n",
        "    encoded = tf.keras.layers.GaussianNoise(noise)(encoded)\n",
        "    encoded = tf.keras.layers.Dense(640, activation='relu')(encoded)\n",
        "    decoded = tf.keras.layers.Dropout(0.2)(encoded)\n",
        "    decoded = tf.keras.layers.Dense(input_dim, name='decoded')(decoded)\n",
        "    x = tf.keras.layers.Dense(320, activation='relu')(decoded)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Dense(output_dim, activation='sigmoid',\n",
        "                              name='label_output')(x)\n",
        "\n",
        "    encoder = tf.keras.models.Model(inputs=inputs, outputs=encoded)\n",
        "\n",
        "    autoencoder = tf.keras.models.Model(inputs=inputs, outputs=[decoded,x])\n",
        "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "                        loss={'decoded':'mse',\n",
        "                              'label_output':'binary_crossentropy'})\n",
        "    return autoencoder, encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpXA-UOkCH0l"
      },
      "source": [
        "## Building the MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6R0rSS2A3Uh"
      },
      "source": [
        "def build_model(hp, input_dim, output_dim, encoder):\n",
        "    inputs = tf.keras.layers.Input(input_dim)\n",
        "    \n",
        "    x = encoder(inputs)\n",
        "    \n",
        "    x = tf.keras.layers.Concatenate()([x,inputs]) #use both raw and encoded features\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(hp.Float('init_dropout',0.0,0.5))(x)\n",
        "    \n",
        "    for i in range(hp.Int('num_layers',1,5)):\n",
        "        x = tf.keras.layers.Dense(hp.Int('num_units_{i}',128,256))(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Lambda(tf.keras.activations.swish)(x)\n",
        "        x = tf.keras.layers.Dropout(hp.Float(f'dropout_{i}',0.0,0.5))(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(output_dim,activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "    model.compile(optimizer = tf.keras.optimizers.Adam(\n",
        "                    hp.Float('lr',0.00001,0.1,default=0.001)),\n",
        "                  loss = tf.keras.losses.BinaryCrossentropy(\n",
        "                    label_smoothing = hp.Float('label_smoothing',0.0,0.1)),\n",
        "                  metrics = [tf.keras.metrics.AUC(name = 'auc')])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN8xSjQKCoct"
      },
      "source": [
        "## Defining and training the autoencoder\n",
        "\n",
        "We add gaussian noise with mean and std from training datea. After training we lock the layersfin the encoder from further training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IawHYvSFvDPm"
      },
      "source": [
        "autoencoder, encoder = build_autoencoder(x_train.shape[-1], \r\n",
        "                                         y_train.shape[-1],\r\n",
        "                                         noise=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxsTJpM1v595",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79e0e43d-69b5-4b63-8fb6-86a390038c51"
      },
      "source": [
        "#collapse-output\r\n",
        "autoencoder.fit(x_train,(x_train, y_train),\r\n",
        "                epochs=1000,\r\n",
        "                batch_size=4096, \r\n",
        "                validation_split=0.1,\r\n",
        "                callbacks = [\r\n",
        "                    tf.keras.callbacks.EarlyStopping(\r\n",
        "                        'val_loss', patience=10,\r\n",
        "                        restore_best_weights=True)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "346/346 [==============================] - 7s 12ms/step - loss: 2.1184 - decoded_loss: 1.3832 - label_output_loss: 0.7352 - val_loss: 0.8046 - val_decoded_loss: 0.1152 - val_label_output_loss: 0.6894\n",
            "Epoch 2/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 1.0088 - decoded_loss: 0.3173 - label_output_loss: 0.6915 - val_loss: 0.7703 - val_decoded_loss: 0.0810 - val_label_output_loss: 0.6893\n",
            "Epoch 3/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9952 - decoded_loss: 0.3053 - label_output_loss: 0.6899 - val_loss: 0.7617 - val_decoded_loss: 0.0728 - val_label_output_loss: 0.6890\n",
            "Epoch 4/1000\n",
            "346/346 [==============================] - 3s 9ms/step - loss: 0.9690 - decoded_loss: 0.2794 - label_output_loss: 0.6895 - val_loss: 0.7576 - val_decoded_loss: 0.0686 - val_label_output_loss: 0.6890\n",
            "Epoch 5/1000\n",
            "346/346 [==============================] - 3s 9ms/step - loss: 0.9750 - decoded_loss: 0.2856 - label_output_loss: 0.6894 - val_loss: 0.7562 - val_decoded_loss: 0.0676 - val_label_output_loss: 0.6886\n",
            "Epoch 6/1000\n",
            "346/346 [==============================] - 3s 9ms/step - loss: 0.9759 - decoded_loss: 0.2867 - label_output_loss: 0.6892 - val_loss: 0.7537 - val_decoded_loss: 0.0645 - val_label_output_loss: 0.6892\n",
            "Epoch 7/1000\n",
            "346/346 [==============================] - 4s 10ms/step - loss: 0.9767 - decoded_loss: 0.2876 - label_output_loss: 0.6891 - val_loss: 0.7552 - val_decoded_loss: 0.0666 - val_label_output_loss: 0.6886\n",
            "Epoch 8/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9884 - decoded_loss: 0.2994 - label_output_loss: 0.6889 - val_loss: 0.7473 - val_decoded_loss: 0.0590 - val_label_output_loss: 0.6882\n",
            "Epoch 9/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9531 - decoded_loss: 0.2644 - label_output_loss: 0.6887 - val_loss: 0.7472 - val_decoded_loss: 0.0592 - val_label_output_loss: 0.6880\n",
            "Epoch 10/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 1.0210 - decoded_loss: 0.3324 - label_output_loss: 0.6886 - val_loss: 0.7472 - val_decoded_loss: 0.0586 - val_label_output_loss: 0.6887\n",
            "Epoch 11/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9625 - decoded_loss: 0.2740 - label_output_loss: 0.6885 - val_loss: 0.7477 - val_decoded_loss: 0.0595 - val_label_output_loss: 0.6883\n",
            "Epoch 12/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9719 - decoded_loss: 0.2835 - label_output_loss: 0.6884 - val_loss: 0.7530 - val_decoded_loss: 0.0650 - val_label_output_loss: 0.6879\n",
            "Epoch 13/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9867 - decoded_loss: 0.2983 - label_output_loss: 0.6884 - val_loss: 0.7494 - val_decoded_loss: 0.0611 - val_label_output_loss: 0.6882\n",
            "Epoch 14/1000\n",
            "346/346 [==============================] - 3s 9ms/step - loss: 0.9518 - decoded_loss: 0.2636 - label_output_loss: 0.6881 - val_loss: 0.7477 - val_decoded_loss: 0.0590 - val_label_output_loss: 0.6887\n",
            "Epoch 15/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9466 - decoded_loss: 0.2585 - label_output_loss: 0.6881 - val_loss: 0.7474 - val_decoded_loss: 0.0595 - val_label_output_loss: 0.6879\n",
            "Epoch 16/1000\n",
            "346/346 [==============================] - 3s 9ms/step - loss: 0.9588 - decoded_loss: 0.2708 - label_output_loss: 0.6880 - val_loss: 0.7467 - val_decoded_loss: 0.0585 - val_label_output_loss: 0.6882\n",
            "Epoch 17/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 1.0079 - decoded_loss: 0.3200 - label_output_loss: 0.6879 - val_loss: 0.7465 - val_decoded_loss: 0.0584 - val_label_output_loss: 0.6881\n",
            "Epoch 18/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9491 - decoded_loss: 0.2614 - label_output_loss: 0.6877 - val_loss: 0.7464 - val_decoded_loss: 0.0584 - val_label_output_loss: 0.6881\n",
            "Epoch 19/1000\n",
            "346/346 [==============================] - 3s 9ms/step - loss: 0.9493 - decoded_loss: 0.2616 - label_output_loss: 0.6877 - val_loss: 0.7551 - val_decoded_loss: 0.0671 - val_label_output_loss: 0.6881\n",
            "Epoch 20/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9348 - decoded_loss: 0.2472 - label_output_loss: 0.6876 - val_loss: 0.7574 - val_decoded_loss: 0.0693 - val_label_output_loss: 0.6881\n",
            "Epoch 21/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9351 - decoded_loss: 0.2474 - label_output_loss: 0.6877 - val_loss: 0.7526 - val_decoded_loss: 0.0644 - val_label_output_loss: 0.6882\n",
            "Epoch 22/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9658 - decoded_loss: 0.2783 - label_output_loss: 0.6874 - val_loss: 0.7521 - val_decoded_loss: 0.0640 - val_label_output_loss: 0.6881\n",
            "Epoch 23/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9430 - decoded_loss: 0.2556 - label_output_loss: 0.6874 - val_loss: 0.7504 - val_decoded_loss: 0.0620 - val_label_output_loss: 0.6884\n",
            "Epoch 24/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9473 - decoded_loss: 0.2602 - label_output_loss: 0.6871 - val_loss: 0.7534 - val_decoded_loss: 0.0653 - val_label_output_loss: 0.6881\n",
            "Epoch 25/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9362 - decoded_loss: 0.2489 - label_output_loss: 0.6873 - val_loss: 0.7517 - val_decoded_loss: 0.0635 - val_label_output_loss: 0.6882\n",
            "Epoch 26/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9463 - decoded_loss: 0.2592 - label_output_loss: 0.6872 - val_loss: 0.7538 - val_decoded_loss: 0.0654 - val_label_output_loss: 0.6883\n",
            "Epoch 27/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9584 - decoded_loss: 0.2712 - label_output_loss: 0.6872 - val_loss: 0.7582 - val_decoded_loss: 0.0699 - val_label_output_loss: 0.6883\n",
            "Epoch 28/1000\n",
            "346/346 [==============================] - 3s 10ms/step - loss: 0.9484 - decoded_loss: 0.2614 - label_output_loss: 0.6870 - val_loss: 0.7580 - val_decoded_loss: 0.0699 - val_label_output_loss: 0.6881\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa49aa04fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gzgpbPJ2TOZ"
      },
      "source": [
        "encoder.save_weights('encoder.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZG-nTmAwUOO"
      },
      "source": [
        "encoder.load_weights('encoder.hdf5')\r\n",
        "encoder.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I58px8-rgDwp"
      },
      "source": [
        "## Running CV\r\n",
        "\r\n",
        "Following [this notebook](https://www.kaggle.com/gogo827jz/jane-street-ffill-xgboost-purgedtimeseriescv) which use 5 `PurgedGroupTimeSeriesSplit` split on the dates in the training data. \r\n",
        "\r\n",
        "We add the locked encoder as the first layer of the MLP. This seems to help in speeding up the submission rather than first predicting using the encoder then using the MLP. \r\n",
        "\r\n",
        "We use a Baysian Optimizer to find the optimal HPs for out model. 20 trials take about 2 hours on GPU.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gSLY6Q0ehFa"
      },
      "source": [
        "model_fn = lambda hp: build_model(\n",
        "    hp, x_train.shape[-1], y_train.shape[-1], encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj7L8EldhIRx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f22a64-5e62-4a61-bd25-198185bfa855"
      },
      "source": [
        "tuner = CVTuner(\r\n",
        "    hypermodel = model_fn,\r\n",
        "    oracle=kt.oracles.BayesianOptimization(\r\n",
        "        objective=kt.Objective('val_auc', direction='max'),\r\n",
        "        num_initial_points=4,\r\n",
        "        max_trials=20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project ./untitled_project/oracle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUeEFEv4iC4U"
      },
      "source": [
        "#collapse-output\r\n",
        "gkf = PurgedGroupTimeSeriesSplit(n_splits = 5, group_gap=20)\r\n",
        "splits = list(gkf.split(y_train, groups=train_df['date'].values))\r\n",
        "tuner.search((x_train,),(y_train,),\r\n",
        "             splits=splits,\r\n",
        "             batch_size=4096,\r\n",
        "             epochs=100,\r\n",
        "             callbacks=[\r\n",
        "                tf.keras.callbacks.EarlyStopping(\r\n",
        "                    'val_auc', mode='max', patience=3)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iImxHjyGyv-z"
      },
      "source": [
        "hp  = tuner.get_best_hyperparameters(1)[0]\r\n",
        "pd.to_pickle(hp,f'./best_hp_{SEED}.pkl')\r\n",
        "for fold, (train_indices, test_indices) in enumerate(splits):\r\n",
        "    model = model_fn(hp)\r\n",
        "    X_train, X_test = X[train_indices], X[test_indices]\r\n",
        "    y_train, y_test = y[train_indices], y[test_indices]\r\n",
        "    model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100,batch_size=4096,callbacks=[EarlyStopping('val_auc',mode='max',patience=10,restore_best_weights=True)])\r\n",
        "    model.save_weights(f'./model_{SEED}_{fold}.hdf5')\r\n",
        "    model.compile(Adam(hp.get('lr')/100),loss='binary_crossentropy')\r\n",
        "    model.fit(X_test,y_test,epochs=3,batch_size=4096)\r\n",
        "    model.save_weights(f'./model_{SEED}_{fold}_finetune.hdf5')\r\n",
        "tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}